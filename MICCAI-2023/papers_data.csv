Title,Abstract,Code Repository Link,Dataset Link,Category 1,Category 2,Category 3,Category 4,Category 5,Category 6,Category 7,Category 8,Category 9,Category 10
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images ,"Automated segmentation of the blood vessels in 3D volumes is an essential step for the quantitative diagnosis and treatment of many vascular diseases.
3D vessel segmentation is being actively investigated in existing works, mostly in deep learning approaches.
However, training 3D deep networks requires large amounts of manual 3D annotations from experts, which are laborious to obtain. This is especially the case for 3D vessel segmentation, as vessels are sparse yet spread out over many slices and disconnected when visualized in 2D slices.
In this work, we propose a novel method to segment the 3D peripancreatic arteries solely from one annotated 2D projection per training image with depth supervision.
We perform extensive experiments on the segmentation of peripancreatic arteries on 3D contrast-enhanced CT images and demonstrate how well we capture the rich depth information from 2D projections.
We demonstrate that by annotating a single, randomly chosen projection for each training sample, we obtain comparable performance to annotating multiple 2D projections, thereby reducing the annotation effort.
Furthermore, by mapping the 2D labels to the 3D space using depth information and incorporating this into training, we almost close the performance gap between 3D supervision and 2D supervision.
Our code is available at: https://github.com/alinafdima/3Dseg-mip-depth.",https://github.com/alinafdima/3Dseg-mip-depth,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Vascular,Image Segmentation,CT,,,,,,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer ,"Accurate segmentation of digital 3D dental mesh plays a crucial role in various specialized applications within oral medicine. While certain deep learning-based methods have been explored for dental mesh segmentation, the current quality of segmentation fails to meet clinical requirements. This limitation can be attributed to the complexity of tooth morphology and the ambiguity of gingival line. Further more, the semantic information of mesh cells which can provide valuable insights into their categories and enhance local geometric attributes is usually disregarded. Therefore, the segmentation of dental mesh presents a significant challenge in digital oral medicine. To better handle the issue, we propose a novel semantics-based feature learning for dental mesh segmentation that can fully leverage the semantic information to grasp the local and non-local dependencies more accurately through a well-designed graph-transformer. Moreover, we perform adaptive feature aggregation of cross-domain features to obtain high-quality cell-wise 3D dental mesh segmentation results. We validate our method using real 3D dental mesh, and the results demonstrate that our method outperforms the state-of-the-art one-stage methods on 3D dental mesh segmentation. Our Codes are available at https://github.com/df-boy/SGTNet.",https://github.com/df-boy/SGTNet,,Musculoskeletal,Attention models,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks ,"Medical image segmentation typically necessitates a large and precisely annotated dataset. However, obtaining pixel-wise annotation is a labor-intensive task that requires significant effort from domain experts, making it challenging to obtain in practical clinical scenarios. In such situations, reducing the amount of annotation required is a more practical approach. One feasible direction is sparse annotation, which involves annotating only a few slices, and has several advantages over traditional weak annotation methods such as bounding boxes and scribbles, as it preserves exact boundaries. However, learning from sparse annotation is challenging due to the scarcity of supervision signals. To address this issue, we propose a framework that can robustly learn from sparse annotation using the cross-teaching of both 3D and 2D networks. Considering the characteristic of these networks, we develop two pseudo label selection strategies, which are hard-soft confidence threshold and consistent label fusion. Our experimental results on the MMWHS dataset demonstrate that our method outperforms the state-of-the-art (SOTA) semi-supervised segmentation methods. Moreover, our approach achieves results that are comparable to the fully-supervised upper bound result.",https://github.com/HengCai-NJU/3D2DCT,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers ,"Accurate 3D mitochondria instance segmentation in electron microscopy (EM) is a challenging problem and serves as a prerequisite to empirically analyze their distributions and morphology. Most existing approaches employ 3D convolutions to obtain representative features. However, these convolution-based approaches struggle to effectively capture long-range dependencies in the volume mitochondria data, due to their limited local receptive field. To address this, we propose a hybrid encoder-decoder framework based on a split spatio-temporal attention module that efficiently computes spatial and temporal self-attentions in parallel, which are later fused through a deformable convolution. Further, we introduce a semantic foreground-background adversarial loss during training that aids in delineating the region of mitochondria instances from the background clutter. Our extensive experiments on three benchmarks, Lucchi, MitoEM-R and MitoEM-H, reveal the benefits of the proposed contributions achieving state-of-the-art results on all three datasets. Our code and pretrained models will be publicly released.",https://github.com/OmkarThawakar/STT-UNET,,Image Segmentation,Microscopy,,,,,,,,
3D Teeth Reconstruction from Panoramic Radiographs using Neural Implicit Functions ,"Panoramic radiography is a widely used imaging modality in dental practice and research. However, it only provides flattened 2D images, which limits the detailed assessment of dental structures. In this paper, we propose Occudent, a framework for 3D teeth reconstruction from panoramic radiographs using neural implicit functions, which, to the best of our knowledge, is the first work to do so. For a given point in 3D space, the implicit function estimates whether the point is occupied by a tooth, and thus implicitly determines the boundaries of 3D tooth shapes. Firstly, Occudent applies multi-label segmentation to the input panoramic radiograph. Next, tooth shape embeddings as well as tooth class embeddings are generated from the segmentation outputs, which are fed to the reconstruction network. A novel module called Conditional eXcitation (CX) is proposed in order to effectively incorporate the combined shape and class embeddings into the implicit function. The performance of Occudent is evaluated using both quantitative and qualitative measures. Importantly, Occudent is trained and validated with actual panoramic radiographs as input, distinct from recent works which used synthesized images. Experiments demonstrate the superiority of Occudent over state-of-the-art methods.",,,Image Reconstruction,Image Segmentation,Other,CT,other,,,,,
A Closed-form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty ,"Total hip arthroplasty (THA) is an orthopaedic surgery to replace the diseased ball and socket of the hip joint with artificial implants. Achieving appropriate leg length and offset in THA is critical to avoid instability, leg length discrepancies, persistent pain, or early implant failure. This paper provides the first electromagnetic (EM) sensor based approach for accurately measuring the change in leg length and offset intraoperatively. The proposed approach does not require the direct line-of-sight, avoids the need for accurately returning the leg back to the neutral reference position, and has an efficient closed-form solution from least squares optimisation. Validations using simulations, phantom experiments, and cadaver tests demonstrate that the proposed method can provide more accurate results than the conventional mechanical method by manual gauge, the optical tracking based approach, and the direct use of one EM reading, thus showing significant potential clinical value. mechanical method by manual gauge, the optical tracking based approach, and the direct use of one EM reading, thus showing significant potential clinical value.",,,Rigorous Evaluations of Methodology in Clinical Workflows,Musculoskeletal,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,,,,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy ,"Generating virtual populations (VPs) of anatomy is essential for conducting in-silico trials of medical devices. Typically, the generated VP should capture sufficient variability while remaining plausible, and should reflect specific characteristics and patient demographics observed in real populations. It is desirable in several applications to synthesize VPs in a \textit{controlled} manner, where relevant covariates are used to conditionally synthesise virtual populations that fit specific target patient populations/characteristics. We propose to equip a conditional variational autoencoder (cVAE) with normalizing flows to boost the flexibility and complexity of the approximate posterior learned, leading to enhanced flexibility for controllable synthesis of VPs of anatomical structures. We demonstrate the performance of our conditional-flow VAE using a dataset of cardiac left ventricles acquired from 2360 patients, with associated demographic information and clinical measurements (used as covariates/conditioning information). The obtained results indicate the superiority of the proposed method for conditional synthesis of virtual populations of cardiac left ventricles relative to a cVAE. Conditional synthesis performance was assessed in terms of generalisation and specificity errors, and in terms of the ability to preserve clinical relevant biomarkers in the synthesised VPs, I.e. left ventricular blood pool and myocardial volume, relative to the observed real population.",,,Cardiac,Computational Anatomy and Physiology,other,,,,,,,
A coupled-mechanisms modelling framework for neurodegeneration ,"Computational models of neurodegeneration aim to emulate the evolving pattern of pathology in the brain during neurodegenerative disease, such as Alzheimer’s disease. Previous studies have made specific choices on the mechanisms of pathology production and diffusion, or assume that all the subjects lie on the same disease progression trajectory. However, the complexity and heterogeneity of neurodegenerative pathology suggests that multiple mechanisms may contribute synergistically with complex interactions, meanwhile the degree of contribution of each mechanism may vary among individuals. We thus put forward a coupled-mechanisms modelling framework which non-linearly combines the network-topology-informed pathology appearance with the process of pathology spreading within a dynamic modelling system. We account for the heterogeneity of disease by fitting the model at the individual level, allowing the epicenters and rate of progression to vary among subjects.  We construct a Bayesian model selection framework to account for feature importance and parameter uncertainty. This provides a combination of mechanisms that best explains the observations for each individual. With the obtained distribution of mechanism importance for each subject, we are able to identify subgroups of patients sharing similar combinations of apparent mechanisms.",,https://adni.loni.usc.edu/,Neuroimaging - Others,Neuroimaging - DWI and Tractography,Neuroimaging - Functional Brain Networks,Uncertainty,,,,,,
A denoised Mean Teacher for domain adaptive point cloud registration ,"Point cloud-based medical registration promises increased computational efficiency, robustness to intensity shifts, and anonymity preservation but is limited by the inefficacy of unsupervised learning with similarity metrics. Supervised training on synthetic deformations is an alternative but, in turn, suffers from the domain gap to the real domain. In this work, we aim to tackle this gap through domain adaptation. Self-training with the Mean Teacher is an established approach to this problem but is impaired by the inherent noise of the pseudo labels from the teacher. As a remedy, we present a denoised teacher-student paradigm for point cloud registration, comprising two complementary denoising strategies. First, we propose to filter pseudo labels based on the Chamfer distances of teacher and student registrations, thus preventing detrimental supervision by the teacher. Second, we make the teacher dynamically synthesize novel training pairs with noise-free labels by warping its moving inputs with the predicted deformations. Evaluation is performed for inhale-to-exhale registration of lung vessel trees on the public PVT dataset under two domain shifts. Our method surpasses the baseline Mean Teacher by 13.5/62.8%, consistently outperforms diverse competitors, and sets a new state-of-the-art accuracy (TRE=2.31mm). Code is available at https://github.com/multimodallearning/denoised_mt_pcd_reg.",https://github.com/multimodallearning/denoised_mt_pcd_reg,https://github.com/uncbiag/robot,Image Registration,Transfer learning,,,,,,,,
A flexible framework for simulating and evaluating biases in deep learning-based medical image analysis ,"Despite the remarkable advances in deep learning for medical image analysis, it has become evident that biases in datasets used for training such models pose considerable challenges for a clinical deployment, including fairness and domain generalization issues. Although the development of bias mitigation techniques has become ubiquitous, the nature of inherent and unknown biases in real-world medical image data prevents a comprehensive understanding of algorithmic bias when developing deep learning models and bias mitigation methods. To address this challenge, we propose a modular and customizable framework for bias simulation in synthetic but realistic medical imaging data. Our framework provides complete control and flexibility for simulating a range of bias scenarios that can lead to undesired model performance and shortcut learning. In this work, we demonstrate how this framework can be used to simulate morphological biases in neuroimaging data for disease classification with a convolutional neural network as a first feasibility analysis. Using this case example, we show how the proportion of bias in the disease class and proximity between disease and bias regions can affect model performance and explainability results. The proposed framework provides the opportunity to objectively and comprehensively study how biases in medical image data affect deep learning pipelines, which will facilitate a better understanding of how to responsibly develop models and bias mitigation methods for clinical use. Code is available at github.com/estanley16/SimBA.",https://github.com/estanley16/SimBA,,Model Generalizability / Federated Learning,Interpretability / Explainability,Other,MRI,,,,,,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images ,"High-throughput 3D nuclei instance segmentation (NIS) is critical to understanding the complex structure and function of individual cells and their interactions within the larger tissue environment in the brain. Despite the significant progress in achieving accurate NIS within small image stacks using cutting-edge machine learning techniques, there has been a lack of effort to extend this approach towards whole-brain NIS. This critical area of research has been largely overlooked, despite its importance in the neuroscience field. To address this challenge, we propose an efficient deep stitching neural network built upon a knowledge graph model characterizing 3D contextual relationships between nuclei. Our deep stitching model is designed to be agnostic, enabling existing limited methods (optimized for image stack only) to overcome the challenges of whole-brain NIS, particularly in addressing the issue of inter- and intra-slice gaps. We have evaluated the NIS accuracy on top of three state-of-the-art deep models with $128\times 128\times 64$ image stacks, and visualized results in both inter- and intra-slice gaps of whole brain. With resolved gap issues, our deep stitching model enables the whole-brain NIS (gigapixel-level) on entry-level GPU servers within 27 hours.",,,Image Segmentation,Microscopy,,,,,,,,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-Organ and Multi-Modal Images ,"The recent success of deep learning relies heavily on the large amount of labeled data. 
However, acquiring manually annotated symptomatic medical images is notoriously time-consuming and laborious, especially for rare or new diseases. In contrast, normal images from symptom-free healthy subjects without the need of manual annotation are much easier to acquire. In this regard, deep learning based anomaly detection approaches using only normal images are actively studied, achieving significantly better performance than conventional methods. Nevertheless, the previous works committed to develop a specific network for each organ and modality separately, ignoring the intrinsic similarity among images within medical field. In this paper, we propose a model-agnostic framework to detect the abnormalities of various organs and modalities with a single network. By imposing organ and modality classification constraints along with center constraint on the disentangled latent representation, the proposed framework not only improves the generalization ability of the network towards the simultaneous detection of anomalous images with various organs and modalities, but also boosts the performance on each single organ and modality. Extensive experiments with four different baseline models on three public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component. The code will be released after the anonymous review.",https://github.com/lianjizhe/MADDR_code,,Other,Computer Aided Diagnosis,,,,,,,,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images ,"Robust delineation of retinal microvasculature in optical coherence tomography angiography (OCTA) images remains a challenging task, particularly in handling the weak continuity of vessels, low visibility of capillaries, and significant noise interferences. This paper introduces a modulatory elongated model to overcome these difficulties by exploiting the facilitatory and inhibitory interactions exhibited by the contextual influences for neurons in the primary visual cortex. We construct the receptive field of the neurons by an elongated representation, which encodes the underlying profile of vasculature structures, elongated-like patterns, in an anisotropic neighborhood. An annular function is formed to capture the contextual influences presented in the surrounding region outside the neuron support and provide an automatic tuning of contextual information. The proposed modulatory method incorporates the elongated responses with the contextual influences to produce spatial coherent responses for delineating microvasculature features more distinctively from their background regions. Experimental evaluation on clinical retinal OCTA images shows the effectiveness of the proposed model in attaining a promising performance, outperforming the state-of-the-art vessel delineation methods.",,,Ophthalmology,Vascular,,,,,,,,
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images ,"Single particle tracking is an important image analysis technique widely used in biomedical sciences to follow the movement of subcellular structures, which typically appear as individual particles in fluorescence microscopy images. In practice, the low signal-to-noise ratio (SNR) of fluorescence microscopy images as well as the high density and complex movement of subcellular structures pose substantial technical challenges for accurate and robust tracking. In this paper, we propose a novel Transformer-based single particle tracking method called Motion Transformer Tracker (MoTT). By using its attention mechanism to learn complex particle behaviors from past and hypothetical future tracklets (i.e., fragments of trajectories), MoTT estimates the matching probabilities between each live/established tracklet and its multiple hypothesis tracklets simultaneously, as well as the existence probability and position of each live tracklet. Global optimization is then used to find the overall best matching for all live tracklets. For those tracklets with high existence probabilities but missing detections due to e.g., low SNRs, MoTT utilizes its estimated particle positions to substitute for the missed detections, a strategy we refer to as relinking in this study. Experiments have confirmed that this strategy substantially alleviates the impact of missed detections and enhances the robustness of our tracking method. Overall, our method substantially outperforms competing state-of-the-art methods on the ISBI Particle Tracking Challenge datasets. It provides a powerful tool for studying the complex spatiotemporal behavior of subcellular structures. The source code is publicly available at https://github.com/imzhangyd/MoTT.git.",https://github.com/imzhangyd/MoTT.git,http://bioimageanalysis.org/track/,Microscopy,Attention models,Video,,,,,,,
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics ,We introduce a disease progression model suited for neurodegenerative pathologies that allows to model associations between covariates and dynamic features of the disease course. We establish a statistical framework and implement an algorithm for its estimation. We show that the model is reliable and can provide uncertainty estimates of the discovered associations thanks to its Bayesian formulation. The model’s interest is showcased by shining a new light on genetic associations.,,,Treatment Response and Outcome/Disease Prediction,Imaging Biomarkers,,,,,,,,
A Multi-Task Method for Immunofixation Electrophoresis Image Classification ,"In the field of plasma cell disorders diagnosis, the detection of abnormal monoclonal (M) proteins through Immunofixation Electrophoresis (IFE) is a widely accepted practice. However, the classification of IFE images into nine distinct categories is a complex task due to the significant class imbalance problem. To address this challenge, a two-sub-task classification approach is proposed, which divides the classification task into the determination of severe and mild cases, followed by their combination to produce the final result. This strategy is based on the expert understanding that the nine classes are different combinations of severe and mild cases. Additionally, the examination of the dense band co-location on the electrophoresis lane and other lanes is crucial in the expert evaluation of the image class. To incorporate this expert knowledge into the model training, inner-task and inter-task regularization is introduced. The effectiveness of the proposed method is demonstrated through experiments conducted on approximately 15,000 IFE images, resulting in interpretable visualization outcomes that are in alignment with expert expectations.",https://github.com/shiy19/IFE-classification,,Computer Aided Diagnosis,other,,,,,,,,
A Multi-Task Network for Anatomy Identification in Endoscopic Pituitary Surgery ,"Pituitary tumours are in an anatomically dense region of the body, and often distort or encase the surrounding critical structures. This, in combination with anatomical variations and limitations imposed by endoscope technology, makes intra-operative identification and protection of these structures challenging. Advances in machine learning have allowed for the opportunity to automatically identifying these anatomical structures within operative videos. However, to the best of the authors’ knowledge, this remains an unaddressed problem in the sellar phase of endoscopic pituitary surgery. In this paper, PAINet (Pituitary Anatomy Identification Network), a multi-task network capable of identifying the ten critical anatomical structures, is proposed. PAINet jointly learns: (1) the semantic segmentation of the two most prominent, largest, and frequently occurring structures (sella and clival recess); and (2) the centroid detection of the remaining eight less prominent, smaller, and less frequently occurring structures. PAINet utilises an EfficientNetB3 encoder and a U-Net++ decoder with a convolution layer for segmentation and pooling layer for detection. A dataset of 64-videos (635 images) were recorded, and annotated for anatomical structures through multi-round expert consensus. Implementing 5-fold cross-validation, PAINet achieved 66.1% and 54.1% IoU for sella and clival recess semantic segmentation respectively, and 53.2% MPCK-20% for centroid detection of the remaining eight structures, improving on single-task performances. This therefore demonstrates automated identification of anatomical critical structures in the sellar phase of endoscopic pituitary surgery is possible.",https://github.com/dreets/pitnet-anat-public,,Surgical Data Science,Image Segmentation,Surgical Scene Understanding,Surgical Skill and Work Flow Analysis,,,,,,
A Novel Multi-Task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images ,"Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients. However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis. A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists’ diagnostic procedures and strategies. Through multi-task learning, the model simultaneously predicts body parts and lesion attributes in addition to the disease itself, enhancing diagnosis accuracy and improving diagnosis interpretability. The designed lesion selection module mimics dermatologists’ zoom-in action, effectively highlighting the local lesion features from noisy backgrounds. Additionally, the presented cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases. To provide a more robust evaluation of the proposed method, a large-scale clinical image dataset of skin diseases with significantly more cases than existing datasets has been established. Extensive experiments on three different datasets consistently demonstrate the state-of-the-art recognition performance of the proposed approach.",,,Computer Aided Diagnosis,Dermatology,,,,,,,,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation ,"Flexible ureteroscopy (FURS) navigation remains challenging since ureteroscopic images are poor quality with artifacts such as water and floating matters, leading to a difficulty in directly registering these images to preoperative images. This paper presents a novel 2D-3D registration method with structure point similarity for robust vision-based flexible ureteroscopic navigation without using any external positional sensors. Specifically, this new method first uses vision transformers to extract structural regions of the internal surface of the kidneys in real FURS video images and then generates virtual depth maps by the ray-casting algorithm from preoperative computed tomography urogram (CTU) images. After that, a novel similarity function without using pixel intensity is defined as an intersection of point sets from the extracted structural regions and virtual depth maps for the video-CTU registration optimization. We evaluate our video-CTU registration method on in-house ureteroscopic data acquired from the operating room, with the experimental results showing that our method attains higher accuracy than current methods. Particularly, it can reduce the position and orientation errors from (11.28 mm, 10.8°) to (5.39 mm, 8.13°).",,,Guided Interventions and Surgery,CT,Video,Surgical Data Science,,,,,,
A One-class Variational Autoencoder (OCVAE) cascade for classifying atypical bone marrow cell sub-types ,"Atypical bone marrow (BM) cell-subtype characterization defines the diagnosis and follow up of different hematologic disorders. However, this process is basically a visual task, which is prone to inter- and intra-observer variability. The presented work introduces a new application of one-class variational autoencoders (OCVAE) for automatically classifying the 4 most common pathological atypical BM cell-subtypes, namely myelocytes, blasts, promyelocytes, and erythroblasts, regardless the disease they are associated with. The presented OCVAE-based representation is obtained by concatenating the bottleneck of 4 separated OCVAEs, specifically set to capture one-cell-sub-type pattern at a time. In addition, this strategy provides a complete validation scheme in a subset of an open access image dataset, demonstrating low requirements in terms of number of training images. Each particular OCVAE is trained to provide specific latent space parameters (64 means and 64 variances) for the corresponding atypical cell class. Afterwards, the obtained concatenated representation space feeds different classifiers which discriminate the proposed classes. Evaluation is done by using a subset (n = 26, 000) of a public single-cell BM image database, including two independent partitions, one for setting the VAEs to extract features (n = 20, 800), and one for training and testing a set classifiers (n = 5, 200). Reported performance metrics show the concatenated-OCVAE characterization successfully differentiates the proposed atypical BM cell classes with accuracy=0.938, precision=0.935, recall=0.935, f1-score=0.932, outperforming previously published strategies for the same task (handcrafted features, ResNext, ResNet-50, XCeption, CoAtnet), while a more thorough experimental validation is included.",,https://doi.org/10.7937/TCIA.AXH3-T579,Computational (Integrative) Pathology,Oncology,Computer Aided Diagnosis,Histopathology,Microscopy,,,,,
A Patient-Specific Self-supervised Model for Automatic X-ray/CT Registration ,"The accurate estimation of X-ray source pose in relation to pre-operative images is crucial for minimally invasive procedures. However, existing deep learning-based automatic registration methods often have one or some limitations, including heavy reliance on subsequent conventional refinement steps, requiring manual annotation for training, or ignoring the patient’s anatomical specificity. To address these limitations, we propose a patient-specific and self-supervised end-to-end framework. Our approach utilizes patient’s preoperative CT to generate simulated X-rays that include patient-specific information. We propose a self-supervised regression neural network trained on the simulated patient-specific X-rays to predict six degrees of freedom pose of the X-ray source. In our proposed network, regularized autoencoder and multi-head self-attention mechanism are employed to encourage the model to automatically capture patient-specific salient information that supports accurate pose estimation, and Incremental Learning strategy is adopted for network training to avoid over-fitting and promote network performance. Meanwhile, an novel refinement model is proposed, which provides a way to obtain gradients with respect to the pose parameters to further refine the pose predicted by the regression network. Our method achieves a mean projection distance of 3.01mm with a success rate of 100% on simulated X-rays, and a mean projection distance of 1.55mm on X-rays. The code is available at github.com/BaochangZhang/PSSS_registration",https://github.com/BaochangZhang/PSSS_registration,,Guided Interventions and Surgery,Image Registration,CT,,,,,,,
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications ,"Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed near-duplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent space, our walking strategy is safer than linear interpolation. We then test our path-finding strategy combined to k-same methods and demonstrate, on two benchmarks for tuberculosis and diabetic retinopathy classification, that training a model using samples generated by our approach mitigate drops in performance, while keeping privacy preservation.",https://github.com/perceivelab/PLAN,,Other,Lung,Ophthalmology,,,,,,,
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging ,"Staging of liver fibrosis is important in the diagnosis and treatment planning of patients suffering from liver diseases. Current deep learning-based methods using abdominal magnetic resonance imaging (MRI) usually take a sub-region of the liver as input, which could miss critical information. To explore richer representations, we formulate this task as a multi-view learning problem and employ multiple sub-regions of the liver. Previously, features or predictions are usually combined in an implicit way, and uncertainty-aware multi-view learning methods have been proposed recently. However, the methods could be challenged to capture cross-view representations, which can be important in the accurate prediction of staging. Therefore, we propose a reliable multi-view learning method with interpretable combination rules, which can model global representations to improve the accuracy of predictions. Specifically, the proposed method estimates uncertainties based on subjective logic to improve reliability, and an explicit combination rule is applied based on Dempster-Shafer’s evidence theory with good power of interpretability. Moreover, a data-efficient transformer is introduced to capture representations in the global view. Results evaluated on enhanced MRI data show that our method delivers superior performance over existing multi-view learning methods.",https://github.com/key1589745/Multi-view_liver,,Computer Aided Diagnosis,Interpretability / Explainability,Uncertainty,,,,,,,
A Semantic-guided and Knowledge-based Generative Framework for Orthodontic Visual Outcome Preview ,"Orthodontic treatment typically lasts for two years, and its outcome cannot be predicted intuitively in advance. In this paper, we propose a semantic-guided and knowledge-based generative framework to predict the visual outcome of orthodontic treatment from a single frontal photo. The framework involves four steps. Firstly, we perform tooth semantic segmentation and mouth cavity segmentation and extract category-specific teeth contours from frontal images. Secondly, we deform the established tooth-row templates to match the projected contours with the detected ones to reconstruct 3D teeth models. Thirdly, we apply a teeth alignment algorithm to simulate the orthodontic treatment. Finally, we train a semantic-guided generative adversarial network to predict the visual outcome of teeth alignment. Quantitative tests are conducted to evaluate the proposed framework, and the results are as follows: the tooth semantic segmentation model achieves a mean intersection of union of 0.834 for the anterior teeth, the average symmetric surface distance error of our 3D teeth reconstruction method is 0.626 mm on the test cases, and the image generation model has an average Fréchet inception distance of 6.847 over all the test images. These evaluation results demonstrate the practicality of our framework in orthodontics.",,,Treatment Response and Outcome/Disease Prediction,Image Reconstruction,Image Segmentation,other,,,,,,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation ,"Deep learning based methods have become the most popular approach for prostate segmentation in MRI. However, domain variations due to the complex acquisition process result in textural differences as well as imaging artefacts which significantly affects the robustness of deep learning models for prostate segmentation across multiple sites. We tackle this problem by using multiple MRI sequences to learn a set of low dimensional shape components whose combinatorially large learnt composition is capable of accounting for the entire distribution of segmentation outputs. We draw on the language of cellular sheaf theory to model compositionality driven by local and global topological correctness. In our experiments, our method significantly improves the domain generalisability of anatomical and tumour segmentation of the prostate.",https://github.com/AinkaranSanthi/A-Sheaf-Theoretic-Perspective-for-Robust-Segmentation,http://medicaldecathlon.com/dataaws/,Image Segmentation,Model Generalizability / Federated Learning,MRI,,,,,,,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding ,"Motor imagery (MI) electroencephalogram (EEG) decoding, as a core component widely used in noninvasive brain-computer interface (BCI) system, is critical to realize the interaction purpose of physical world and brain activity. However, the conventional methods are challenging to obtain desirable results for two main reasons: there is a small amount of labeled data making it difficult to fully exploit the features of EEG signals, and lack of unified expert knowledge among different individuals. To handle these dilemmas, a novel small-sample EEG decoding method based on abductive learning (SSE-ABL) is proposed in this paper, which integrates perceiving module that can extract multiscale features of multi-channel EEG in semantic level and knowledge base module of brain science. The former module is trained via pseudo-labels of unlabeled EEG signals generated by abductive learning, and the latter is refined via the label distribution predicted by semi-supervised learning. Experimental results demonstrate that SSE-ABL has a superior performance compared with state-of-the-art methods and is also convenient for visualizing the underlying information flow of EEG decoding.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Active Learning,EEG/ECG,,,,,,,
A Spatial-Temporal Deformable Attention based Framework for Breast Lesion Detection in Videos ,"Detecting breast lesion in videos is crucial for computer-
aided diagnosis. Existing video-based breast lesion detection approaches
typically perform temporal feature aggregation of deep backbone fea-
tures based on the self-attention operation. We argue that such a strat-
egy struggles to effectively perform deep feature aggregation and ig-
nores the useful local information. To tackle these issues, we propose a
spatial-temporal deformable attention based framework, named STNet.
Our STNet introduces a spatial-temporal deformable attention module
to perform local spatial-temporal feature fusion. The spatial-temporal
deformable attention module enables deep feature aggregation in each
stage of both encoder and decoder. To further accelerate the detection
speed, we introduce an encoder feature shuffle strategy for multi-frame
prediction during inference. In our encoder feature shuffle strategy, we
share the backbone and encoder features, and shuffle encoder features
for decoder to generate the predictions of multiple frames. The exper-
iments on the public breast lesion ultrasound video dataset show that
our STNet obtains a state-of-the-art detection performance, while oper-
ating twice as fast inference speed. The code and model are available at
https://github.com/AlfredQin/STNet.",https://github.com/AlfredQin/STNet,,Attention models,Breast,Computer Aided Diagnosis,Ultrasound,Video,,,,,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference ,"Physics-informed neural network (PINN) is a new paradigm for solving the forward and inverse problems of partial differential equations (PDEs). Its penetration into 3D bi-ventricular electrophysiology (EP) however has been slow, owing to its fundamental limitations to solve PDEs over large or complex solution domains with sharp transitions. In this paper, we propose a new PINN framework to overcome these challenges via three key innovations: 1) a weak-form PDE residual to bypass the challenges of high-order spatial derivatives over irregular spatial domains, 2) a spatial-temporally adaptive training strategy to mitigate the failure of PINN to propagate correct solutions and accelerate convergence, and 3) a sequential learning strategy to enable solutions over longer time domains. We experimentally demonstrated the effectiveness of the presented PINN framework to obtain the complete forward and inverse EP solutions over the 3D bi-ventricular geometry, which is otherwise not possible with vanilla PINN frameworks.",,,Computational Anatomy and Physiology,Cardiac,Interventional Simulation Systems,Other,EEG/ECG,,,,,
A Style Transfer-based Augmentation Framework for Improving Segmentation and Classification Performance across Different Sources in Ultrasound Images ,"Ultrasound imaging can vary in style/appearance due to differences in scanning equipment and other factors, resulting in degraded segmentation and classification performance of deep learning models for ultrasound image analysis. Previous studies have attempted to solve this problem by using style transfer and augmentation techniques, but these methods usually require a large amount of data from multiple sources and source-specific discriminators, which are not feasible for medical datasets with limited samples. Moreover, finding suitable augmentation methods for ultrasound data can be difficult. To address these challenges, we propose a novel style transfer-based augmentation framework that consists of three components: mixed style augmentation (MixStyleAug), feature augmentation (FeatAug), and mask-based style augmentation (MaskAug). MixStyleAug uses a style transfer network to transform the style of a training image into various reference styles, which enriches the information from different sources for the network. FeatAug augments the styles at the feature level to compensate for possible style variations, especially for small-size datasets with limited styles. MaskAug leverages segmentation masks to highlight the key regions in the images, which enhances the model’s generalizability. We evaluate our framework on five ultrasound datasets collected from different scanners and centers. Our framework outperforms previous methods on both segmentation and classification tasks, especially on small-size datasets. Our results suggest that our framework can effectively improve the performance of deep learning models across different ultrasound sources with limited data.",,,Ultrasound,Abdomen,Computer Aided Diagnosis,Image Segmentation,,,,,,
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging ,"Brachial plexopathy is a form of peripheral neuropathy, which occurs when there is damage to the brachial plexus (BP).  However, the diagnosis of breast cancer related BP from radiological imaging is still a great challenge.  This paper proposes a texture pattern based convolutional neural network, called TPPNet, to carry out abnormal prediction of BP from multiple routine magnetic resonance image (MRI) pulse sequences, i.e. T2, T1, and T1 post-gadolinium contrast administration.  Different from classic CNNs, the input of the proposed TPPNet is multiple texture patterns instead of images.  This allows for direct integration of radiomic (i.e. texture) features into the classification models.  Beyond conventional radiomic features, we also developed a new family of texture patterns, called triple point patterns (TPPs), to extract huge number of texture patterns as representations of BP’ heterogeneity from its MRIs.  These texture patterns share the same size and show very stable properties under several geometric transformations.  Then, the TPPNet is proposed to carry out the differentiation task of abnormal BP for our study.  It has several special characteristics including 1) avoidance of image augmentation, 2) huge number of channels, 3) simple end-to-end architecture, 4) free from the interference of multi-texture-pattern arrangements.  Ablation study and comparisons demonstrate that the proposed TPPNet yields outstanding performances with the accuracies of 96.1%, 93.5% and 93.6% over T2, T1 and post-gadolinium sequences which exceed at least 1.3%, 5.3% and 3.4% over state-of-the-art methods for classification of normal vs. abnormal brachial plexus.",,,Neuroimaging - Others,Computer Aided Diagnosis,Other,MRI,,,,,,
A Transfer Learning Approach to Localise a Deep Brain Stimulation Target ,"The ventral intermediate nucleus of thalamus (Vim) is a well-established surgical target in magnetic resonance-guided (MR-guided) surgery for the treatment of tremor. As the structure is not identifiable from conventional MR sequences, targeting the Vim has predominantly relied on standardised Vim atlases and thus fails to model individual anatomical variability. To overcome this limitation, recent studies define the Vim using its white matter connectivity with both primary cortex and dentate nucleus, estimated via tractography. Although successful in accounting for individual variability, these connectivity-based methods are sensitive to variations in image acquisition and processing, and require high-quality diffusion imaging techniques which are often not available in clinical contexts. Here we propose a novel transfer learning approach to accurately target the Vim particularly on clinical-quality data. The approach transfers anatomical information from publicly-available high-quality datasets to a wide range of white matter connectivity features in low-quality data, to augment inference on the Vim. We demonstrate that the approach can robustly and reliably identify the Vim despite compromised data quality, and is generalisable to different datasets, outperforming previous surgical targeting methods.",https://git.fmrib.ox.ac.uk/yqzheng1/hqaugmentation.jl,https://www.humanconnectome.org/study/hcp-young-adult/data-releases,Guided Interventions and Surgery,Transfer learning,MRI,Surgical Data Science,Surgical Planning and Simulation,,,,,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization ,"Cochlear implants (CIs) are neuroprosthetics that can provide a sense of sound to people with severe-to-profound hearing loss. A CI contains an electrode array (EA) that is threaded into the cochlea during surgery. Recent studies have shown that hearing outcomes are correlated with EA placement. An image-guided cochlear implant programming technique is based on this correlation and utilizes the EA location with respect to the intracochlear anatomy to help audiologists adjust the CI settings to improve hearing. Automated methods to localize EA in postoperative CT images are of great interest for large-scale studies and for translation into the clinical workflow. In this work, we propose a unified deep-learning-based framework for automated EA localization. It consists of a multi-task network and a series of postprocessing algorithms to localize various types of EAs. The evaluation on a dataset with 27 cadaveric samples shows that its localization error is slightly smaller than the state-of-the-art method. Another evaluation on a large-scale clinical dataset containing 561 cases across two institutions demonstrates a significant improvement in robustness compared to the state-of-the-art method. This suggests that this technique could be integrated into the clinical workflow and provide audiologists with information that facilitates the programming of the implant leading to improved patient care.",,,Guided Interventions and Surgery,Other,CT,Visualization in Biomedical Imaging,,,,,,
A Video-based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants ,"We present an end-to-end computer vision pipeline to detect non-nutritive sucking (NNS)—an infant sucking pattern with no nutrition delivered—as a potential biomarker for developmental delays, using off-the-shelf baby monitor video footage. One barrier to clinical (or algorithmic) assessment of NNS stems from its sparsity, requiring experts to wade through hours of footage to find minutes of relevant activity. Our NNS activity segmentation algorithm solves this problem by identifying periods of NNS with high certainty—up to 94.0% average precision and 84.9% average recall across 30 heterogeneous 60 s clips, drawn from our manually annotated NNS clinical in-crib dataset of 183 hours of overnight baby monitor footage from 19 infants. Our method is based on an underlying NNS action recognition algorithm, which uses spatiotemporal deep learning networks and infant-specific pose estimation, achieving 94.9% accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips. Tested on our second, independent, and public NNS in-the-wild dataset, NNS recognition classification reaches 92.3% accuracy, and NNS segmentation achieves 90.8% precision and 84.2% recall.",https://github.com/ostadabbas/NNS-Detection-and-Segmentation,https://github.com/ostadabbas/NNS-Detection-and-Segmentation,Data Efficient Learning,Video,,,,,,,,
A2FSeg: Adaptive Multi-Modal Fusion Network for Medical Image Segmentation ,"Magnetic Resonance Imaging (MRI) plays an important role in multi-modal brain tumor segmentation. However, missing modality is very common in clinical diagnosis, which will lead to severe segmentation performance degradation. In this paper, we propose a simple adaptive multi-modal fusion network for brain tumor segmentation, which has two stages of feature fusion, including a simple average fusion and an adaptive fusion based on an attention mechanism. Both fusion techniques are capable to handle the missing modality situation and contribute to the improvement of segmentation results, especially the adaptive one. We evaluate our method on the BraTS2020 dataset, achieving the state-of-the-art performance for the incomplete multi-modal brain tumor segmentation, compared to four recent methods. Our A2FSeg (Average and Adaptive Fusion Segmentation network) is simple yet effective and has the capability of handling any number of image modalities for incomplete multi-modal segmentation. Our source code is online and available at https://github.com/Zirui0623/A2FSeg.git.",https://github.com/Zirui0623/A2FSeg.git,https://www.med.upenn.edu/cbica/brats2020/data.html,Image Segmentation,Oncology,Attention models,MRI,,,,,,
ACC-UNet: A Completely Convolutional UNet model for the 2020s ,"This decade is marked by the introduction of Vision Transformer, a radical paradigm shift in broad computer vision. A similar trend is followed in medical imaging, UNet, one of the most influential architectures, has been redesigned with transformers. Recently, the efficacy of convolutional models in vision is being reinvestigated by seminal works such as ConvNext, which elevates a ResNet to Swin Transformer level. Deriving inspiration from this, we aim to improve a purely convolutional UNet model so that it can be on par with the transformer-based models, e.g, Swin-Unet or UCTransNet. We examined several advantages of the transformer-based UNet models, primarily long-range dependencies and cross-level skip connections. We attempted to emulate them through convolution operations and thus propose, ACC-UNet, a completely convolutional UNet model that brings the best of both worlds, the inherent inductive biases of convnets with the design decisions of transformers. ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and consistently outperformed convnets, transformers, and their hybrids. Notably, ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by  $2.64 \pm 2.54\%$ and $0.45 \pm 1.61\%$ in terms of dice score, respectively, while using a fraction of their parameters ($59.26\%$ and $24.24\%$). Our codes are available at https://github.com/kiharalab/ACC-UNet.",https://github.com/kiharalab/ACC-UNet,https://challenge.isic-archive.com/data/#2018,Image Segmentation,,,,,,,,,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging using a Depth Camera ,"Accurate and robust estimation of the patient’s height and weight is essential for many clinical imaging workflows. Patient’s safety, as well as a number of scan optimizations, rely on this information. In this paper we present a deep-learning based method for estimating the patient’s height and weight in unrestricted clinical environments using depth images from a 3-dimensional camera. We train and validate our method on a very large dataset of more than 1850 volunteers and/or patients captured in more than 7500 clinical workflows. Our method achieves a PH5 of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 of 95.6% and a PW20 of 99.8% for weight estimation, making the proposed method state-of-the-art in clinical setting.",,,Rigorous Evaluations of Methodology in Clinical Workflows,Other,Transfer learning,CT,MRI,other,,,,
Accurate multi-contrast MRI super-resolution via a dual cross-attention transformer network ,"Magnetic Resonance Imaging (MRI) is a critical imaging tool in clinical diagnosis, but obtaining high-resolution MRI images can be challenging due to hardware and scan time limitations. Recent studies have shown that using reference images from multi-contrast MRI data could improve super-resolution quality. However, the commonly employed strategies, e.g., channel concatenation or hard-attention based texture transfer, may not be optimal given the visual differences between multi-contrast MRI images. To address these limitations, we propose a new Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) framework. This approach introduces a dual cross-attention transformer architecture, where the features of the reference image and the up-sampled input image are extracted and promoted with both spatial and channel attention in multiple resolutions. Unlike existing hard-attention based methods where only the most correlated features are sought via the highly down-sampled reference images, the proposed architecture is more powerful to capture and fuse the shareable information between the multi-contrast images. Extensive experiments are conducted on fastMRI knee data at high field and more challenging brain data at low field, demonstrating that DCAMSR can substantially outperform the state-of-the-art single-image and multi-contrast MRI super-resolution methods, and even remains robust in a self-referenced manner. The code for DCAMSR is avaliable at https://github.com/Solor-pikachu/DCAMSR.",https://github.com/Solor-pikachu/DCAMSR,https://github.com/facebookresearch/fastMRI,Image Reconstruction,MRI,,,,,,,,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast ,"Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes, i.e., boundary regions or rare objects. Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed class centers. Moreover, we argue that blindly adopting a constant temperature in the contrastive loss on long-tailed medical data is not optimal, and propose to use a dynamic temperature via a simple cosine schedule to yield better separation between majority and minority classes. Empirically, we evaluate ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art across two semi-supervised settings. Theoretically, we analyze the performance of adaptive anatomical contrast and confirm its superiority in label efficiency.",https://github.com/charlesyou999648/ACTION,,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
ACT-Net: Anchor-context Action Detection in Surgery Videos ,"Recognition and localization of surgical detailed actions is an essential component of developing a context-aware decision support system. However, most existing detection algorithms fail to provide high-accuracy action classes even having their locations, as they do not consider the surgery procedure’s regularity in the whole video. This limitation hinders their application. Moreover, implementing the predictions in  clinical applications seriously needs to convey model confidence to earn entrustment, which is unexplored in surgical action prediction.  In this paper, to accurately detect  fine-grained actions that happen at every moment, we propose an anchor-context action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, to answer the following questions: 1) where the actions happen; 2) what actions are; 3) how confidence predictions are. Specifically, the proposed ACD module spatially and temporally highlights the regions interacting with  the extracted anchor in surgery video, which outputs action location and its class distribution based on anchor-context interactions. Considering the full distribution of action classes in videos, the CCD module adopts a denoising diffusion-based generative model conditioned on our ACD estimator to further reconstruct accurately the action predictions. Moreover, we utilize the stochastic nature of the diffusion model outputs to access model confidence for each prediction. Our method reports the state-of-the-art performance, with improvements of 4.0% mAP against baseline on the surgical video dataset.",,,Surgical Scene Understanding,Video,Surgical Skill and Work Flow Analysis,,,,,,,
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion ,"In treating acute ischemic stroke (AIS), determining the time since stroke onset (TSS) is crucial. Computed tomography perfusion (CTP) is vital for determining TSS by providing sufficient cerebral blood flow information. However, the CTP has small samples and high dimensions. In addition, the CTP is multi-map data, which has heterogeneity and complementarity. To address these issues, this paper demonstrates a classification model using CTP to classify the TSS of AIS pa-tients. Firstly, we use dynamic convolution to improve model representation without increasing network complexity. Secondly, we use multi-scale feature fu-sion to fuse the local correlation of low-order features and use a transformer to fuse the global correlation of higher-order features. Finally, multi-head pooling attention is used to learn the feature information further and obtain as much im-portant information as possible. We use a five-fold cross-validation strategy to verify the effectiveness of our method on the private dataset from a local hospital. The experimental results show that our proposed method achieves at least 5% higher accuracy than other methods in TTS classification task.",,,Computer Aided Diagnosis,Neuroimaging - Others,,,,,,,,
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases ,"Currently intelligent diagnosis systems lack the ability of continually learning to diagnose new diseases once deployed, under the condition of preserving old disease knowledge. In particular, updating an intelligent diagnosis system with training data of new diseases would cause catastrophic forgetting of old disease knowledge. To address the catastrophic forgetting issue, an Adapter-based Continual Learning framework called ACL is proposed to help effectively learn a set of new diseases at each round (or task) of continual learning, without changing the shared feature extractor. The learnable lightweight task-specific adapter(s) can be flexibly designed (e.g., two convolutional layers) and then added to the pretrained and fixed feature extractor. Together with a specially designed task-specific head which absorbs all previously learned old dis- eases as a single ‘out-of-distribution’ category, task-specific adapter(s) can help the pretrained feature extractor more effectively extract dis- criminative features between diseases. In addition, a simple yet effective fine-tuning is applied to collaboratively fine-tune multiple task-specific heads such that outputs from different heads are comparable and consequently the appropriate classifier head can be more accurately selected during model inference. Extensive empirical evaluations on three im- age datasets demonstrate the superior performance of ACL in continual learning of new diseases. The source code is available at https://github.com/GiantJun/CL_Pytorch.",https://github.com/GiantJun/CL_Pytorch,https://challenge.isic-archive.com/data/#2019,Continual Learning,Computer Aided Diagnosis,Other,,,,,,,
Adaptive Multi-scale Online Likelihood Network for AI-assisted Interactive Segmentation ,"Existing interactive segmentation methods leverage automatic segmentation and user interactions for label refinement, significantly reducing the annotation workload compared to manual annotation. However, these methods lack quick adaptability to ambiguous and noisy data, which is a challenge in CT volumes containing lung lesions from COVID-19 patients. In this work, we propose an adaptive multi-scale online likelihood network (MONet) that adaptively learns in a data-efficient online setting from both an initial automatic segmentation and user interactions providing corrections. We achieve adaptive learning by proposing an adaptive loss that extends the influence of user-provided interaction to neighboring regions with similar features. In addition, we propose a data-efficient probability-guided pruning method that discards uncertain and redundant labels in the initial segmentation to enable efficient online training and inference. Our proposed method was evaluated by an expert in a blinded comparative study on COVID-19 lung lesion annotation task in CT. Our approach achieved 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score than the state-of-the-art. Source code is available at: https://github.com/masadcv/MONet-MONAILabel",https://github.com/masadcv/MONet-MONAILabel,,Data Efficient Learning,CT,,,,,,,,
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation ,"The process of annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter.  Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6\% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at https://github.com/DeepMicroscopy/AdaptiveRegionSelection.",https://github.com/DeepMicroscopy/AdaptiveRegionSelection,http://gigadb.org/dataset/100439,Active Learning,Histopathology,,,,,,,,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs ,"Immunohistochemical (IHC) staining highlights the molecular information critical to diagnostics in tissue samples. However, compared to H&E staining, IHC staining can be much more expensive in terms of both labor and the laboratory equipment required. This motivates recent research that demonstrates that the correlations between the morphological information present in the H&E-stained slides and the molecular information in the IHC-stained slides can be used for H&E-to-IHC stain translation. However, due to a lack of pixel-perfect H&E-IHC groundtruth pairs, most existing methods have resorted to relying on expert annotations. To remedy this situation, we present a new loss function, Adaptive Supervised PatchNCE (ASP), to directly deal with the input to target inconsistencies in a proposed H&E-to-IHC image-to-image translation framework. The ASP loss is built upon a patch-based contrastive learning criterion, named Supervised PatchNCE (SP), and augments it further with weight scheduling to mitigate the negative impact of noisy supervision. Lastly, we introduce the Multi-IHC Stain Translation (MIST) dataset, which contains aligned H&E-IHC patches for 4 different IHC stains critical to breast cancer diagnosis. In our experiment, we demonstrate that our proposed method outperforms existing image-to-image translation methods for stain translation to multiple IHC stains. All of our code and datasets are available at https://github.com/lifangda01/AdaptiveSupervisedPatchNCE.",https://github.com/lifangda01/AdaptiveSupervisedPatchNCE,https://github.com/lifangda01/AdaptiveSupervisedPatchNCE,Computational (Integrative) Pathology,Breast,Image Reconstruction,Other,Histopathology,Microscopy,,,,
Additional Positive Enables Better Representation Learning for Medical Images ,"This paper presents a new way to identify additional positive pairs for BYOL, a state-of-the-art (SOTA) self-supervised learning framework, to improve its representation learning ability. Unlike conventional BYOL which relies on only one positive pair generated by two augmented views of the same image, we argue that information from different images with the same label can bring more diversity and variations to the target features, thus benefiting representation learning. To identify such pairs without any label, we investigate TracIn, an instance-based and computationally efficient influence function, for BYOL training. Specifically, TracIn is a gradient-based method that reveals the impact of a training sample on a test sample in supervised learning. We extend it to the self-supervised learning setting and propose an efficient batch-wise per-sample gradient computation method to estimate the pairwise TracIn to represent the similarity of samples in the mini-batch during training. For each image, we select the most similar sample from other images as the additional positive and pull their features together with BYOL loss. Experimental results on two public medical datasets (i.e., ISIC 2019 and ChestX-ray) demonstrate that the proposed method can improve the classification performance compared to other competitive baselines in both semi-supervised and transfer learning settings.",,https://challenge.isic-archive.com/landing/2019/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,,
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography ,"Myopia is a manifestation of visual impairment caused by an excessively elongated eyeball. Image data is critical material for studying high myopia and pathological myopia. Measurements of spherical equivalent and axial length are the gold standards for identifying high myopia, but the available image data for matching them is scarce. In addition, the criteria for defining high myopia vary from study to study, and therefore the inclusion of samples in automated screening efforts requires an appropriate assessment of interpretability. In this work, we propose a model called adjustable robust transformer (ARTran) for high myopia screening of optical coherence tomography (OCT) data. Based on vision transformer, we propose anisotropic patch embedding (APE) to capture more discriminative features of high myopia. To make the model effective under variable screening conditions, we propose an adjustable class embedding (ACE) to replace the fixed class token, which changes the output to adapt to different conditions. Considering the confusion of the data at high myopia and low myopia threshold, we introduce the label noise learning strategy and propose a shifted subspace transition matrix (SST) to enhance the robustness of the model. Besides, combining the two structures proposed above, the model can provide evidence for uncertainty evaluation. The experimental results demonstrate the effectiveness and reliability of the proposed method. Code is available at: https://github.com/maxiao0234/ARTran.",https://github.com/maxiao0234/ARTran,,Computer Aided Diagnosis,Ophthalmology,Uncertainty,other,,,,,,
Adult-like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation ,"Precise brain tissue segmentation is crucial for infant development tracking and early brain disorder diagnosis. However, it remains challenging to automatically segment the brain tissues of a 6-month-old infant (isointense phase), even for manual labeling, due to inherent ongoing myelination during the first postnatal year. The intensity contrast between gray matter and white matter is extremely low in isointense MRI data. To resolve this problem, in this study, we propose a novel network with multi-phase data and multi-scale assistance to accurately segment the brain tissues of the isointense phase. Specifically, our framework consists of two main modules, \textit{i.e.}, semantics-preserved generative adversarial network (SPGAN) and Transformer-based multi-scale segmentation network (TMSN). SPAGN bi-directionally transfers the brain appearance between the isointense phase and the adult-like phase. On the one hand, the synthesized isointense phase data augments the isointense dataset. On the other hand, the synthesized adult-like images provide prior knowledge to the ambiguous tissue boundaries in the paired isointense phase data. TMSN integrates features of multi-phase image pairs in a multi-scale manner, which exploits both the adult-like phase data, with much clearer boundaries as structural prior, and the surrounding tissues, with a larger receptive field to assist the isointense data tissue segmentation. Extensive experiments on the public dataset show that our proposed framework achieves significant improvement over the state-of-the-art methods quantitatively and qualitatively.",https://github.com/SaberPRC/IsointenseBrainTissueSeg.git,https://www.nitrc.org/projects/ndarportal/,Image Segmentation,MRI,,,,,,,,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling ,"Pulmonary airway labeling identifies anatomical names for branches in bronchial trees. These fine-grained labels are critical for disease diagnosis and intra-operative navigation. Recently, various methods have been proposed for this task. However, accurate labeling of each bronchus is challenging due to the fine-grained categories and inter-individual variations. On the one hand, training a network with limited data to recognize multitudinous classes sets an obstacle to the design of algorithms. We propose to maximize the use of latent relationships by a transformer-based network. Neighborhood information is properly integrated to capture the priors in the tree structure, while a U-shape layout is introduced to exploit the correspondence between different nomenclature levels. On the other hand, individual variations cause the distribution overlapping of adjacent classes in feature space. To resolve the confusion between sibling categories, we present a novel generator that predicts the weight matrix of the classifier to produce dynamic decision boundaries between subsegmental classes. Extensive experiments performed on publicly available datasets demonstrate that our method can perform better than state-of-the-art methods.",https://github.com/EndoluminalSurgicalVision-IMR/AirwayFormer,https://github.com/yuyouxixi/airway-labeling/tree/main,Lung,Other,,,,,,,,
Alias-Free Co-Modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images ,"Cross-modality synthesis (CMS) and super-resolution (SR) have both been extensively studied with learning-based methods, which aim to synthesize desired modality images and reduce slice thickness for magnetic resonance imaging (MRI), respectively. It is also desirable to build a network for simultaneous cross-modality and super-resolution (CMSR) so as to further bridge the gap between clinical scenarios and research studies. However, these works are limited to specific fields. None of them can flexibly adapt to various combinations of resolution and modality, and perform CMS, SR, and CMSR with a single network. Moreover, alias frequencies are often treated carelessly in these works, leading to inferior detail-restoration ability. In this paper, we propose Alias-Free Co-Modulated network (AFCM) to accomplish all the tasks with a single network design. To this end, we propose to perform CMS and SR consistently with co-modulation, which also provides the flexibility to reduce slice thickness to various, non-integer values for SR. Furthermore, the network is redesigned to be alias-free under the Shannon-Nyquist signal processing framework, ensuring efficient suppression of alias frequencies. Experiments on three datasets demonstrate that AFCM outperforms the alternatives in CMS, SR, and CMSR of MR images. Our codes are available at https://github.com/zhiyuns/AFCM.",https://github.com/zhiyuns/AFCM,https://brain-development.org/ixi-dataset/,Image Reconstruction,Other,MRI,,,,,,,
ALL-IN: A Local GLobal Graph-based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment ,"The utility of machine learning models in histopathology image analysis for disease diagnosis has been extensively studied. However, efforts to stratify patient risk are relatively under-explored. While most current techniques utilize small fields of view (so-called local features) to link histopathology images to patient outcome, in this work we investigate the combination of global (i.e., contextual) and local features in a graph-based neural network for patient risk stratification. The proposed network not only combines both fine and coarse histological patterns but also utilizes their interactions for improved risk stratification. We compared the performance of our proposed model against the state-of-the-art (SOTA) techniques in histopathology risk stratification in two cancer datasets. Our results suggest that the proposed model is capable of stratifying patients into statistically significant risk groups (p < 0.01 across the two datasets) with clinical utility while competing models fail to achieve a statistical significance endpoint (p = 0.148-0.494).",https://github.com/pazadimo/ALL-IN,,Computational (Integrative) Pathology,Oncology,Histopathology,Treatment Response and Outcome/Disease Prediction,,,,,,
AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays ,"Unsupervised anomaly detection in medical images such as chest radiographs is stepping into the spotlight as it mitigates the scarcity of the labor-intensive and costly expert annotation of anomaly data. However, nearly all existing methods are formulated as a one-class classification trained only on representations from the normal class and discard a potentially significant portion of the unlabeled data. This paper focuses on a more practical setting, dual distribution anomaly detection for chest X-rays, using the entire training data, including both normal and unlabeled images. Inspired by a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions– we propose AMAE, a two-stage algorithm for adaptation of the pre-trained masked autoencoder (MAE). Starting from MAE initialization, AMAE first creates synthetic anomalies from only normal training images and trains a lightweight classifier on frozen transformer features. Subsequently, we propose an adaptation strategy to leverage unlabeled images containing anomalies. The adaptation scheme is accomplished by assigning pseudo-labels to unlabeled images and using two separate MAE based modules to model the normative and anomalous distributions of pseudo-labeled images. The effectiveness of the proposed adaptation strategy is evaluated with different anomaly ratios in an unlabeled training set. AMAE leads to consistent performance gains over competing self-supervised and dual distribution anomaly detection methods, setting the new state-of-the-art on three public chest X-ray benchmarks - RSNA, NIH-CXR, and VinDr-CXR.",,https://www.kaggle.com/c/rsna-pneumonia-detection-challenge,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Lung,Computer Aided Diagnosis,other,,,,,,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor ,"Magnetic resonance imaging (MRI) is commonly used for brain tumor segmentation, which is critical for patient evaluation and treatment planning. To reduce the labor and expertise required for labeling, weakly-supervised semantic segmentation (WSSS) methods with class activation mapping (CAM) have been proposed. However, existing CAM methods suffer from low resolution due to strided convolution and pooling layers, resulting in inaccurate predictions. In this study, we propose a novel CAM method, Attentive Multiple-Exit CAM (AME-CAM), that extracts activation maps from multiple resolutions to hierarchically aggregate and improve prediction accuracy. We evaluate our method on the BraTS 2021 dataset and show that it outperforms state-of-the-art methods.",https://github.com/windstormer/AME-CAM,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,,,,,,,,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment ,"We introduce a new AI-ready computational pathology dataset containing restained and co-registered digitized images from eight head-and-neck squamous cell carcinoma patients. Specifically, the same tumor sections were stained with the expensive multiplex immunofluorescence (mIF) assay first and then restained with cheaper multiplex immunohistochemistry (mIHC). This is a first public dataset that demonstrates the equivalence of these two staining methods which in turn allows several use cases; due to the equivalence, our cheaper mIHC staining protocol can offset the need for expensive mIF staining/scanning which requires highly-skilled lab technicians. As opposed to subjective and error-prone immune cell annotations from individual pathologists (disagreement > 50%) to drive SOTA deep learning approaches, this dataset provides objective immune and tumor cell annotations via mIF/mIHC restaining for more reproducible and accurate characterization of tumor immune microenvironment (e.g. for immunotherapy). We demonstrate the effectiveness of this dataset in three use cases: (1) IHC quantification of CD3/CD8 tumor-infiltrating lymphocytes via style transfer, (2) virtual translation of cheap mIHC stains to more expensive mIF stains, and (3) virtual tumor/immune cellular phenotyping on standard hematoxylin images. The dataset is available at https://github.com/nadeemlab/DeepLIIF.",https://github.com/nadeemlab/DeepLIIF,https://github.com/nadeemlab/DeepLIIF,Computational (Integrative) Pathology,Oncology,Histopathology,Visualization in Biomedical Imaging,,,,,,
An Anti-Biased TBSRTC-Category Aware Nuclei Segmentation Framework with A Multi-Label Thyroid Cytology Benchmark ,"The Bethesda System for Reporting Thyroid Cytopathology (TBSRTC) has been widely accepted as a reliable criterion for thyroid cytology diagnosis, where extensive diagnostic information can be deduced from the allocation and boundary of cell nuclei. However, two major challenges hinder accurate nuclei segmentation from thyroid cytology. Firstly, unbalanced distribution of nuclei morphology across different TBSRTC categories can lead to a biased model. Secondly, the insufficiency of densely annotated images results in a less generalized model. In contrast, image-wise TBSRTC labels, while containing lightweight information, can be deeply explored for segmentation guidance. To this end, we propose a TBSRTC-category aware nuclei segmentation framework (TCSegNet). To top up the small amount of pixel-wise annotations and eliminate the category preference, a larger amount of image-wise labels are taken in as the complementary supervision signal in TCSegNet. This integration of data can effectively guide the pixel-wise nuclei segmentation task with a latent global context. We also propose a semi-supervised extension of TCSegNet that leverages images with only TBSRTC-category labels. To evaluate the proposed framework and also for further cytology cell studies, we curated and elaborately annotated a multi-label thyroid cytology benchmark, collected clinically from 2019 to 2022, which will be made public upon acceptance. Our TCSegNet outperforms state-of-the-art segmentation approaches with an improvement of 2.0% Dice and 2.7% IoU; besides, the semi-supervised extension can further boost this margin. In conclusion, our study explores the weak annotations by constructing an image-wise-label-guided nuclei segmentation framework, which has the potential medical importance to assist thyroid abnormality examination. Code is available at https://github.com/Junchao-Zhu/TCSegNet.",https://github.com/Junchao-Zhu/TCSegNet,,Computational (Integrative) Pathology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,,,,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and its Application for Automatically Computing the Conformational Pathway ","Structural analysis by cryo-electron microscopy (Cryo-EM) has become well-established in the field of structural biology. Recently, cutting-edge methods have been proposed for the purpose of reconstructing either a small set of structures or a conformational pathway (continuous structural change), where a 3D density map represents the structure. However, we usually perform heavy manual labor to define the plausible pathway related to biological significance. In this study, for automatizing such manual labor, we propose a deep Auto-Encoder (AE) with a trainable prior. The AE is trained using only a set of single particle Cryo-EM images. The trained AE reconstructs the corresponding structures for the latent variables of the Cryo-EM images. The latent distribution can not only be theoretically proportional to a distribution of the structure but also consistent with the trained prior. Taking advantage of this property, we can automatically compute the pathway by only accessing the latent space as follows: i) generating a ridgeline on the latent distribution and ii) defining the conformational pathway as a sequence of the reconstructed structures along the ridgeline using the trained decoder. In our numerical experiments, we evaluate the computed pathways by comparing them with existing ones that were  manually determined by other researchers, and confirm that they are sufficiently consistent.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Reconstruction,Visualization in Biomedical Imaging,,,,,,,
An automated pipeline for quantitative T2* fetal body MRI and segmentation at low field ,"Fetal Magnetic Resonance Imaging at low field strengths is emerging as an exciting direction in perinatal health. Clinical low field (0.55T) scanners are beneficial for fetal imaging due to their reduced susceptibility-induced artefacts, increased T2* values, and wider bore (widening access for the increasingly obese pregnant population). However, the lack of standard automated image processing tools such as segmentation and reconstruction hampers wider clinical use. In this study, we introduce a semi-automatic pipeline using quantitative MRI for the fetal body at low field strength resulting in fast and detailed quantitative T2* relaxometry analysis of all major fetal body organs. Multi-echo dynamic sequences of the fetal body were acquired and reconstructed into a single high-resolution volume using deformable slice-to-volume reconstruction, generating both structural and quantitative T2* 3D volumes. A neural network trained using a semi-supervised approach was created to automatically segment these fetal body 3D volumes into ten different organs (resulting in dice values > 0.74 for 8 out of 10 organs). The T2* values revealed a strong relationship with GA in the lungs, liver, and kidney parenchyma (R^2 >0.5). This pipeline was used successfully for a wide range of GAs (17-40 weeks), and is robust to motion artefacts. Low field fetal MRI can be used to perform advanced MRI analysis, and is a viable option for clinical scanning.",https://github.com/SVRTK/Fetal-T2star-Recon,,Fetal Imaging,Image Reconstruction,,,,,,,,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis ,"Multi-sequence MRI is valuable in clinical settings for reliable diagnosis and treatment prognosis, but some sequences may be unusable or missing for various reasons. To address this issue, MRI synthesis is a potential solution. Recent deep learning-based methods have achieved good performance in combining multiple available sequences for missing sequence synthesis. Despite their success, these methods lack the ability to quantify the contributions of different input sequences and estimate region-specific quality in generated images, making it hard to be practical. Hence, we propose an explainable task-specific synthesis network, which adapts weights automatically for specific sequence generation tasks and provides interpretability and reliability from two sides: (1) visualize and quantify the contribution of each input sequence in the fusion stage by a trainable task-specific weighted average module; (2) highlight the area the network tried to refine during synthesizing by a task-specific attention module. We conduct experiments on the BraTS2021 dataset of 1251 subjects, and results on arbitrary sequence synthesis indicate that the proposed method achieves better performance than the state-of-the-art methods. Our code is available at https://github.com/fiy2W/mri_seq2seq.",https://github.com/fiy2W/mri_seq2seq,http://braintumorsegmentation.org/,Image Reconstruction,Neuroimaging - Brain Development,Interpretability / Explainability,MRI,,,,,,
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment ,"One of the hallmark symptoms of Parkinson’s Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT.",https://github.com/favour-nerrise/xGW-GAT,,Attention models,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,
An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography ,"Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and robustness. Finally, the study presents visualizations that explain the results of the analysis and highlights the potential of attention mechanism for improving the efficiency and effectiveness of EEG data analysis in a variety of applications.",,,EEG/ECG,Attention models,,,,,,,,
An Unsupervised Multispectral Image Registration Network for Skin Diseases ,"Multispectral imaging has a broad, promising and advantageous application prospect in the diagnosis of skin diseases. However, there are inherent deviations such as rigid or non-rigid deformation among multispectral images (MSI), which makes accurate and robust registration algorithms desirable to extract reliable multispectral features. Existing registration algorithms are susceptible to significant and nonlinear amplitude differences and geometric distortions among MSI, resulting in an unsatisfactory estimation of the registration field (RF). In this study, we propose an end-to-end multispectral image registration (MSIR) network with unsupervised learning for human skin disease diagnosis. First, we propose a basic adjacent-band pair registration (ABPR) model to obtain the corresponding RFs through simultaneously modeling a series of image pairs from adjacent bands. Second, we introduce a multispectral attention module (MAM) for extraction and adaptive weight allocation of the high-level pathological features of multiple MSI pairs. Third, we design a registration field refinement module (RFRM) to rectify and reconstruct a general RF solution. Fourth, we propose an unsupervised center-toward registration loss function, combining a similarity loss for features in the frequency domain and a smoothness loss for RF. In addition, we built a MSI dataset of multi-type skin diseases and conducted extensive experiments. The results show that our method not only outperforms state-of-the-art methods on MSI registration task, but also contributes to the subsequent task of benign and malignant disease classification.",https://github.com/SH-Diao123/MSIR,,Image Registration,Dermatology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps ,"Skin flap is a common technique used by surgeons to close the wound after the resection of a lesion. Careful planning of a skin flap procedure is essential for the most optimal functional and aesthetic outcome. However, currently surgical planning is mostly done based on surgeons’ experience and preferences. In this paper, we introduce a finite element method (FEM) simulation that is used to make objective recommendations of the most optimal flap orientation. Rhomboid flap is chosen as the main flap type of interest because it is a very versatile flap. We focus on evaluating suture forces required to close a wound as large tension around it could lead to complications. We model the skin as an anisotropic material where we use a single direction to represent the course of relaxed skin tension lines (RSTLs). We conduct a thorough search by rotating the rhomboid flap in small increments (1°-10°) and find the orientation that minimizes the suture force. We repeat the setup with different material properties and the recommendation is compared with textbook knowledge. Our simulation is validated with minimal error in comparison with other existing simulations. Our simulation shows to minimize suture force, the existing textbook knowledge recommendation needs to be further rotated by 15°-20°.",,,Surgical Planning and Simulation,Dermatology,Guided Interventions and Surgery,Video,Surgical Visualization and Mixed/Augmented/Virtual Reality,Visualization in Biomedical Imaging,,,,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model ,"Accurate localization of anatomical landmarks has a critical role in clinical diagnosis, treatment planning, and research. Most existing deep learning methods for anatomical landmark localization rely on heatmap regression-based learning, which generates label representations as 2D Gaussian distributions centered at the labeled coordinates of each of the landmarks and integrates them into a single spatial resolution heatmap. However, the accuracy of this method is limited by the resolution of the heatmap, which restricts its ability to capture finer details. In this study, we introduce a multiresolution heatmap learning strategy that enables the network to capture semantic feature representations precisely using multiresolution heatmaps generated from the feature representations at each resolution independently, resulting in improved localization accuracy. Moreover, we propose a novel network architecture called hybrid transformer-CNN (HTC), which combines the strengths of both CNN and vision transformer models to improve the network’s ability to effectively extract both local and global representations. 
Extensive experiments demonstrated that our approach outperforms state-of-the-art deep learning-based anatomical landmark localization networks on the numerical XCAT 2D projection images and two public X-ray landmark detection benchmark datasets. Our code is available at https://github.com/seriee/Multiresolution-HTC.git.",https://github.com/seriee/Multiresolution-HTC.git,,Computer Aided Diagnosis,Attention models,CT,Visualization in Biomedical Imaging,,,,,,
Anatomical-aware Point-Voxel Network for Couinaud Segmentation in Liver CT ,"Accurately segmenting the liver into anatomical segments is crucial for surgical planning and lesion monitoring in CT imaging. However, this is a challenging task as it is defined based on vessel structures, and there is no intensity contrast between adjacent segments in CT images.
In this paper, we propose a novel point-voxel fusion framework to address this challenge. Specifically, we first segment the liver and vessels from the CT image, and generate 3D liver point clouds and voxel grids embedded with vessel structure prior.
Then, we design a multi-scale point-voxel fusion network to capture the anatomical structure and semantic information of the liver and vessels, respectively, while also increasing important data access through vessel structure prior. Finally, the network outputs the classification of Couinaud segments in the continuous liver space, producing a more accurate and smooth 3D Couinaud segmentation mask. Our proposed method outperforms several state-of-the-art methods, both point-based and voxel-based, as demonstrated by our experimental results on two public liver datasets. Code, datasets, and models are released at https://github.com/xukun-zhang/Couinaud-Segmentation.",,,Image Segmentation,Abdomen,CT,,,,,,,
Anatomy-Driven Pathology Detection on Chest X-rays ,"Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions.
However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. 
We therefore propose  anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. 
We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.",https://github.com/philip-mueller/adpd,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computational Anatomy and Physiology,Computer Aided Diagnosis,Interpretability / Explainability,Transfer learning,,,,,
Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection ,"Data augmentation (DA) is a key factor in medical image analysis, such as in prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art computer-aided diagnosis systems still rely on simplistic spatial transformations to preserve the pathological label post transformation. However, such augmentations do not substantially increase the organ as well as tumor shape variability in the training set, limiting the model’s ability to generalize to unseen cases with more diverse localized soft-tissue deformations. We propose a new anatomy-informed transformation that leverages information from adjacent organs to simulate typical physiological deformations of the prostate and generates unique lesion shapes without altering their label. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. We demonstrate the effectiveness of our augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a state-of-the-art method for PCa detection with different augmentation settings.",https://github.com/MIC-DKFZ/anatomy_informed_DA,,Oncology,Computational Anatomy and Physiology,Computer Aided Diagnosis,Other,MRI,,,,,
Aneurysm Pose Estimation with Deep Learning ,"The diagnosis of unruptured intracranial aneurysms from Magnetic Resonance Angiography (MRA) images is a challenging clinical problem that is extremely difficult to automate. We propose to go beyond the mere detection of each aneurysm and also estimate its size and the orientation of its main axis for an immediate visualization in appropriate reformatted cut planes. To address this issue, and inspired by the idea behind YOLO architecture, a novel one-stage deep learning approach is described to simultaneously estimate the localization, size and orientation of each aneurysm in 3D time-of-flight MRA images. It combines fast and approximate annotation, data sampling and generation to tackle the class imbalance problem, and a cosine similarity loss to optimize the orientation. We evaluate our approach on two large datasets containing 416 patients with 317 aneurysms using a 5-fold cross-validation scheme. Our method achieves a median localization error of 0.48mm and a median 3D orientation error of 12.27 degrees, demonstrating an accurate localization of aneurysms and an orientation estimation that comply with clinical practice. Further evaluation is performed in a more classical detection setting to compare with state-of-the-art nnDetecton and nnUnet methods. Competitive performance is reported with an average precision of 76.60%, a sensitivity score of 82.93%, and 0.44 false positives per case.",https://gitlab.inria.fr/yassis/DeepAnePose,https://openneuro.org/datasets/ds003949/versions/1.0.1,Data Efficient Learning,Neuroimaging - Others,Vascular,Computer Aided Diagnosis,MRI,,,,,
AngioMoCo: Learning-based Motion Correction in Cerebral Digital Subtraction Angiography ,"Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging technique for visualizing blood flow and guiding endovascular treatments. The quality of DSA is often negatively impacted by body motion during acquisition, leading to decreased diagnostic value. Traditional methods address motion correction based on non-rigid registration and employ sparse key points and non-rigidity penalties to limit vessel distortion, which is time-consuming. Recent methods alleviate subtraction artifacts by predicting the subtracted frame from the corresponding unsubtracted frame, but do not explicitly compensate for motion-induced misalignment between frames. This hinders the serial evaluation of blood flow, and often causes undesired vasculature and contrast flow alterations, leading to impeded usability in clinical practice. To address these limitations, we present AngioMoCo, a learning-based framework that generates motion-compensated DSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction and motion correction, enabling differentiation between patient motion and intensity changes caused by contrast flow. This strategy improves registration quality while being orders of magnitude faster than iterative elastix-based methods. We demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN Registry) of clinically acquired angiographic images through comprehensive qualitative and quantitative analyses. AngioMoCo produces high-quality motion-compensated DSA, removing while preserving contrast flow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.",https://github.com/RuishengSu/AngioMoCo,,Vascular,Image Registration,Image Segmentation,Guided Interventions and Surgery,Visualization in Biomedical Imaging,,,,,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models ,"A major challenge in the segmentation of medical images is the large inter- and intra-observer variability in annotations provided by multiple experts. To address this challenge, we propose a novel method for multi-expert prediction using diffusion models.  Our method leverages the diffusion-based approach to incorporate the information from multiple annotations and fuse them into a unified segmentation map that reflects the consensus of multiple experts. We evaluate the performance of our method on several datasets of medical segmentation that were annotated by multiple experts and compare it with the state-of-the-art methods. Our results demonstrate the effectiveness and robustness of the proposed method.",https://github.com/tomeramit/Annotator-Consensus-Prediction,,Image Segmentation,,,,,,,,,
Anti-Adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation ,"Modern deep learning methods for semantic segmentation require labor-intensive labeling for large-scale datasets with dense pixel-level annotations. Recent data augmentation methods such as dropping, mixing image patches, and adding random noises suggest effective ways to address the labeling issues for natural images. However, they can only be restrictively applied to medical image segmentation as they carry risks of distorting or ignoring the underlying clinical information of local regions of interest in an image. In this paper, we propose a novel data augmentation method for medical image segmentation without losing the semantics of the key objects (e.g., polyps). This is achieved by perturbing the objects with quasi-imperceptible adversarial noises and training a network to expand discriminative regions with a guide of anti-adversarial noises. Such guidance can be realized by a consistency regularization between the two contrasting data, and the strength of regularization is automatically and adaptively controlled considering their prediction uncertainty. Our proposed method significantly outperforms various existing methods with high sensitivity and Dice scores and extensive experiment results with multiple backbones on two datasets validate its effectiveness.",,,Image Segmentation,Active Learning,Data Efficient Learning,,,,,,,
AR2T: Advanced Realistic Rendering Technique for Biomedical Volumes ,"Three-dimensional (3D) rendering of biomedical volumes can be used to illustrate the diagnosis to patients, train inexperienced clinicians, or facilitate surgery planning for experts. The most realistic visualization can be achieved by the Monte-Carlo path tracing (MCPT) rendering technique which is based on the physical transport of light. However, this technique applied to biomedical volumes has received relatively little attention, because, naively implemented, it does not allow to interact with the data. In this paper, we present our application of MCPT to the biomedical volume rendering – Advanced Realistic Rendering Technique (AR2T), in an attempt to achieve more realism and increase the level of detail in data representation. The main result of our research is a practical framework that includes different visualization techniques: iso-surface rendering, direct volume rendering (DVR) combined with local and global illumination, maximum intensity projection (MIP), and AR2T. The framework allows interaction with the data in high quality for the deterministic algorithms, and in low quality for the stochastic AR2T. A high-quality AR2T image can be generated on user request; the quality improves in real-time, and the process is stopped automatically on the algorithm convergence, or by user, when the desired quality is achieved. The framework enables direct comparison of different rendering algorithms, i.e., utilizing the same view/light position and transfer functions. It therefore can be used by medical experts for immediate one-to-one visual comparison between different data representations in order to collect feedback about the usefulness of the realistic 3D visualization in clinical environment.",,https://www.kaggle.com/datasets/imaginar2t/cbctdata,Visualization in Biomedical Imaging,Image Reconstruction,Guided Interventions and Surgery,Interventional Imaging Systems,CT,Surgical Planning and Simulation,,,,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images ,"Segmentation of the infected areas of the lung is essential for quantifying the severity of lung disease like pulmonary infections. Existing medical image segmentation methods are almost uni-modal methods based on image. However, these image-only methods tend to produce inaccurate results unless trained with large amounts of annotated data. To overcome this challenge, we propose a language-driven segmentation method that uses text prompt to improve to the segmentation result. Experiments on the QaTa-COV19 dataset indicate that our method improves the Dice score by 6.09\% at least compared to the uni-modal methods. Besides, our extended study reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required.",https://github.com/Junelin2333/LanGuideMedSeg-MICCAI2023,https://github.com/HUANGLIZI/LViT,Image Segmentation,Lung,Other,Text (clinical/radiology reports),,,,,,
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models ,"Colonoscopy analysis, particularly automatic polyp segmentation and detection, is essential for assisting clinical diagnosis and treatment. However, as medical image annotation is labour- and resource-intensive, the scarcity of annotated data limits the effectiveness and generalization of existing methods. Although recent research has focused on data generation and augmentation to address this issue, the quality of the generated data remains a challenge, which limits the contribution to the performance of subsequent tasks. Inspired by the superiority of diffusion models in fitting data distributions and generating high-quality data, in this paper, we propose an Adaptive Refinement Semantic Diffusion Model (ArSDM) to generate colonoscopy images that benefit the downstream tasks. Specifically, ArSDM utilizes the ground-truth segmentation mask as a prior condition during training and adjusts the diffusion loss for each input according to the polyp/background size ratio. Furthermore, ArSDM incorporates a pre-trained segmentation model to refine the training process by reducing the difference between the ground-truth mask and the prediction mask. Extensive experiments on segmentation and detection tasks demonstrate the generated data by ArSDM could significantly boost the performance of baseline methods.",https://github.com/DuYooho/ArSDM,,Other,Image Segmentation,other,,,,,,,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models ,"Histological whole slide images (WSIs) can be usually compromised by artifacts, such as tissue folding and bubbles, which will increase the examination difficulty for both pathologists and Computer-Aided Diagnosis (CAD) systems. Existing approaches to restoring artifact images are confined to Generative Adversarial Networks (GANs), where the restoration process is formulated as an image-to-image transfer. Those methods are prone to suffer from mode collapse and unexpected mistransfer in the stain style, leading to unsatisfied and unrealistic restored images. Innovatively, we make the first attempt at a denoising diffusion probabilistic model for histological artifact restoration, namely ArtiFusion.
Specifically, ArtiFusion formulates the artifact region restoration as a gradual denoising process, and its training relies solely on artifact-free images to simplify the training complexity. Furthermore, to capture local-global correlations in the regional artifact restoration, a novel Swin-Transformer denoising architecture is designed, along with a time token scheme. Our extensive evaluations demonstrate the effectiveness of ArtiFusion as a pre-processing method for histology analysis, which can successfully preserve the tissue structures and stain style in artifact-free regions during the restoration. Code is available at https://github.com/zhenqi-he/ArtiFusion.",https://github.com/zhenqi-he/ArtiFusion,https://camelyon17.grand-challenge.org,Histopathology,,,,,,,,,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation ,"Automatic tissue segmentation of fetal brain images is essential for the quantitative analysis of prenatal neurodevelopment. However, producing voxel-level annotations of fetal brain imaging is time-consuming and expensive. To reduce labeling costs, we propose a practical unsupervised domain adaptation (UDA) setting that adapts the segmentation labels of high-quality fetal brain atlases to unlabeled fetal brain MRI data from another domain. To address the task, we propose a new UDA framework based on Appearance and Structure Consistency, named ASC. We adapt the segmentation model to the appearances of different domains by constraining the consistency before and after a frequency-based image transformation, which is to swap the appearance between brain MRI data and atlases. Consider that even in the same domain, the fetal brain images of different gestational ages could have significant variations in the anatomical structures. To make the model adapt to the structural variations in the target domain, we further encourage prediction consistency under different structural perturbations. Extensive experiments on FeTA 2021 benchmark demonstrate the effectiveness of our ASC in comparison to registration-based, semi-supervised learning-based, and existing UDA-based methods.",,,Computer Aided Diagnosis,Fetal Imaging,MRI,,,,,,,
ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising ,"While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.",https://github.com/hao1635/ASCON,,Image Reconstruction,Attention models,Interpretability / Explainability,Other,CT,,,,,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling ,"Identifying and detecting a set of objects that conform to a structured pattern, but may also have misaligned, missing, or duplicated elements is a difficult task. Dental structures serve as a real-world example of such objects, with high variability in their shape, alignment, and number across different individuals. This study introduces an assignment theory-based approach for recognizing objects based on their positional inter-dependencies. We developed a distance-based anatomical model of teeth consisting of pair-wise displacement vectors and relative positional scores. The dental model was transformed into a cost function for a bipartite graph using a convolutional neural network (CNN). The graph connected candidate tooth labels to the correct tooth labels. We re-framed the problem of determining the optimal tooth labels for a set of candidate labels into the problem of assigning jobs to workers. This approach established a theoretical connection between our task and the field of assignment theory. To optimize the learning process for specific output requirements, we incorporated a loss term based on assignment theory into the objective function. We used the Hungarian method to assign greater importance to the costs returned on the optimal assignment path. The database used in this study consisted of 1200 dental meshes, which included separate upper and lower jaw meshes, collected from 600 patients. The testing set was generated by an indirect segmentation pipeline based on the 3D U-net architecture. To evaluate the ability of the proposed approach to handle anatomical anomalies, we introduced artificial tooth swaps, missing and double teeth. The identification accuracies of the candidate labels were 0.887 for the upper jaw and 0.888 for the lower jaw. The optimal labels predicted by our method improved the identification accuracies to 0.991 for the upper jaw and 0.992 for the lower jaw.",,,Other,Image Segmentation,other,,,,,,,
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation ,"Aleatoric uncertainty estimation is a critical step in medical image segmentation. Most techniques for estimating aleatoric uncertainty for segmentation purposes assume a Gaussian distribution over the neural network’s logit value modeling the uncertainty in the predicted class. However, in many cases, such as image segmentation, there is no uncertainty about the presence of a specific structure, but rather about the precise outline of that structure. For this reason, we explicitly model the location uncertainty  by redefining the conventional per-pixel segmentation task as a contour regression problem.  This allows for modeling the uncertainty of contour points using a more appropriate multivariate distribution.  Additionally, as contour uncertainty may be asymmetric, we use a multivariate skewed Gaussian distribution. In addition to being directly interpretable, our uncertainty estimation method outperforms previous methods on three datasets using two different image modalities.",https://github.com/ThierryJudge/contouring-uncertainty,https://www.creatis.insa-lyon.fr/Challenge/camus/,Uncertainty,Cardiac,Lung,Image Segmentation,CT,Ultrasound,,,,
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease using Multimodal Imaging Genetics ,"Integration of imaging genetics data provides unprecedented opportunities for revealing biological mechanisms underpinning diseases and certain phenotypes. In this paper, a new model called attentive deep canonical correlation analysis (ADCCA) is proposed for the diagnosis of Alzheimer’s disease using multimodal brain imaging genetics data. ADCCA combines the strengths of deep neural networks, attention mechanisms, and canonical correlation analysis to integrate and exploit the complementary information from multiple data modalities. This leads to improved interpretability and strong multimodal feature learning ability. The ADCCA model is evaluated using the ADNI database with three imaging modalities (VBM-MRI, FDG-PET, and AV45-PET) and genetic SNP data. The results indicate that this approach can achieve outstanding performance and identify meaningful biomarkers for Alzheimer’s disease diagnosis. To promote reproducibility, the code has been made publicly available at https://github.com/rongzhou7/ADCCA.",https://github.com/rongzhou7/ADCCA,,Attention models,Imaging Biomarkers,Interpretability / Explainability,MRI,PET/SPECT,Treatment Response and Outcome/Disease Prediction,,,,
atTRACTive: Semi-automatic white matter tract segmentation using active learning ,"Accurately identifying white matter tracts in medical images is essential for various applications, including surgery planning and tract-specific analysis. Supervised machine learning models have reached state-of-the-art solving this task automatically. However, these models are primarily trained on healthy subjects and struggle with strong anatomical aberrations, e.g. caused by brain tumors. This limitation makes them unsuitable for tasks such as preoperative planning, wherefore time-consuming and challenging manual delineation of the target tract is typically employed. We propose semi-automatic entropy-based active learning for quick and intuitive segmentation of white matter tracts from whole-brain tractography consisting of millions of streamlines. The method is evaluated on 21 openly available healthy subjects from the Human Connectome Project and an internal dataset of ten neurosurgical cases. With only a few annotations, the proposed approach enables segmenting tracts on tumor cases comparable to healthy subjects (dice = 0.71), while the performance of automatic methods, like TractSeg dropped substantially (dice = 0.34) in comparison to healthy subjects. The method is implemented as a prototype named atTRACTive in the freely available softwareanonymous. Manual experiments on tumor data showed higher efficiency due to lower segmentation times  compared to traditional ROI-based segmentation.",https://github.com/MIC-DKFZ/atTRACTive_simulations,https://zenodo.org/record/1477956,Neuroimaging - DWI and Tractography,Active Learning,MRI,,,,,,,
AUA-dE: An adaptive uncertainty guided attention for diffusion MRI models estimation ,"Diffusion MRI (dMRI) is a well-established tool for probing tissue microstruc-ture properties. However, advanced dMRI models commonly have multiple compartments that are highly nonlinear and complex, and also require dense sampling in q-space. These problems have been investigated using deep learning based techniques. In existing approaches, the labels were calculated from the fully sampled q-space as the ground truth. However, for some of the dMRI models, dense sampling is hard to achieve due to the long scan time, and the low signal-to-noise ratio could lead to noisy labels that make it hard for the network to learn the relationship between the signals and labels. A good example is the time-dependent dMRI (TD-dMRI), which captures the microstructural size and transmembrane exchange by measuring the signal at varying diffusion times but requires dense sampling in both q-space and t-space. To overcome the noisy label problem and accelerate the acquisition, in this work, we proposed an adaptive uncertainty guided attention for diffusion MRI models estimation (AUA-dE) to estimate the microstructural parameters in the TD-dMRI model. We evaluated our proposed method with three different downsampling strategies, including q-space downsampling, t-space downsampling, and q-t space downsampling, on two different datasets: a simulation dataset and an experimental dataset from normal and injured rat brains. Our proposed method achieved the best performance compared to the previous q-space learning methods and the conventional optimization methods in terms of accuracy and robustness.",,,MRI,Neuroimaging - DWI and Tractography,Other,,,,,,,
Automated CT Lung Cancer Screening Workflow using 3D Camera ,"Despite recent developments in CT planning that enabled automation in patient positioning, time-consuming scout scans are still needed to compute dose profile and ensure the patient is properly positioned. In this paper, we present a novel method which eliminates the need for scout scans in CT lung cancer screening by estimating patient scan range, isocenter, and Water Equivalent Diameter (WED) from 3D camera images. We achieve this task by training an implicit generative model on over 60,000 CT scans and introduce a novel approach for updating the prediction using real-time scan data. We demonstrate the effectiveness of our method on a testing set of 110 pairs of depth data and CT scan, resulting in an average error of 5mm in estimating the isocenter, 13mm in determining the scan range, 10mm and 16mm in estimating the AP and lateral WED respectively. The relative WED error of our method is 4%, which is well within the International Electrotechnical Commission (IEC) acceptance criteria of 10%.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Lung,CT,,,,,,,
Automatic Bleeding Risk Rating System of Gastric Varices ,"An automated bleeding risk rating system of gastric varices
(GV) aims to predict the bleeding risk and severity of GV, in order to assist endoscopists in diagnosis and decrease the mortality rate of patients
with liver cirrhosis and portal hypertension. However, since the lack of
commonly accepted quantification standards, the risk rating highly relies
on the endoscopists’ experience and may vary a lot in different application scenarios. In this work, we aim to build an automatic GV bleeding
risk rating method that can learn from experienced endoscopists and
provide stable and accurate predictions. Due to the complexity of GV
structures with large intra-class variation and small inter-class varia-
tions, we found that existing models perform poorly on this task and
tend to lose focus on the important varices regions. To solve this issue,
we constructively introduce the segmentation of GV into the classification framework and propose the region-constraint module and cross-
region attention module for better feature localization and to learn the
correlation of context information. We also collect a GV bleeding risks
rating dataset (GVbleed) with 1678 gastroscopy images from 411 patients that are jointly annotated in three levels of risks by senior clinical
endoscopists. The experiments on our collected dataset show that our
method can improve the rating accuracy by nearly 5% compared to the
baseline. Codes and dataset will be available at https://github.com/LuyueShi/gastric-varices.",https://github.com/LuyueShi/gastric-varices,,Computer Aided Diagnosis,Treatment Response and Outcome/Disease Prediction,,,,,,,,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations ,"Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive
learning approach to automatically retrieve similar ultrasound muscle
views at different scan times. Three different models were compared using
data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7%±0.24% error in cross-sectional area. Furthermore, a user study survey confirmed the efficacy of our model for muscle view retrieval.",https://github.com/hamidehkerdegari/Muscle-view-retrieval,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,Other,Ultrasound,,,,,,
Automatic Segmentation of Internal Tooth Structure from CBCT Images using Hierarchical Deep Learning ,"Accurate segmentation of teeth is crucial for effective treatment planning. Previous approaches attempted to segment a tooth as a whole, which has limitations because most treatments involve internal structures of teeth. In this paper, we propose fully automated segmentation of internal tooth structure, including enamel, dentin, and pulp, which is the first attempt to the best of our knowledge. The task is challenging,  because a total of 96 classes of tooth structures need to be identified from a CBCT image. We design a 3-stage process of coarse-to-fine segmentation of tooth structures without compromising the original resolution. We propose Dual-Hierarchy U-Net (DHU-Net) in order to capture hierarchical structures of teeth, and to effectively fuse encoder and decoder features from higher and lower hierarchies. Experiments demonstrate that our method outperforms state-of-the-art methods in both tasks of segmenting the whole tooth and internal tooth structure.",https://github.com/Saeeeae/Internal-Tooth-Segmentation,,Image Segmentation,Other,CT,,,,,,,
Automatic Surgical Reconstruction for Orbital Blow-out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network ,"Orbital blow-out fracture (OBF) is a complex disease that can cause severe dam-age to the orbital wall. The ultimate means of treating this disease is orbital wall repair surgery, where automatic reconstruction of the orbital wall is a crucial step. However, accurately reconstructing the orbital wall is a great challenge due to the collapse, damage, fracture, and deviation in OBF. Manual or semi-automatic re-construction methods used in clinics also suffer from poor accuracy and low effi-ciency. Therefore, we propose a symmetric prior anatomical knowledge (SPAK)-guided generative adversarial network (GAN) for automatic reconstruction of the orbital wall in OBF. Above all, a spatial transformation-based SPAK generation method is proposed to generate prior anatomy that guides the reconstruction of the fractured orbital wall. Next, the generated SPAK is introduced into the GAN network, to guide the network towards automatic reconstruction of the fractured orbital wall. Additionally, a multi-function combination supervision strategy is proposed to further improve the network reconstruction performance. Our eval-uation on the test set showed that the proposed network achieved a Dice similari-ty coefficient (DSC) of 92.35±2.13% and a 95% Hausdorff distance of 0.59±0.23mm, which is significantly better than other networks. The proposed network is the first method to implement the automatic reconstruction of OBF, ef-fectively improving the reconstruction accuracy and efficiency of the fractured orbital wall. In the future, it has a promising application prospect in the surgical planning of OBF.",,,Surgical Planning and Simulation,Ophthalmology,,,,,,,,
B-Cos Aligned Transformers Learn Human-Interpretable Features ,"Vision Transformers (ViTs) and Swin Transformers (Swin) are currently state-of-the-art in computational pathology. However, domain experts are still reluctant to use these models due to their lack of interpretability. This is not surprising, as critical decisions need to be transparent and understandable. The most common approach to understanding transformers is to visualize their attention. However, attention maps of ViTs are often fragmented, leading to unsatisfactory explanations. Here, we introduce a novel architecture called the B-cos Vision Transformer (BvT) that is designed to be more interpretable. It replaces all linear transformations with the B-cos transform to promote weight-input alignment. In a blinded study, medical experts clearly ranked BvTs above ViTs, suggesting that our network is better at capturing biomedically relevant structures. This is also true for the B-cos Swin Transformer (Bwin). Compared to the Swin Transformer, it even improves the F1-score by up to 4.7% on two public datasets.",,https://zenodo.org/record/1214456,Computational (Integrative) Pathology,Interpretability / Explainability,Histopathology,Microscopy,,,,,,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation ,"Medical image segmentation is a challenging task with inherent ambiguity and high uncertainty attributed to factors such as unclear tumor boundaries and multiple plausible annotations. The accuracy and diversity of segmentation masks are both crucial for providing valuable references to radiologists in clinical practice. While existing diffusion models have shown strong capacities in various visual generation tasks, it is still challenging to deal with discrete masks in segmentation. To achieve accurate and diverse medical image segmentation masks, we propose a novel conditional Bernoulli Diffusion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, resulting in more accurate segmentation masks. Second, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernoulli noise and intermediate latent variables multiple times to produce a range of diverse segmentation masks, which can highlight salient regions of interest that can serve as a valuable reference for radiologists. In addition, our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion, thereby speeding up the segmentation process. Extensive experimental results on two medical image segmentation datasets with different modalities demonstrate that our BerDiff outperforms other recently published state-of-the-art methods. Source code is made available at https://github.com/takimailto/BerDiff.",https://github.com/takimailto/BerDiff,,Image Segmentation,Lung,Neuroimaging - Others,Oncology,Uncertainty,CT,MRI,,,
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding ,"Under the framework of network-based neurodegeneration, brain functional connectome (FC)-based Graph Neural Networks (GNN) have emerged as a valuable tool for the diagnosis and prognosis of neurodegenerative diseases such as Alzheimer’s disease (AD). However, these models are tailored for brain FC at a single time point instead of characterizing FC trajectory. Discerning how FC evolves with disease progression, particularly at the predementia stages such as cognitively normal individuals with amyloid deposition or individuals with mild cognitive impairment (MCI), is crucial for delineating disease spreading patterns and developing effective strategies to slow down or even halt disease advancement. In this work, we proposed the first interpretable framework for brain FC trajectory embedding with application to neurodegenerative disease diagnosis and prognosis, namely Brain Tokenized Graph Transformer (Brain TokenGT). It consists of two modules: 1) Graph Invariant and Variant Embedding (GIVE) for generation of node and spatio-temporal edge embeddings, which were tokenized for downstream processing; 2) Brain Informed Graph Transformer Readout (BIGTR) which augments previous tokens with trainable type identifiers and non- trainable node identifiers and feeds them into a standard transformer encoder to readout. We conducted extensive experiments on two public longitudinal fMRI datasets of the AD continuum for three tasks, including differentiating MCI from controls, predicting dementia conversion in MCI, and classification of amyloid positive or negative cognitively normal individuals. Based on brain FC trajectory, the proposed Brain TokenGT approach outperformed all the other benchmark models and at the same time provided excellent interpretability.",https://github.com/ZijianD/Brain-TokenGT.git,https://adni.loni.usc.edu/,Computer Aided Diagnosis,Neuroimaging - Functional Brain Networks,Attention models,MRI,,,,,,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data ,"The modeling of the interaction between brain structure and function using deep learning techniques has yielded remarkable success in identifying potential biomarkers for different clinical phenotypes and brain diseases. However, most existing studies focus on one-way mapping, either projecting brain function to brain structure or inversely. This type of unidirectional mapping approach is limited by the fact that it treats the mapping as a one-way task and neglects the intrinsic unity between these two modalities. Moreover, when dealing with the same biological brain, mapping from structure to function and from function to structure yields dissimilar outcomes, highlighting the likelihood of bias in one-way mapping. To address this issue, we propose a novel bidirectional mapping model, named Bidirectional Mapping with Contrastive Learning (BMCL), to reduce the bias between these two unidirectional mappings via ROI-level contrastive learning. We evaluate our framework on clinical phenotype and neurodegenerative disease predictions using two publicly available datasets (HCP and OASIS).
The experimental results demonstrate the superiority of our model compared to several state-of-the-art methods.",https://github.com/FlynnYe/BMCL,,Other,Neuroimaging - Functional Brain Networks,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,Treatment Response and Outcome/Disease Prediction,,,,
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior ,"Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples with minimal photo-damage, experiences “defocused” image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dual-view imaging is particularly helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the distinctive image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes’ Theorem, where (i) the adverse effect of light scattering onto focus measures is included within Likelihood; and (ii) the spatial consistency regarding focus-defocus is imposed in Prior. The expectation-maximum algorithm, aided by a reliable initialization, is then adopted to estimate the focus-defocus boundary. Competitive experimental results show that BigFUSE is the first dual-view LSFM fuser that is able to exclude structured artifacts when fusing information, highlighting its abilities of automatic image fusion.",,,Microscopy,Image Reconstruction,,,,,,,,
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation ,"Cell segmentation plays a critical role in diagnosing various cancers. Although deep learning techniques have been widely investigated, the enormous types and diverse appearances of histopathological cells still pose significant challenges for clinical applications. Moreover, data protection policies in different clinical centers and hospitals limit the training of data-dependent deep models. In this paper, we present a novel framework for cross-tissue domain adaptative cell segmentation without access both source domain data and model parameters, namely Multi-source Black-box Domain adaptation (MBDA). Given the target domain data, our framework can achieve the cell segmentation based on knowledge distillation, by only using the outputs of models trained on multiple source domain data. Considering the domain shift cross different pathological tissues, predictions from the source models may not reliable, where the noise labels can limit the training of the target model. To address this issue, we propose two practical approaches for weighting knowledge from the multi-source model predictions and filtering out noisy predictions. First, we assign pixel-level weights to the outputs of source models to reduce uncertainty during knowledge distillation. Second, we design a pseudo-label cutout and selection strategy for these predictions to facilitate the knowledge distillation from local cell to global pathological images. Experimental results on four types of pathological tissues demonstrate that our proposed black-box domain adaptation approach can achieve comparable and even better performance in comparison with state-of-the-art white-box approaches.",,,Transfer learning,Computational (Integrative) Pathology,Image Segmentation,Model Generalizability / Federated Learning,Histopathology,,,,,
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers ,"Breast ultrasound videos contain richer information than ultrasound images, therefore it is more meaningful to develop video models for this diagnosis task. However, the collection of ultrasound video datasets is much harder. In this paper, we explore the feasibility of enhancing the performance of ultrasound video classification using the static image dataset. To this end, we propose KGA-Net and coherence loss. The KGA-Net adopts both video clips and static images to train
the network. The coherence loss uses the feature centers generated by the static images to guide the frame attention in the video model. Our KGA-Net boosts the performance on the public BUSV dataset by a large margin. The visualization results of frame attention prove the explainability of our method. We release the code and model weights in https://github.com/PlayerSAL/KGA-Net.",https://github.com/PlayerSAL/KGA-Net,,Computer Aided Diagnosis,Breast,Attention models,Interpretability / Explainability,Ultrasound,Video,,,,
Boundary Difference Over Union Loss For Medical Image Segmentation ,"Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.",https://github.com/sunfan-bvb/BoundaryDoULoss,https://www.synapse.org/#!Synapse:syn3193805/wiki/217789,Image Segmentation,CT,MRI,,,,,,,
Boundary-weighted logit consistency improves calibration of segmentation networks ,Neural network prediction probabilities and accuracy are often only weakly-correlated. Inherent label ambiguity in training data for image segmentation aggravates such miscalibration. We show that logit consistency across stochastic transformations acts as a spatially varying regularizer that prevents overconfident predictions at pixels with ambiguous labels. Our boundary-weighted extension of this regularizer provides state-of-the-art calibration for prostate and heart MRI segmentation. Code is available at https://github.com/neerakara/BWCR.,https://github.com/neerakara/BWCR,https://wiki.cancerimagingarchive.net/display/Public/NCI-ISBI+2013+Challenge+-+Automated+Segmentation+of+Prostate+Structures,Uncertainty,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI ,"Brain structural MRI has been widely used for assessing future progression of cognitive impairment (CI) based on learning-based methods. Previous studies generally suffer from the limited number of labeled training data, while there exists a huge amount of MRIs in large-scale public databases. Even without task-specific label information, brain anatomical structures provided by these MRIs can be used to boost learning performance intuitively. Unfortunately, existing research seldom takes advantage of such brain anatomy prior. To this end, this paper proposes a brain anatomy-guided representation (BAR) learning framework for assessing the clinical progression of cognitive impairment with T1-weighted MRIs. The BAR consists of (a) a pretext model and (b) a downstream model, with a shared brain anatomy-guided encoder for MRI feature extraction. The pretext model also contains a decoder for brain tissue segmentation, while the downstream model relies on a predictor for classification. We first train the pretext model through a brain tissue segmentation task on large-scale auxiliary MRIs, yielding a generalizable encoder. To provide accurate brain anatomy, we perform tissue segmentation for 9,544 MRIs from ADNI to generate ground truth using an established toolbox with manual verification. The downstream model with learned encoder is further fine-tuned on target MRIs for prediction tasks. We validate the proposed BAR on two CI-related studies of late-life depression analysis with 309 subjects and diabetes mellitus analysis with 82 subjects. Experimental results suggest that the BAR outperforms several state-of-the-art (SOTA) methods in MRI-based depression recognition and cognitive impairment identification, and the pretext model can be potentially used for tissue segmentation in other MRI-based studies.",https://github.com/goodaycoder/BAR,,Neuroimaging - Brain Development,Transfer learning,MRI,,,,,,,
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis ,"The functional connectivity (FC) between brain regions is usually estimated through a statistical dependency method with functional magnetic resonance imaging (fMRI) data. It inevitably yields redundant and noise connections, limiting the performance of deep supervised models in brain disease diagnosis. Besides, the supervised signals of fMRI data are insufficient due to the shortage of labeled data. To address these issues, we propose an end-to-end unsupervised graph structure learning method for sufficiently capturing the structure or characteristics of the functional brain network itself without relying on manual labels. More specifically, the proposed method incorporates a graph generation module for automatically learning the discriminative graph structures of functional brain networks and a topology-aware encoding module for sufficiently capturing the structure information in the functional brain networks. Furthermore, we also design the view consistency and correlation-guided contrastive regularizations such that the network parameters can be trained jointly in an end-to-end manner. We evaluated our model on two real medical clinical applications: the diagnosis of Bipolar Disorder (BD) and Major Depressive Disorder (MDD). The results suggest that the proposed method outperforms state-of-the-art methods. In addition, our model is capable of identifying associated biomarkers and providing evidence of disease association. To the best of our knowledge, our work attempts to construct learnable functional brain networks with unsupervised graph structure learning. Our code is available at https://github.com/IntelliDAL/Graph/tree/main/BrainUSL.",https://github.com/IntelliDAL/Graph/tree/main/BrainUSL,,Neuroimaging - Functional Brain Networks,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network ,"Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional
neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and
long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing
global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid
multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid ar-
chitecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods
and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the
highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.",,,Image Segmentation,Breast,Attention models,Model Generalizability / Federated Learning,Transfer learning,,,,,
Bridging ex-vivo training and intra-operative deployment for surgical margin assessment with Evidential Graph Transformer ,"PURPOSE: The use of intra-operative mass spectrometry along with Graph Transformer models showed promising results for margin detection on ex-vivo data. Although highly interpretable, these methods lack the ability to handle the uncertainty associated with intra-operative decision making. In this paper for the first time, we propose Evidential Graph Transformer network, a combination of attention mapping and uncertainty estimation to increase the performance and interpretability of surgical margin assessment. METHODS: The Evidential Graph Transformer was formulated to output the uncertainty estimation along with intermediate attentions. The performance of the model was compared with different baselines in an ex-vivo cross-validation scheme, with extensive ablation study. The association of the model with clinical features were explored. The model was further validated for a prospective ex-vivo data, as well as a breast conserving surgery intra-operative data. RESULTS: The purposed model outperformed all baselines, statistically significantly, with average balanced accuracy of 91.6\%. When applied to intra-operative data, the purposed model improved the false positive rate of the baselines. The estimated attention distribution for status of different hormone receptors agreed with reported metabolic findings in the literature. CONCLUSION: Deployment of ex-vivo models is challenging due to the tissue heterogeneity of intra-operative data. The proposed Evidential Graph Transformer is a powerful tool that while providing the attention distribution of biochemical subbands, improve the surgical deployment power by providing decision confidence.",https://github.com/med-i-lab/evidential_graph_transformers/,,Oncology,Breast,Computer Aided Diagnosis,Attention models,Interpretability / Explainability,Uncertainty,other,Surgical Data Science,,
Building A Bridge: Close The Domain Gap in CT Metal Artifact Reduction ,"Metal artifacts in computed tomography (CT) degrade the imaging quality, leading to a negative impact on the clinical diagnosis. Empowered by medical big data, many DL-based approaches have been proposed for metal artifact reduction (MAR). In supervised MAR methods, models are usually trained on simulated data and applied to the clinical data. However, inferior MAR performance on clinical data is usually observed due to the domain gap existing between simulated and clinical data. Existing unsupervised MAR methods usually use clinical unpaired data for training and validation, which often distort the anatomical structure due to the absence of supervising information. To address these problems, we propose a novel semi-supervised MAR frame work. We use the clean image as the bridge between the synthetic and clinical metal-affected image domains to close the domain gap. We also break the cycle-consistency loss, which is often utilized for domain adaptation, since the bijective assumption is too harsh and does not accurately respond to facts of real situations. To improve the MAR performance, we proposed Artifact Filtering Module (AFM) to eliminate features helpless in recovering clean images. Experiments demonstrate the performance of the proposed method is competitive with several state-of-the-art unsupervised and semi-supervised MAR methods in both qualitative and quantitative aspects.",,,Image Reconstruction,,,,,,,,,
Can point cloud networks learn statistical shape models of anatomies? ,"Statistical Shape Modeling (SSM) is a valuable tool for investigating and quantifying anatomical variations within populations of anatomies. However, traditional correspondence-based SSM generation methods have a prohibitive inference process and require complete geometric proxies (e.g., high-resolution binary volumes or surface meshes) as input shapes to construct the SSM. Unordered 3D point cloud representations of shapes are more easily acquired from various medical imaging practices (e.g., thresholded images and surface scanning). Point cloud deep networks have recently achieved remarkable success in learning permutation-invariant features for different point cloud tasks (e.g., completion, semantic segmentation, classification). However, their application to learning SSM from point clouds is to-date unexplored. In this work, we demonstrate that existing point cloud encoder-decoder-based completion networks can provide an untapped potential for SSM, capturing population-level statistical representations of shapes while reducing the inference burden and relaxing the input requirement. We discuss the limitations of these techniques to the SSM application and suggest future improvements. Our work paves the way for further exploration of point cloud deep learning for SSM, a promising avenue for advancing shape analysis literature and broadening SSM to diverse use cases.",https://github.com/jadie1/PointCompletionSSM,https://github.com/jadie1/PointCompletionSSM,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computational Anatomy and Physiology,,,,,,,,
CARL: Cross-aligned Representation Learning for Multi-view Lung Cancer Histology Classification ,"Accurately classifying the histological subtype of non-small cell lung cancer (NSCLC) using computed tomography (CT) images is critical for clinicians in determining the best treatment options for patients. Although recent advances in multi-view approaches have shown promising results, discrepancies between CT images from different views introduce various representations in the feature space, hindering the effective integration of multiple views and thus impeding classification performance. To solve this problem, we propose a novel method called cross-aligned representation learning (CARL) to learn both view-invariant and view-specific representations for more accurate NSCLC histological subtype classification. Specifically, we introduce a cross-view representation alignment learning network which learns effective view-invariant representations in a common subspace to reduce multi-view discrepancies in a discriminability-enforcing way. Additionally, CARL learns view-specific representations as a complement to provide a holistic and disentangled perspective of the multi-view CT images. Experimental results demonstrate that CARL can effectively reduce the multi-view discrepancies and outperform other state-of-the-art NSCLC histological subtype classification methods.",https://github.com/candyknife/CARL,,Computer Aided Diagnosis,Other,CT,Treatment Response and Outcome/Disease Prediction,,,,,,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation ,"Automatic segmentation of intestinal lesions (e.g., polyps and adenomas) in colonoscopy is essential for early diagnosis and treatment of colorectal cancers. Current deep learning-driven methods still get trapped in inaccurate colonoscopic lesion segmentation due to diverse sizes (large scales) and irregular shapes of different types of polyps and adenomas, noise and artifacts, and illumination variations in colonoscopic video images. This work proposes a new deep learning model called cascade transformer encoded boundary-aware multibranch fusion networks for white-light and narrow-band colorectal lesion segmentation. Specifically, this architecture employs cascade transformers as its encoder to retain both global and local feature representation. It further introduces a boundary-aware multibranch fusion mechanism as a decoder that can enhance blurred lesion edges and extract salient features, and simultaneously suppress image noise and artifacts and illumination changes. Such a new designed encoder-decoder architecture can preserve lesion appearance feature details while aggregating the semantic global cues at several different feature levels. Additionally, a hybrid spatial-frequency loss function is explored to adaptively concentrate on the loss of important frequency components due to the inherent bias of neural networks. We evaluated our method not only on our in-house database with four types of colorectal lesions with different pathological features, but also on four public databases, with the experimental results showing that our method significantly outperforms state-of-the-art network models. In particular, it can improve the average dice similarity coefficient and intersection over union from (84.3\%, 78.4\%) to (87.0\%, 80.5\%) on the five databases.",,,Interventional Imaging Systems,Surgical Data Science,,,,,,,,
CAS-Net: Cross-view Aligned Segmentation by Graph Representation of Knees ,"Magnetic Resonance Imaging (MRI) has become an essential tool for clinical knee examinations. In clinical practice, knee scans are acquired from multiple views with stacked 2D slices, ensuring diagnosis accuracy while saving scanning time. However, obtaining fine 3D knee segmentation from multi-view 2D scans is challenging, which is yet necessary for morphological analysis. Moreover, radiologists need to annotate the knee segmentation in multiple 2D scans for medical studies, bringing additional labor. In this paper, we propose the Cross-view Aligned Segmentation Network (CAS-Net) to produce 3D knee segmentation from multi-view 2D MRI scans and annotations of sagittal views only. Specifically, a knee graph representation is firstly built in a 3D isotropic space after the super-resolution of multi-view 2D scans. Then, we utilize a graph-based network to segment individual multi-view patches along the knee surface, and piece together these patch segmentations into a complete knee segmentation with help of the knee graph. Experiments conducted on the Osteoarthritis Initiative (OAI) dataset demonstrate the validity of the CAS-Net to generate accurate 3D segmentation.",,https://github.com/zixuzhuang/OAI_seg,Image Segmentation,Musculoskeletal,MRI,,,,,,,
Category-independent Visual Explanation for Medical Deep Network Understanding ,"Visual explanations have the potential to improve our understanding of deep learning models and their decision-making process, which is critical for building transparent, reliable, and trustworthy AI systems. However, existing visualization methods have limitations, including their reliance on categorical labels to identify regions of interest, which may be inaccessible during model deployment and lead to incorrect diagnoses if an incorrect label is provided.
To address this issue, we propose a novel category-independent visual explanation method called Hessian-CIAM. Our algorithm uses the Hessian matrix, which is the second-order derivative of the activation function, to weigh the activation weight in the last convolutional layer and generate a region of interest heatmap at inference time. We then apply an SVD-based post-process to create a smoothed version of the heatmap. By doing so, our algorithm eliminates the need for categorical labels and modifications to the deep learning model.
To evaluate the effectiveness of our proposed method, we compared it to seven state-of-the-art algorithms using the Chestx-ray8 dataset. Our approach achieved a 55% higher IoU measurement than classical GradCAM and a 17% higher IoU measurement than EigenCAM. Moreover, our algorithm obtained a Judd AUC score of 0.70 on the glaucoma retinal image database, demonstrating its potential applicability in various medical applications.
In summary, our category-independent visual explanation method, Hessian-CIAM, can generate high-quality region of interest heatmaps that are not dependent on categorical labels, making it a promising tool for improving our understanding of deep learning models and their decision-making process, particularly in medical applications.",,,Interpretability / Explainability,Visualization in Biomedical Imaging,,,,,,,,
Category-level Regularized Unlabeled-to-labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data ,"Segmenting prostate from MRI is crucial for diagnosis and treatment planning of prostate cancer. Given the scarcity of labeled data in medical imaging, semi-supervised learning (SSL) presents an attractive option as it can utilize both limited labeled data and abundant unlabeled data. However, if the local center has limited image collection capability, there may also not be enough unlabeled data for semi-supervised learning to be effective. To overcome this issue, other partner centers can be consulted to help enrich the pool of unlabeled images, but this can result in data heterogeneity, which could hinder SSL that functions under the assumption of consistent data distribution. Tailoring for this important yet under-explored scenario, this work presents a novel Category-level regularized Unlabeled-to-Labeled (CU2L) learning framework for semi-supervised prostate segmentation with multi-site unlabeled MRI data. Specifically, CU2L is built upon the teacher-student architecture with the following tailored learning processes: (i) local pseudo-label learning for reinforcing confirmation of the data distribution of the local center; (ii) category-level regularized non-parametric unlabeled-to-labeled learning for robustly mining shared information by using the limited expert labels to regularize the intra-class features across centers to be discriminative and generalized; (iii) stability learning under perturbations to further enhance robustness to heterogeneity. Our method is evaluated on prostate MRI data from six different clinical centers and shows superior performance compared to other semi-supervised methods.",,https://liuquande.github.io/SAML/,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
CAT-ViL: Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery ,"Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with the Co-Attention gaTed Vision-Language (CAT-ViL) embedding for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse multimodal features from visual and textual sources. The fused embedding will feed a standard Data-Efficient Image Transformer (DeiT) module, before the parallel classifier and detector for joint prediction. We conduct the experimental validation on public surgical videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results highlight the superior performance and robustness of our proposed model compared to the state-of-the-art approaches. Ablation studies further prove the outstanding performance of all the proposed components. The proposed method provides a promising solution for surgical scene understanding, and opens up a primary step in the Artificial Intelligence (AI)-based VQLA system for surgical training. Our code is available at github.com/longbai1006/CAT-ViL.",https://github.com/longbai1006/CAT-ViL,https://endovissub2018-roboticscenesegmentation.grand-challenge.org/home/,Surgical Scene Understanding,Attention models,Text (clinical/radiology reports),Video,Surgical Data Science,,,,,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI? ,"Deep learning has shown the capability to substantially accelerate MRI reconstruction while acquiring fewer measurements. Recently, diffusion models have gained burgeoning interests as a novel type of deep learning-based generative methods. These methods aim to sample data points that belong to a target distribution from a Gaussian distribution, which has been successfully extended to MRI reconstruction. In this work, we proposed a Cold Diffusion-based MRI reconstruction method called CDiffMR. Different from conventional diffusion models, the degradation operation of our CDiffMR is based on k-space undersampling instead of adding Gaussian noise, and the restoration network is trained to harness a de-aliaseing function.We also design starting point and data consistency conditioning strategies to guide and accelerate the reverse process. More intriguingly, the pre-trained CDiffMR model can be reused for reconstruction tasks with different undersampling rates. We demonstrated, through extensive numerical and visual experiments, that the proposed CDiffMR can achieve comparable or even superior reconstruction results than state-of-the-art models. Compared to the diffusion model-based counterpart, CDiffMR reaches readily competing results using only 1.6 ∼ 3.4% for inference time. The code is publicly available at https://github.com/ayanglab/CDiffMR.",https://github.com/ayanglab/CDiffMR,,Image Reconstruction,MRI,,,,,,,,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification ,"Automatic examination of thin-prep cytologic test (TCT) slides can assist pathologists in finding cervical abnormality for accurate and efficient cancer screening. Current solutions mostly need to localize suspicious cells and classify abnormality based on local patches, concerning the fact that whole slide images of TCT are extremely large. It thus requires many annotations of normal and abnormal cervical cells, to supervise the training of the patch-level classifier for promising performance. In this paper, we propose CellGAN to synthesize cytopathological images of various cervical cell types for augmenting patch-level cell classification. Built upon a lightweight backbone, CellGAN is equipped with a non-linear class mapping network to effectively incorporate cell type information into image generation. We also propose the Skip-layer Global Context module to model the complex spatial relationship of the cells, and attain high fidelity of the synthesized images through adversarial learning. Our experiments demonstrate that CellGAN can produce visually plausible TCT cytopathological images for different cell types. We also validate the effectiveness of using CellGAN to greatly augment patch-level cell classification performance. Our code and model checkpoint are available at https://github.com/ZhenrongShen/CellGAN.",https://github.com/ZhenrongShen/CellGAN,,Computational (Integrative) Pathology,Histopathology,Microscopy,Visualization in Biomedical Imaging,,,,,,
CenterlinePointNet++: A new point cloud based architecture for coronary artery pressure drop and vFFR estimation ,"Estimation of patient-specific hemodynamic features, and in particular fractional flow reserve (FFR) in coronary arteries is an essential step in providing personalized and accurate diagnosis of coronary artery disease (CAD). In recent years, in the domain of computed tomography angiography (CTA), a virtual FFR (vFFR) derived from coronary CTA using computational fluid dynamics (CFD), has been used as a compelling,  non-invasive, in-silico replacement for invasive diagnostic techniques. Unfortunately, the time and computational demands of CFD are major obstacles to introducing vFFR from CT as a commonly used prophylactic tool. In this work, we propose a novel geometric-based artificial deep learning (DL) architecture, CenterlinePointNet++, which acts as a surrogate for CFD engines for the task of hemodynamic features estimation of the coronary arteries. Our architecture works directly on the vessel geometry represented as a surface point cloud and a centerline graph. As a result of that, it utilizes implicit geometry embedding without the need for hand-crafted features to estimate directly hemodynamic features. We evaluate our approach on the task of pressure drops and vFFR estimation for a synthetically generated dataset of coronary arteries and showcase significant improvement over commonly used geometry-based approaches.",,,Vascular,Cardiac,Other,CT,,,,,,
Centroid-aware feature recalibration for cancer grading in pathology images ,"Cancer grading is an essential task in pathology. The recent developments of artificial neural networks in computational pathology have shown that these methods hold great potential for improving the accuracy and quality of can-cer diagnosis. However, the issues with the robustness and reliability of such methods have not been fully resolved yet. Herein, we propose a centroid-aware feature recalibration network that can conduct cancer grading in an accurate and robust manner. The proposed network maps an input pathology image into an embedding space and adjusts it by using centroids embedding vectors of different cancer grades via attention mechanism. Equipped with the recalibrated embedding vector, the proposed network classifiers the input pathology image into a pertinent class label, i.e., cancer grade. We evaluate the proposed network using colorectal cancer datasets that were collected under different environments. The experimental results confirm that the proposed network is able to conduct cancer grading in pathology images with high accuracy regardless of the environmental changes in the datasets.",https://github.com/colin19950703/CaFeNet,https://github.com/QuIIL/KBSMC_colon_cancer_grading_dataset,Attention models,Computational (Integrative) Pathology,Other,Histopathology,,,,,,
Certification of Deep Learning Models for Medical Image Segmentation ,"In medical imaging, segmentation models have known a significant improvement in the past decade and are now used daily in clinical practice. However, similar to classification models, segmentation models are affected by adversarial attacks. In a safety-critical field like healthcare, certifying model predictions is of the utmost importance. Randomized smoothing has been introduced lately and provides a framework to certify models and obtain theoretical guarantees. 
In this paper, we present for the first time a certified segmentation baseline for medical imaging based on randomized smoothing and diffusion models. 
Our results show that leveraging the power of denoising diffusion probabilistic models helps us overcome the limits of randomized smoothing. We conduct extensive experiments on five public datasets of chest X-rays, skin lesions, and colonoscopies, and empirically show that we are able to maintain high certified Dice scores even for highly perturbed images. 
Our work represents the first attempt to certify medical image segmentation models, and we aspire for it to set a foundation for future benchmarks in this crucial and largely uncharted area.",https://github.com/othmanela/medical_cert_seg,http://db.jsrt.or.jp/eng.php,Image Segmentation,Uncertainty,other,,,,,,,
Chest X-ray Image Classification: A Causal Perspective ,"The chest X-ray (CXR) is a widely used and easily accessible medical test for diagnosing common chest diseases. Recently, there have been numerous advancements in deep learning-based methods capable of effectively classifying CXR. However, assessing whether these algorithms truly capture the cause-and-effect relationship between diseases and their underlying causes, or merely learn to map labels to images, remains a challenge.
In this paper, we propose a causal approach to address the CXR classification problem, which involves constructing a structural causal model (SCM) and utilizing backdoor adjustment to select relevant visual information for CXR classification. Specifically, we design various probability optimization functions to eliminate the influence of confounding factors on the learning of genuine causality. Experimental results demonstrate that our proposed method surpasses the performance of two open-source datasets in terms of classification performance. To access the source code for our approach, please visit: \url{https://github.com/zc2024/Causal_CXR}.",https://github.com/zc2024/Causal_CXR,https://stanfordmlgroup.github.io/competitions/chexpert/,Uncertainty,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI ,"Clinical AI applications, particularly medical imaging, are
increasingly being adopted in healthcare systems worldwide. However,
a crucial question remains: what happens after the AI model is put into
production? We present our novel multi-modal model drift framework
capable of tracking drift without contemporaneous ground truth using
only readily available inputs, namely DICOM metadata, image appearance representation from a variational autoencoder (VAE), and model
output probabilities. CheXStray was developed and tested using CheXpert, PadChest and Pediatric Pneumonia Chest X-ray datasets and we
demonstrate that our framework generates a strong proxy for ground
truth performance. In this work, we offer new insights into the challenges and solutions for observing deployed medical imaging AI and
make three key contributions to real-time medical imaging AI monitoring: (1) proof-of-concept for medical imaging drift detection including
use of VAE and domain specific statistical methods (2) a multi-modal
methodology for measuring and unifying drift metrics (3) new insights
into the challenges and solutions for observing deployed medical imaging
AI. Our framework is released as open-source tools so that others may
easily run their own workflows and build upon our work. Code available
at: https://github.com/microsoft/MedImaging-ModelDriftMonitoring",https://github.com/microsoft/MedImaging-ModelDriftMonitoring,https://bimcv.cipf.es/bimcv-projects/padchest/padchest-dataset-research-use-agreement/,Other,Interpretability / Explainability,Model Generalizability / Federated Learning,Uncertainty,,,,,,
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention ,"Both CNN-based and Transformer-based object detection with bounding box representation  have been extensively studied in computer vision and medical image analysis, but circular object detection in medical images is still underexplored. Inspired by the recent  anchor free CNN-based circular object detection method (CircleNet) for ball-shape glomeruli detection in renal pathology, in this paper, we present CircleFormer, a Transformer-based circular medical object detection with dynamic anchor circles. Specifically, queries with circle representation in Transformer decoder iteratively refine the circular object detection results, and  a circle cross attention module is introduced to compute the similarity between circular queries and image features.  A generalized circle IoU (gCIoU) is proposed to serve as a new regression loss of circular object detection as well. Moreover, our approach is easy to generalize to the segmentation task by adding a simple segmentation branch to CircleFormer. We evaluate our method in circular nuclei detection and segmentation on the public MoNuSeg dataset, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches.  The effectiveness of each component is validated via ablation studies as well.",https://github.com/zhanghx-iim-ahu/CircleFormer,,Microscopy,Image Segmentation,,,,,,,,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions ,"Effective representations of human brain function are essential for fMRI-based predictions of individual traits and classifications of neuropsychiatric disorders. Contrastive learning techniques can be favorable choices for representations of human brain function, if it were not for their requirement of large batch sizes. In this study, we proposed a novel method, namely, contrastive learning with amplitude-driven data augmentation (CL-ADDA), for effective representations of human brain function and ultimately fMRI-based individualized predictions. SimSiam, which sets no requirement on large batches, was used in this study to obtain discriminative representations among subjects to facilitate later predictions of individuals’ traits. The fMRI data in this study was augmented based on recent neuroscience findings that fMRI frames with high- and low-amplitude are of quite different functional significance. Accordingly, we generated a positive pair by concatenating the fMRI frames with high-amplitude into one augmented sample and the frames with low-amplitude into another sample. The two augmented samples were used as inputs for CL-ADDA, and individualized predictions were made in an end-to-end way. The performance of the proposed CL-ADDA was evaluated with individualized age and IQ predictions based on a public dataset (Cam-CAN). The experimental results demonstrate that the proposed CL-ADDA can substantially improve the prediction performance as compared to the existing methods.",https://github.com/tianbjtu/CL-ADDA.git,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - Functional Brain Networks,MRI,,,,,,,
Class Specific Feature Disentanglement And Text Embeddings For Multi-Label Generalized Zero Shot CXR Classification ,"Robustness of medical image classification models is limited by its exposure to the candidate disease classes.  Generalized zero shot learning (GZSL) aims at correctly predicting seen and unseen classes and most current GZSL approaches have focused on the single label case. It is  common for chest x-rays to be labelled with multiple disease classes. We propose a novel multi-label GZSL approach using: 1) class specific feature disentanglement and 2) semantic relationship between disease labels distilled from BERT models pre-trained on biomedical literature. We learn a dictionary from distilled text embeddings, and leverage them to synthesize feature vectors that are representative of multi-label samples. Compared to existing methods, our approach does not require class attribute vectors, which are an essential part of GZSL methods for natural images but are not available for medical images. Our approach outperforms state of the art GZSL methods for chest xray images.",,,Data Efficient Learning,Lung,Other,,,,,,,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation ,"Unsupervised domain adaptation (UDA) has gained great popularity in mitochondria segmentation, aiming to improve the adaptability of models from the labeled source domain to the unlabeled target domain via domain alignment. However, existing UDA methods only focus on aligning domains on the prediction level, while ignoring the feature space containing more adequate information than the predictions. In this paper, we propose a class-aware domain adaptation method for mitochondria segmentation on the feature level, which relies on the prototype representation to achieve more fine-grained alignment. In particular, we first extract the feature centroids of classes from the source domain as prototypes. Leveraging the extracted prototypes as a bridge, we constrain that features belonging to the same class but from different domains are pulled closer to each other, achieving the class-aware alignment. Meanwhile, we derive a segmentation prediction directly from feature space based on the distance between target features and source prototypes.  By incorporating a pseudo label to supervise the learning of this prediction, the feature distribution gap across domains is further reduced. Furthermore, to take full advantage of the potential of target domain, we propose an intra-domain consistency constraint to maintain consistent predictions of samples perturbed differently from the target image. Extensive experiments on different datasets demonstrate the superiority of our proposed method over existing UDA methods.",https://github.com/Danyin813/CAFA.,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Microscopy,,,,,,,
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging ,"Despite recent progress in enhancing the privacy of federated learning (FL) via differential privacy (DP), the DP trade-off between privacy protection and performance is still underexplored for real-world medical use. In this paper, we propose to optimize the trade-off under the context of client-level DP, which focuses on privacy during communications. However, FL for medical imaging involves typically much fewer participants (hospitals) than other domains (e.g., mobile devices), thus ensuring clients be differentially private is much more challenging. To tackle this, we propose an adaptive intermediary strategy to improve performance without harming privacy. Specifically, we theoretically find splitting clients into sub-clients, which serve as intermediaries between hospitals and the server, can mitigate the noises introduced by DP without harming privacy. Our proposed approach is empirically evaluated on both classification and segmentation tasks using two public datasets, and its effectiveness is demonstrated with significant performance improvements and comprehensive analytical studies.",https://github.com/med-air/Client-DP-FL,,Model Generalizability / Federated Learning,,,,,,,,,
Clinical Evaluation of AI-assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma ,"This study aims to investigate the clinical efficacy of AI generated virtual contrast-enhanced MRI (VCE-MRI) in primary gross-tumor-volume (GTV) delineation for patients with nasopharyngeal carcinoma (NPC). We retrospectively retrieved 303 biopsy-proven NPC patients from three oncology centers. 288 patients were used for model training and 15 patients were used to synthesize VCE-MRI for clinical evaluation. Two board-certified oncologists were invited for evaluating the VCE-MRI in two aspects: image quality and effectiveness in primary tumor delineation. Image quality of VCE-MRI evaluation includes distinguishability between real contrast-enhanced MRI (CE-MRI) and VCE-MRI, clarity of tumor-to-normal tissue interface, veracity of contrast enhancement in tumor invasion risk areas, and efficacy in primary tumor staging. For primary tumor delineation, the GTV was manually delineated by oncologists. Results showed the mean accuracy to distinguish VCE-MRI from CE-MRI was 53.33%; no significant difference was observed in clarity of tumor-to-normal tissue interface between VCE-MRI and CE-MRI; for the veracity of contrast enhancement in tumor invasion risk areas and efficacy in primary tumor staging, a Jaccard Index of 76.04% and accuracy of 86.67% were obtained, respectively. The image quality evaluation suggests that the quality of VCE-MRI is approximated to real CE-MRI. In tumor delineation evaluation, the Dice Similarity Coefficient and Hausdorff Distance of the GTVs that delineated from VCE-MRI and CE-MRI were 0.762 (0.673-0.859) and 1.932mm (0.763mm-2.974mm) respectively, which were clinically acceptable according to the experience of the radiation oncologists. This study demonstrated the VCE-MRI is highly promising in replacing the use of gadolinium-based CE-MRI for NPC delineation.",,,Oncology,MRI,Rigorous Evaluations of Methodology in Clinical Workflows,,,,,,,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction ,"Lung nodule malignancy prediction has been enhanced by advanced deep-learning techniques and effective tricks. Nevertheless, current methods are mainly trained with cross-entropy loss using one-hot categorical labels, which results in difficulty in distinguishing those nodules with closer progression labels.
Interestingly, we observe that clinical text information annotated by radiologists provides us with discriminative knowledge to identify challenging samples. 
Drawing on the capability of the contrastive language-image pre-training (CLIP) model to learn generalized visual representations from text annotations, in this paper, we propose CLIP-Lung, a textual knowledge-guided framework for lung nodule malignancy prediction. 
First, CLIP-Lung introduces both class and attribute annotations into the training of the lung nodule classifier without any additional overheads in inference. 
Second, we design a channel-wise conditional prompt (CCP) module to establish consistent relationships between learnable context prompts and specific feature maps. 
Third, we align image features with both class and attribute features via contrastive learning, rectifying false positives and false negatives in latent space.
Experimental results on the benchmark LIDC-IDRI dataset demonstrate the superiority of CLIP-Lung,  in both classification performance and interpretability of attention maps. Source code is available at https://github.com/ymLeiFDU/CLIP-Lung.",,,Lung,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans ,"Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self- and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtain a specificity of 97.7% on an external test set with 903 normal cases. Our approach performs comparably to established state-of-the-art gastric cancer screening tools like blood testing and endoscopy, while also being more sensitive in detecting early-stage cancer. This demonstrates the potential of our approach as a novel, non-invasive, low-cost, and accurate method for opportunistic gastric cancer screening.",,,Computer Aided Diagnosis,Abdomen,CT,,,,,,,
Clustering disease trajectories in contrastive feature space for biomarker proposal in age-related macular degeneration ,"Age-related macular degeneration (AMD) is the leading cause of blindness in the elderly. Current grading systems based on imaging biomarkers only coarsely group disease stages into broad categories that lack prognostic value for future disease progression. It is widely believed that this is due to their focus on a single point in time, disregarding the dynamic nature of the disease. In this work, we present the first method to automatically propose biomarkers that capture temporal dynamics of disease progression. Our method represents patient time series as trajectories in a latent feature space built with contrastive learning. Then, individual trajectories are partitioned into atomic sub-sequences that encode transitions between disease states. These are clustered using a newly introduced distance metric. In quantitative experiments we found our method yields temporal biomarkers that are predictive of conversion to late AMD. Furthermore, these clusters were highly interpretable to ophthalmologists who confirmed that many of the clusters represent dynamics that have previously been linked to the progression of AMD, even though they are currently not included in any clinical grading system.",,,Ophthalmology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Treatment Response and Outcome/Disease Prediction,,,,,,,
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation ,"New lesion segmentation is essential to estimate the disease progression and therapeutic effects during multiple sclerosis (MS) clinical treatments.  However, the expensive data acquisition and expert annotation restrict the feasibility of applying large-scale deep learning models.  Since single-time-point samples with all-lesion labels are relatively easy to collect, exploiting them to train deep models is highly desirable to improve new lesion segmentation.  Therefore, we proposed a coaction segmentation (CoactSeg) framework to exploit the heterogeneous data (i.e., new-lesion annotated two-time-point data and all-lesion annotated single-time-point data) for new MS lesion segmentation.  The CoactSeg model is designed as a unified model, with the same three inputs (the baseline, follow-up, and their longitudinal brain differences) and the same three outputs (the corresponding all-lesion and new-lesion predictions), no matter which type of heterogeneous data is being used. Moreover, a simple and effective relation regularization is proposed to ensure the longitudinal relations among the three outputs to improve the model learning. Extensive experiments demonstrate that utilizing the heterogeneous data and the proposed longitudinal relation constraint can significantly improve the performance for both new-lesion and all-lesion segmentation tasks.  Meanwhile, we also introduce an in-house MS-23v1 dataset, including 38 Oceania single-time-point samples with all-lesion labels.  Codes and the dataset are released at https://github.com/ycwu1997/CoactSeg.",https://github.com/ycwu1997/CoactSeg,https://github.com/ycwu1997/CoactSeg,Neuroimaging - Others,MRI,,,,,,,,
Co-assistant Networks for Label Correction ,"The presence of corrupted labels is a common problem in
the medical image datasets due to the difficulty of annotation. Mean-
while, corrupted labels might significantly deteriorate the performance
of deep neural networks (DNNs), which have been widely applied to
medical image analysis. To alleviate this issue, in this paper, we propose
a novel framework, namely Co-assistant Networks for Label Correction
(CNLC), to simultaneously detect and correct corrupted labels. Specif-
ically, the proposed framework consists of two modules, i.e., noise de-
tector and noise cleaner. The noise detector designs a CNN-based model
to distinguish corrupted labels from all samples, while the noise cleaner
investigates class-based GCNs to correct the detected corrupted labels.
Moreover, we design a new bi-level optimization algorithm to optimize
our proposed objective function. Extensive experiments on three popu-
lar medical image datasets demonstrate the superior performance of our
framework over recent state-of-the-art methods.",https://github.com/shannak-chen/CNLC,https://www.isic-archive.com/,Model Generalizability / Federated Learning,,,,,,,,,
Cochlear Implant Fold Detection in Intra-operative CT using Weakly Supervised Multi-Task Deep Learning ,"In cochlear implant (CI) procedures, an electrode array is surgically inserted into the cochlea. The electrodes are used to stimulate the auditory nerve and restore hearing sensation for the recipient. If the array folds inside the coch-lea during the insertion procedure, it can lead to trauma, damage to the resid-ual hearing, and poor hearing restoration. Intraoperative detection of such a case can allow a surgeon to perform reimplantation. However, this intraoper-ative detection requires experience and electrophysiological tests sometimes fail to detect an array folding. Due to the low incidence of array folding, we generated a dataset of CT images with folded synthetic electrode arrays with realistic metal artifact. The dataset was used to train a multitask custom 3D-UNet model for array fold detection. We tested the trained model on real post-operative CTs (7 with folded arrays and 200 without). Our model could correctly classify all the fold-over cases while misclassifying only 3 non fold-over cases. Therefore, the model is a promising option for array fold detection.",,,Guided Interventions and Surgery,Computer Aided Diagnosis,Image Reconstruction,Data Efficient Learning,CT,Surgical Data Science,Surgical Planning and Simulation,Surgical Skill and Work Flow Analysis,,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis ,"MRI synthesis promises to mitigate the challenge of missing MRI modality in clinical practice. Diffusion model has emerged as an effective technique for image synthesis by modelling complex and variable data distributions. However, most diffusion-based MRI synthesis models are using a single modality. As they operate in the original image domain, they are memory-intensive and less feasible for multi-modal synthesis. Moreover, they often fail to preserve the anatomical structure in MRI. Further, balancing the multiple conditions from multi-modal MRI inputs is crucial for multi-modal synthesis. Here, we propose the first diffusion-based multi-modality MRI synthesis model, namely Conditioned Latent Diffusion Model (CoLa-Diff). To reduce memory consumption, we perform the diffusion process in the latent space. We propose a novel network architecture, e.g., similar cooperative filtering, to solve the possible compression and noise in latent space. To better maintain the anatomical structure, brain region masks are introduced as the priors of density distributions to guide diffusion process. We further present auto-weight adaptation to employ multi-modal information effectively. Our experiments demonstrate that CoLa-Diff outperforms other state-of-the-art MRI synthesis methods, promising to serve as an effective tool for multi-modal MRI synthesis.",https://github.com/SeeMeInCrown/CoLa_Diff_MultiModal_MRI_Synthesis,https://brain-development.org/ixi-dataset/,Image Reconstruction,Attention models,Other,Uncertainty,MRI,,,,,
Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration ,"The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting. The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at \url{https://github.com/brain-intelligence-lab/GIRNet",https://github.com/brain-intelligence-lab/GIRNet,https://www.med.upenn.edu/cbica/brats-reg-challenge,Image Registration,,,,,,,,,
Collaborative modality generation and tissue segmentation for early-developing macaque brain MR images ,"In neuroscience research, automatic segmentation of macaque brain tissues in magnetic resonance imaging (MRI) is crucial for understanding brain structure and function during development and evolution. Acquisition of multimodal information is a key enabler of accurate tissue segmentation, especially in early-developing macaques with extremely low contrast and dynamic myelination. However, many MRI scans of early-developing macaques are acquired only in a single modality. While various generative adversarial networks (GAN) have been developed to impute missing modality data, current solutions treat modality generation and image segmentation as two independent tasks, neglecting their inherent relationship and mutual benefits. To address these issues, this study proposes a novel Collaborative Segmentation-Generation Framework (CSGF) that enables joint missing modality generation and tissue segmentation of macaque brain MR images. Specifically, the CSGF consists of a modality generation module (MGM) and a tissue segmentation module (TSM) that are trained jointly by a cross-module feature sharing (CFS) and transferring generated modality. The training of the MGM under the supervision of the TSM enforces anatomical feature consistency, while the TSM learns multi-modality information related to anatomical structures from both real and synthetic multi-modality MR images. Experiments show that the CSGF outperforms the conventional independent-task mode on an early-developing macaque MRI dataset with 155 scans, achieving superior quality in both missing modality generation and tissue segmentation.",https://github.com/XueyangWWW/CSGF,https://pubmed.ncbi.nlm.nih.gov/28210206/,Image Segmentation,MRI,,,,,,,,
COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation ,"Medical image segmentation is a critical task in medical image analysis. In recent years, deep learning based approaches have shown exceptional performance when trained on a fully-annotated dataset. However, data annotation is often a significant bottleneck, especially for 3D medical images. Active learning (AL) is a promising solution for efficient annotation but requires an initial set of labeled samples to start active selection. When the entire data pool is unlabeled, how do we select the samples to annotate as our initial set? This is also known as the cold-start AL, which permits only one chance to request annotations from experts without access to previously annotated data. Cold-start AL is highly relevant in many practical scenarios but has been under-explored, especially for 3D medical segmentation tasks requiring substantial annotation effort. In this paper, we present a benchmark named COLosSAL by evaluating six cold-start AL strategies on five 3D medical image segmentation tasks from the public Medical Segmentation Decathlon collection. We perform a thorough performance analysis and explore important open questions for cold-start AL, such as the impact of budget on different strategies. Our results show that cold-start AL is still an unsolved problem for 3D segmentation tasks but some important trends have been observed. The code repository, data partitions, and baseline results for the complete benchmark are publicly available at https://github.com/MedICL-VU/COLosSAL",https://github.com/han-liu/COLosSAL,http://medicaldecathlon.com/,Active Learning,Image Segmentation,Data Efficient Learning,Other,Uncertainty,,,,,
Combat Long-tails in Medical Classification with Relation-aware Consistency and Virtual Features Compensation ,"Deep learning techniques have achieved promising performance for computer-aided diagnosis, which is beneficial to alleviate the workload of clinicians. However, due to the scarcity of diseased samples, medical image datasets suffer from an inherent imbalance, and lead diagnostic algorithms biased to majority categories. This degrades the diagnostic performance, especially in recognizing rare categories. Existing works formulate this challenge as long-tails and adopt decoupling strategies to mitigate the effect of the biased classifier. But these works only use the imbalanced dataset to train the encoder and resample data to re-train the classifier by discarding the samples of head categories, thereby restricting the diagnostic performance. To address these problems, we propose a Multi-view Relation-aware Consistency and Virtual Features Compensation (MRC-VFC) framework for long-tailed medical image classification in two stages. In the first stage, we devise a Multi-view Relation-aware Consistency (MRC) for representation learning, which provides the training of encoders with unbiased guidance in addition to the imbalanced supervision. In the second stage, to produce an impartial classifier, we propose the Virtual Features Compensation (VFC) to recalibrate the classifier by generating massive balanced virtual features. Compared with the resampling, VFC compensates the minority classes to optimize an unbiased classifier with preserving complete knowledge of the majority ones. Extensive experiments on two long-tailed public benchmarks confirm that our MRC-VFC framework remarkably outperforms state-of-the-art algorithms.",https://github.com/jhonP-Li/MRC_VFC,,Computer Aided Diagnosis,Data Efficient Learning,,,,,,,,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning ,"Deep learning-based AI diagnostic models rely heavily on high-quality exhaustive-annotated data for algorithm training but suffer from noisy label information. To enhance the model’s robustness and prevent noisy label memorization, this paper proposes a robust Semi-supervised Contrastive Learning paradigm called SSCL, which can efficiently merge semi-supervised learning and contrastive learning for combating medical label noise. Specifically, the proposed SSCL framework consists of three well-designed components: the Mixup Feature Embedding (MFE) module, the Semi-supervised Learning (SSL) module, and the Similarity Contrastive Learning (SCL) module. By taking the hybrid augmented images as inputs, the MFE module with momentum update mechanism is designed to mine abstract distributed feature representations. Meanwhile, a flexible pseudo-labeling promotion strategy is introduced into the SSL module, which can refine the supervised information of the noisy data with pseudo-labels based on initial categorical predictions. Benefitting from the measure of similarity between classification distributions, the SCL module can effectively capture more reliable confident pairs, further reducing the effects of label noise on contrastive learning. Furthermore, a noise-robust loss function is also leveraged to ensure the samples with correct labels dominate the learning process. Extensive experiments on multiple benchmark datasets demonstrate the superiority of SSCL over state-of-the-art baselines.",https://github.com/Binz-Chen/MICCAI2023_SSCL,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Breast,Dermatology,Computer Aided Diagnosis,Ultrasound,,,,,
Community-Aware Transformer for Autism Prediction in fMRI Connectome ,"Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition that affects social communication and behaviour. Investigating functional magnetic resonance imaging (fMRI)-based  brain functional connectome can aid in the understanding and diagnosis of ASD, leading to more effective treatments. The brain is modelled as a network of brain Regions of Interest(ROIs) and ROIs form communities and knowledge of these communities is crucial for ASD diagnosis. On one hand, Transformer-based models have proven to be highly effective across several tasks, including fMRI connectome analysis to learn useful representations of ROIs. On the other hand, existing transformer-based models treat all ROIs equally and overlook the impact of community-specific associations when learning node embeddings. To fill this gap, we propose a novel method, Com-BrainTF, a hierarchical local-global transformer architecture that learns intra and inter-community aware node embeddings for ASD prediction task. Furthermore, we avoid over-parameterization by sharing the local transformer parameters for different communities but optimize unique learnable prompt tokens for each community. Our model outperforms state-of-the-art (SOTA) architecture on ABIDE dataset and has high interpretability, evident from the attention module.",https://github.com/ubc-tea/Com-BrainTF,,Neuroimaging - Functional Brain Networks,Attention models,Interpretability / Explainability,MRI,,,,,,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP ,"Compared with 2D MRI, 3D MRI provides superior volumetric spatial resolution and signal-to-noise ratio. However, it is more challenging to reconstruct 3D MRI images. Current methods are mainly based on convolutional neural networks (CNN) with small kernels, which are difficult to scale up to have sufficient fitting power for 3D MRI reconstruction due to the large image size and GPU memory constraint. Furthermore, MRI reconstruction is a deconvolution problem, which demands long-distance information that is difficult to capture by CNNs with small convolution kernels. The multi-layer perceptron (MLP) can model such long-distance information, but it requires a fixed input size. In this paper, we proposed Recon3DMLP,  a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction, for 3D MRI reconstruction. We further utilized the circular shift operation based on MRI physics such that dMLP accepts arbitrary image size and can extract global information from the entire FOV. We also propose a GPU memory efficient data fidelity module that can reduce >50% memory. We compared Recon3DMLP with other CNN-based models on a high-resolution (HR) 3D MRI dataset. Recon3DMLP improves HR 3D reconstruction and outperforms several existing CNN-based models under similar GPU memory consumption, which demonstrates that Recon3DMLP is a practical solution for HR 3D MRI reconstruction.",,,Image Reconstruction,MRI,,,,,,,,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation ,"Recent advances in denoising diffusion probalistic models have shown great success in image synthesis tasks.
While there are already works exploring the potential of this powerful tool in image semantic segmentation, its application in weakly supervised semantic segmentation (WSSS) remains relatively under-explored.Observing that conditional diffusion models (CDM) is capable of generating images subject to specific distributions, in this work, we utilize category-aware semantic information underlied in CDM to get the prediction mask of the target object with only image-level annotations.
More specifically, we locate the desired class by approximating the derivative of the output of CDM w.r.t the input condition.
Our method is different from previous diffusion model methods with guidance from an external classifier, which accumulates noises in the background during the reconstruction process.
Our method outperforms state-of-the-art CAM and diffusion model methods on two public medical image segmentation datasets, which demonstrates that CDM is a promising tool in WSSS. 
Also, experiment shows our method is more time-efficient than existing diffusion model methods, making it practical for wider applications.",https://github.com/xhu248/cond_ddpm_wsss,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI,,,,,,
Conditional Physics-Informed Graph Neural Network For Fractional Flow Reserve Assessment ,"The assessment of fractional flow reserve (FFR) is significant for diagnosing coronary artery disease and determining the patients and lesions in need of revascularization. Deep learning has become a promising approach for the assessment of FFR, due to its high computation efficiency in contrast to computational fluid dynamics. However, it suffers from the lack of appropriate priors. The current study only considers adding priors into the loss function, which is insufficient to learn features having strong relationships with the boundary conditions. In this paper, we propose a conditional physics-informed graph neural network (CPGNN) for FFR assessment under the morphology and boundary condition information. Specially, CPGNN adds morphology and boundary conditions into inputs to learn the conditioned features and penalizes the residual of physical equations and the boundary condition in the loss function. Additionally, CPGNN consists of a multi-scale graph fusion module (MSGF) and a physics-informed loss. MSGF is to generate the features constrained by the coronary topology and better represent the different-range dependence. The physics-informed loss uses the finite difference method to calculate the residuals of physical equations. Our CPGNN is evaluated over 183 real-world coronary observed from 143 X-ray and 40 CT angiography. The FFR values of CPGNN correlate well with FFR measurements r=0.89 in X-ray and r=0.88 in CT.",,,Computational Anatomy and Physiology,Cardiac,CT,other,,,,,,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction ,"Cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 seconds to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12±0.03mm geometric error and 0.07±0.03% self-intersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.",https://github.com/m-qiang/CoTAN,https://biomedia.github.io/dHCP-release-notes/,Image Segmentation,Neuroimaging - Brain Development,Attention models,MRI,,,,,,
Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation ,"Medical imaging has witnessed remarkable progress but usually requires a large amount of high-quality annotated data which is time-consuming and costly to obtain. To alleviate this burden, semi- supervised learning has garnered attention as a potential solution. In this paper, we present Meta-Learning for Bootstrapping Medical Image Segmentation (MLB-Seg), a novel method for tackling the challenge of semi-supervised medical image segmentation. Specifically, our approach first involves training a segmentation model on a small set of clean labeled images to generate initial labels for unlabeled data. To further optimize this bootstrapping process, we introduce a per-pixel weight mapping system that dynamically assigns weights to both the initialized labels and the model’s own predictions. These weights are determined using a meta-process that prioritizes pixels with loss gradient directions closer to those of clean data, which is based on a small set of precisely annotated images. To facilitate the meta-learning process, we additionally introduce a consistency-based Pseudo Label Enhancement (PLE) scheme that improves the quality of the model’s own predictions by ensembling predictions from various augmented versions of the same input. In order to improve the quality of the weight maps obtained through multiple augmentations of a single input, we introduce a mean teacher into the PLE scheme. This method helps to reduce noise in the weight maps and stabilize its generation process. Our extensive experimental results on public atrial and prostate segmentation datasets demonstrate that our proposed method achieves state-of-the-art results under semi-supervision. Our code is available at https://github.com/aijinrjinr/MLB-Seg.",https://github.com/aijinrjinr/MLB-Seg,https://github.com/yulequan/UA-MT/tree/88ed29ad794f877122e542a7fa9505a76fa83515/data,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Content-Preserving Diffusion Model for Unsupervised AS-OCT image Despeckling ,"Anterior segment optical coherence tomography (AS-OCT) is a non-invasive imaging technique that is highly valuable for ophthalmic diagnosis. However, speckles in AS-OCT images can often degrade the image quality and affect clinical analysis. As a result, removing speckles in AS-OCT images can greatly benefit automatic ophthalmology analysis. Unfortunately, challenges still exist in deploying effective AS-OCT image denoising algorithms, including collecting sufficient paired training data and the requirement to preserve consistent content in medical images.
To address these practical issues, we propose an unsupervised AS-OCT despeckling algorithm via Content Preserving Diffusion Model (CPDM) with statistical knowledge.
At the training stage, a Markov chain transforms clean images to white Gaussian noise by repeatedly adding random noise and removes the predicted noise in a reverse procedure. At the inference stage, we first analyze the statistical distribution of speckles and convert it into a Gaussian distribution, aiming to match the fast truncated reverse diffusion process. We then explore the posterior distribution of observed images as a fidelity term to ensure content consistency in the iterative procedure.
Our experimental results show that CPDM significantly improves image quality compared to competitive methods. Furthermore, we validate the benefits of CPDM for subsequent clinical analysis, including ciliary muscle (CM) segmentation and scleral spur (SS) localization.",,,Ophthalmology,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,,,
Context-Aware Pseudo-Label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation ,"In the domain adaptation problem, source data may be unavailable to the target client side due to privacy or intellectual property issues. Source-free unsupervised domain adaptation (SF-UDA) aims at adapting a model trained on the source side to align the target distribution with only the source model and unlabeled target data. The source model usually produces noisy and context-inconsistent pseudo-labels on the target domain, i.e., neighbouring regions that have a similar visual appearance are annotated with different pseudo-labels. This observation motivates us to refine pseudo-labels with context relations. Another observation is that features of the same class tend to form a cluster despite the domain gap, which implies context relations can be readily calculated from feature distances. To this end, we propose a context-aware pseudo-label refinement method for SF-UDA. Specifically, a context-similarity learning module is developed to learn context relations. Next, pseudo-label revision is designed utilizing the learned context relations. Further, we propose calibrating the revised pseudo-labels to compensate for wrong revision caused by inaccurate context relations. Additionally, we adopt a pixel-level and class-level denoising scheme to select reliable pseudo-labels for domain adaptation. Experiments on cross-domain fundus images indicate that our approach yields the state-of-the-art results. Code is available at https://github.com/xmed-lab/CPR.",https://github.com/xmed-lab/CPR,,Ophthalmology,Transfer learning,,,,,,,,
Continual Learning for Abdominal Multi-Organ and Tumor Segmentation ,"The ability to dynamically extend a model to new data and classes is critical for multiple organ and tumor segmentation. However, due to privacy regulations, accessing previous data and annotations can be problematic in the medical domain. This poses a significant barrier to preserving the high segmentation accuracy of the old classes when learning from new classes because of the catastrophic forgetting problem. In this paper, we first empirically demonstrate that simply using high-quality pseudo labels can fairly mitigate this problem in the setting of organ segmentation. Furthermore, we put forward an innovative architecture designed specifically for continuous organ and tumor segmentation, which incurs minimal computational overhead. Our proposed design involves replacing the conventional output layer with a suite of lightweight, class-specific heads, thereby offering the flexibility to accommodate newly emerging classes. These heads enable independent predictions for newly introduced and previously learned classes, effectively minimizing the impact of new classes on old ones during the course of continual learning. We further propose incorporating Contrastive Language–Image Pretraining (CLIP) embeddings into the organspecific heads. These embeddings encapsulate the semantic information of each class, informed by extensive image-text co-training. The proposed method is evaluated on both in-house and public abdominal CT datasets under organ and tumor segmentation tasks. Empirical results suggest that the proposed design improves the segmentation performance of a baseline model on newly-introduced and previously-learned classes along the learning trajectory.",,,Continual Learning,Abdomen,Image Segmentation,CT,,,,,,
ConTrack: Contextual Transformer for Device Tracking in X-ray ,"Device tracking is an important prerequisite for guidance during endovascular procedures. Especially during cardiac interventions, detection and tracking of guiding the catheter tip in 2D fluoroscopic images is important for applications such as mapping vessels from angiography (high dose with contrast) to fluoroscopy (low dose without contrast). Tracking the catheter tip poses different challenges: the tip can be occluded by contrast during angiography or interventional devices; and it is always in continuous movement due to the cardiac and respiratory motions. To overcome these challenges, we propose ConTrack, a transformer-based network that uses both spatial and temporal contextual information for accurate device detection and tracking in both X-ray fluoroscopy and angiography. The spatial information comes from the template frames and the segmentation module: the template frames define the surroundings of the device, whereas the segmentation module detects the entire device to bring more context for the tip prediction. Using multiple templates makes the model more robust to the change in appearance of the device when it is occluded by the contrast agent. The flow information computed on the segmented catheter mask between the current and the previous frame helps in further refining the prediction by compensating for the respiratory and cardiac motions. The experiments show that our method achieves 45% or higher accuracy in detection and tracking when compared to state-of-the-art tracking models.",,,Attention models,Cardiac,Vascular,Guided Interventions and Surgery,Interventional Imaging Systems,,,,,
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction ,"To obtain high-quality positron emission tomography (PET) scans while reducing radiation exposure to the human body, various approaches have been proposed to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images. One widely adopted technique is the generative adversarial networks (GANs), yet recently, diffusion probabilistic models (DPMs) have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET (RPET) image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods.",https://github.com/Show-han/PET-Reconstruction,https://doi.org/10.5281/zenodo.6361846,Image Reconstruction,Neuroimaging - Brain Development,Computer Aided Diagnosis,Other,PET/SPECT,,,,,
Contrastive Feature Decoupling for Weakly-supervised Disease Detection ,"Machine learning based Computer-Aided Diagnosis (CAD) aims to assist clinicians in the pathological diagnosis process. While dealing with video pathological diagnosis such as colonoscopy polyp detection, the recent SOTA method employs Weakly-supervised Video Anomaly Detection (WVAD) in the Multiple Instance Learning (MIL) scenarios to concern the temporal correlation within data and to formulate the concept of the interest disease simultaneously. Such a MIL-based WVAD method leverages video-level annotations to detect frame-level diseases and shows promising results. This paper casts the video pathological diagnosis as a MIL-based WVAD task and introduces Contrastive Feature Decoupling (CFD) network to decouple normal and abnormal feature ingredients per snippet. With such decoupled features, we are able to highlight the abnormal feature ingredients for accurately reasoning the disease score per snippet. The core components within our CFD model are the memory bank and contrastive loss. The former is used to learn atoms for representing normal features, and the latter is used to encourage our model to gain robust disease detection. We demonstrate that our CFD network is achieving new SOTA performance on the existing Polyp dataset and the introduced PANDA-MIL dataset. Our dataset are available at https://github.com/Jhih-Ciang/PANDA-MIL.",https://github.com/Jhih-Ciang/PANDA-MIL,https://github.com/Jhih-Ciang/PANDA-MIL,Computer Aided Diagnosis,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,Video,,,,,,
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning ,"Self-supervised learning (SSL) of visual representations from paired medical images and text reports has recently shown great promise for various downstream tasks. However, previous work has focused on investigating the effectiveness of two major SSL techniques separately, i.e, contrastive learning and masked autoencoding, without exploring their potential synergies. In this paper, we aim to integrate the strengths of these two techniques by proposing a contrastive masked image-text modeling framework for medical visual representation learning. On one hand, our framework conducts cross-modal contrastive learning between masked medical images and text reports, with a representation decoder being incorporated to recover the misaligned information in the masked images. On the other hand, to further leverage masked autoencoding, a masked image is also required to be able to reconstruct the original image itself and the masked information in the text reports. With pre-training on a large-scale medical image and report dataset, our framework shows complementary benefits of integrating the two SSL techniques on four downstream classification datasets. Extensive evaluations demonstrate consistent improvements of our method over state-of-the-art approaches, especially when very scarce labeled data are available.",https://github.com/cchen-cc/CMITM,,Computer Aided Diagnosis,Lung,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation ,"Transformers have been extensively studied in medical image segmentation to build pairwise long-range dependence. Yet, relatively limited well-annotated medical image data makes transformers struggle to extract diverse global features, resulting in attention collapse where attention maps become similar or even identical. Comparatively, convolutional neural networks (CNNs) have better convergence properties on small-scale training data but suffer from limited receptive fields. Existing works are dedicated to exploring the combinations of CNN and transformers while ignoring attention collapse, leaving the potential of transformers under-explored. In this paper, we propose to build CNN-style Transformers (ConvFormer) to promote better attention convergence and thus better segmentation performance. Specifically, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) corresponding to tokenization, self-attention, and feed-forward network in vanilla vision transformers. In contrast to positional embedding and tokenization, ConvFormer adopts 2D convolution and max-pooling for both position information preservation and feature size reduction. In this way, CSA takes 2D feature maps as inputs and establishes long-range dependency by constructing self-attention matrices as convolution kernels with adaptive sizes. Following CSA, 2D convolution is utilized for feature refinement through CFFN. Experimental results on multiple datasets demonstrate the effectiveness of ConvFormer working as a plug-and-play module for consistent performance improvement of transformer-based frameworks.",https://github.com/xianlin7/ConvFormer,https://challenge.isic-archive.com/data/,Image Segmentation,Cardiac,Dermatology,Attention models,CT,,,,,
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis ,"A brain network, viewed as a graph wiring different regions of interest (ROIs) in the brain, has been widely used to investigate brain dysfunction with various graph neural networks (GNNs). However, existing GNNs are built upon graph convolution that transforms measurements on the nodes, where ROI-wise features are not always guaranteed for brain networks. Therefore, the majority of existing graph analysis methods that rely on node features are inapplicable for network analysis unless a proxy such as node degree is provided. Moreover, the complex neurological interactions across different brain regions cannot be directly expressed in a simple node-to-node (i.e., 0-simplex) representation. In this paper, we propose a novel method, Hodge-Graph Neural Network (Hodge-GNN), that allows the GNN to directly derive desirable representations of graph edges and capture complex edge-wise topological features spatially via the Hodge Laplacian. Specifically, representing a graph as a simplicial complex holds a significant advantage over conventional methods that extract higher-order connectivity of a graph through hierarchical convolution in the spatial domain or graph transformation. The superiority of our method is validated in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study, in comparison to benchmarking GNNs as well as state-of-the-art graph classification models.",,,Computer Aided Diagnosis,Neuroimaging - DWI and Tractography,Other,MRI,,,,,,
cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations ,"Classification of heterogeneous diseases is challenging due to their complexity, variability of symptoms and imaging findings. Chronic Obstructive Pulmonary Disease (COPD) is a prime example, being underdiagnosed despite being the third leading cause of death. Its sparse, diffuse and heterogeneous appearance on computed tomography challenges supervised binary classification. We reformulate COPD binary classification as an anomaly detection task, proposing cOOpD: heterogeneous pathological regions are detected as Out-of-Distribution (OOD) from normal homogeneous lung regions. To this end, we learn representations of unlabeled lung regions employing a self-supervised contrastive pretext model, potentially capturing specific characteristics of diseased and healthy unlabeled regions. A generative model then learns the distribution of healthy representations and identifies abnormalities (stemming from COPD) as deviations. Patient-level scores are obtained by aggregating region OOD scores. We show that cOOpD achieves the best performance on two public datasets, with an increase of 8.2% and 7.7% in terms of AUROC compared to the previous supervised state-of-the-art. Additionally, cOOpD yields well-interpretable spatial anomaly maps and patient-level scores which we show to be of additional value in identifying individuals in the early stage of progression. Experiments in artificially designed real-world prevalence settings further support that anomaly detection is a powerful way of tackling COPD classification. Code is at https://github.com/MIC-DKFZ/cOOpD.",https://github.com/MIC-DKFZ/cOOpD,https://dbgap.ncbi.nlm.nih.gov/aa/wga.cgi?view_pdf&stacc=phs000951.v5.p5,Computer Aided Diagnosis,Lung,Image Registration,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,Population Imaging and Imaging Genetics,Treatment Response and Outcome/Disease Prediction,,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation ,"Semi-supervised learning has become increasingly popular in medical image segmentation due to its ability to leverage large amounts of unlabeled data to extract additional information. However, most existing semi-supervised segmentation methods only focus on extracting information from unlabeled data, disregarding the potential of labeled data to further improve the performance of the model.In this paper, we propose a novel Correlation Aware Mutual Learning (CAML) framework that leverages labeled data to guide the extraction of information from unlabeled data. Our approach is based on a mutual learning strategy that incorporates two modules: the Cross-sample Mutual Attention Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module establishes dense cross-sample correlations among a group of samples, enabling the transfer of label prior knowledge to unlabeled data. The OCC module constructs omni-correlations between the unlabeled and labeled datasets and regularizes dual models by constraining the omni-correlation matrix of each sub-model to be consistent. Experiments on the Atrial Segmentation Challenge dataset demonstrate that our proposed approach outperforms state-of-the-art methods, highlighting the effectiveness of our framework in medical image segmentation tasks. The codes, pre-trained weights, and data are publicly available.",https://github.com/Herschel555/CAML,https://github.com/Herschel555/CAML,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,MRI,,,,,,,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography ,"Accurate extraction of coronary arteries from coronary computed tomography angiography (CCTA) is a prerequisite for the computer-aided diagnosis of coronary artery disease (CAD). Deep learning-based methods can achieve automatic segmentation of vasculatures, but few of them focus on the connectivity and completeness of the coronary tree. In this paper, we propose CorSegRec, a topology-preserving scheme for extracting fully-connected coronary artery, which integrates image segmentation, centerline reconnection, and geometry reconstruction. First, we employ a new centerline enhanced loss in the segmentation process. Second, for the broken vessel segments, we propose a regularized walk algorithm, by integrating distance, probabilities predicted by centerline classifier, and cosine similarity to reconnect centerlines. Third, we apply level-set segmentation and implicit modeling techniques to reconstruct the geometric model of the missing vessels. Experiment results on two datasets demonstrate that the proposed method outperforms other methods with better volumetric scores and higher vascular connectivity. Code will be available at https://github.com/YH-Qiu/CorSegRec.",https://github.com/YH-Qiu/CorSegRec,https://asoca.grand-challenge.org/,Image Segmentation,Vascular,CT,,,,,,,
CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph ,"The thickness of the cortical band is linked to various neurological and psychiatric conditions, and is often estimated through surface-based methods such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical thickness using a diffeomorphic deformation of the gray-white matter interface towards the pial surface, offers an
    alternative to surface-based methods. Recent studies using a synthetic cortical thickness phantom have demonstrated that the combination of \mbox{DiReCT} and deep-learning-based segmentation is more sensitive to subvoxel
    cortical thinning than Freesurfer.",https://github.com/SCAN-NRAD/CortexMorph,brain-development.org/ixi-dataset,Computational Anatomy and Physiology,Neuroimaging - Others,Image Registration,Other,MRI,,,,,
Cortical analysis of heterogeneous clinical brain MRI scans for large-scale neuroimaging studies ,"Surface analysis of the cortex is ubiquitous in human neuroimaging with MRI, e.g., for cortical registration, parcellation, or thickness estimation. The convoluted cortical geometry requires isotropic scans (e.g., 1mm MPRAGEs) and good gray-white matter contrast for 3D reconstruction. This precludes the analysis of most brain MRI scans acquired for clinical purposes. Analyzing such scans would enable neuroimaging studies with sample sizes that cannot be achieved with current research datasets, particularly for underrepresented populations and rare diseases. Here we present the first method for cortical reconstruction, registration, parcellation, and thickness estimation for clinical brain MRI scans of any resolution and pulse sequence. The methods has a learning component and a classical optimization module. The former uses domain randomization to train a CNN that predicts an implicit representation of the white matter and pial surfaces (a signed distance function) at 1mm isotropic resolution, independently of the pulse sequence and resolution of the input. The latter uses geometry processing to place the surfaces while accurately satisfying topological and geometric constraints, thus enabling subsequent parcellation and thickness estimation with existing methods. We present results on 5mm axial FLAIR scans from ADNI and on a highly heterogeneous clinical dataset with 5,000 scans. Code and data are publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all-clinical",https://surfer.nmr.mgh.harvard.edu/fswiki/recon-all-clinical,,Neuroimaging - Others,MRI,,,,,,,,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models ,"Delineating and removing brackets on 3D dental models and then reconstructing the tooth surface can enable orthodontists to pre-make retainers for patients. It eliminates the waiting time and avoids the change of tooth position. However, it is time-consuming and labor-intensive to process 3D dental models manually. To automate the entire process, accurate bracket segmentation and tooth surface reconstruction algorithms are of high need. In this paper, we propose a graph-based network named BSegNet for bracket segmentation on 3D dental models. The dynamic dilated neighborhood construction and residual connection in the graph network promote the bracket segmentation performance. Then, we propose a simple yet effective projection-based method to reconstruct the tooth surface. We project the vertices of the hole boundary on the tooth surface onto a 2D plane and then triangulate the projected polygon. We evaluate the performance of BSegNet on the bracket segmentation dataset and the results show the superiority of our proposed method. The framework integrating the segmentation and reconstruction achieves a low reconstruction error and can be used as an effective tool to assist orthodontists in orthodontic treatment.",,,Computer Aided Diagnosis,Image Segmentation,Other,,,,,,,
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities ,"COVID-19 is a viral disease that causes severe acute respiratory inflammation.
Although with less death rate, its increasing infectivity rate, together with its acute symptoms and high number of infections, is still attracting growing interests in the image analysis of COVID-19 pneumonia. Current accurate diagnosis by radiologists requires two modalities of X-Ray and Computed Tomography(CT) images from one patient. However, one modality might miss in clinical practice. In this study, we propose a novel multi-modality model to integrate X-Ray and CT data to further increase the versatility and robustness of the AI-assisted COVID-19 pneumonia diagnosis. We develop a Convolutional Neural Networks(CNN) and Transformers hybrid architecture, which  extracts extensive features from the distinct data modalities. This classifier is designed to be able to predict COVID-19 images with X-Ray image, or CT image, or both, while at the same time preserving the robustness  when missing modalities are found. Conjointly, a new method is proposed to fuse three-dimensional and two-dimensional images, which further increase the feature extraction and feature correlation of the input data. 
Thus, verified with a real-world public dataset of BIMCV-COVID19,  the model outperform state-of-the-arts with the AUC score of 79.93%. Clinically, the model has important medical significance for COVID-19 examination when some image modalities are missing, offering relevant flexibility to medical teams. Besides, the structure may be extended to other chest abnormalities to be detected by X-ray or CT examinations.Code is available at https://github.com/edurbi/MICCAI2023 .",https://github.com/edurbi/MICCAI2023,https://bimcv.cipf.es/bimcv-projects/bimcv-covid19/,Computer Aided Diagnosis,Lung,Attention models,,,,,,,
Cross-adversarial local distribution regularization for semi-supervised medical image segmentation ,"Medical semi-supervised segmentation is a technique where a model is trained to segment objects of interest in medical images with limited annotated data. Existing semi-supervised segmentation methods are usually based on the smoothness assumption. This assumption implies that the model output distributions of two similar data samples are encouraged to be invariant. In other words, the smoothness assumption states that similar samples (e.g., adding small perturbations to an image) should have similar outputs. In this paper, we introduce a novel cross-adversarial local distribution (Cross-ALD) regularization to further enhance the smoothness assumption for semi-supervised medical image segmentation task. We conducted comprehensive experiments that the Cross-ALD archives state-of-the-art  performance against many recent methods on the public LA and ACDC datasets.",https://github.com/PotatoThanh/Cross-adversarial-local-distribution-regularization,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,Data Efficient Learning,,,,,,,
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos ,"Surgical tool presence detection is an important part of the intra-operative and post-operative analysis of a surgery. State-of-the-art models, which perform this task well on a particular dataset, however, perform poorly when tested on another dataset. This occurs due to a significant domain shift between the datasets resulting from the use of different tools, sensors, data resolution etc. In this paper, we highlight this domain shift in the commonly performed cataract surgery and propose a novel end-to-end Unsupervised Domain Adaptation (UDA) method called the Barlow Adaptor that addresses the problem of distribution shift without requiring any labels from another domain. In addition, we introduce a novel loss called the Barlow Feature Alignment Loss (BFAL) which aligns features across different domains while reducing redundancy and the need for higher batch sizes, thus improving cross-dataset performance. The use of BFAL is a novel approach to address the challenge of domain shift in cataract surgery data. Extensive experiments are conducted on two cataract surgery datasets and it is shown that the proposed method outperforms the state-of-the-art UDA methods by 6%. The code can be found at https://github.com/JayParanjape/Barlow-Adaptor",https://github.com/JayParanjape/Barlow-Adaptor,https://ieee-dataport.org/open-access/cataracts,Transfer learning,Ophthalmology,Model Generalizability / Federated Learning,Surgical Data Science,,,,,,
Cross-modulated Few-shot Image Generation for Colorectal Tissue Classification ,"In this work, we propose a few-shot colorectal tissue image generation method for addressing the scarcity of histopathological training data for rare cancer tissues. Our few-shot generation method, named XM-GAN, takes one base and a pair of reference tissue images as input and generates high-quality yet diverse images. Within our XM-GAN, a novel controllable fusion block densely aggregates local regions of reference images based on their similarity to those in the base image, resulting in locally consistent features. To the best of our knowledge, we are the first to investigate few-shot generation in colorectal tissue images. We evaluate our few-shot colorectral tissue image generation by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Specifically, in specialist-based evaluation, pathologists could differentiate between our XM-GAN generated tissue images and real images only 55% time. Moreover, we utilize these generated images as data augmentation to address the few-shot tissue image classification task, achieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shot classifier.",https://github.com/VIROBO-15/XM-GAN,https://zenodo.org/record/53169,Other,Histopathology,,,,,,,,
Cross-view Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-ray Pair ,"Hip fractures are a common cause of morbidity and mortality and are usually diagnosed from the X-ray images in clinical routine. Deep learning has achieved promising progress for automatic hip fracture detection. However, for fractures where displacement appears not obvious (i.e., non-displaced fracture), the single-view X-ray image can only provide limited diagnostic information and integrating features from cross-view X-ray images (i.e., Frontal/Lateral-view) is needed for an accurate diagnosis. Nevertheless, it remains a technically challenging task to find reliable and discriminative cross-view representations for automatic diagnosis. First, it is difficult to locate discriminative task-related features in each X-ray view due to the weak supervision of image-level classification labels. Second, it is hard to extract reliable complementary information between different X-ray views as there is a displacement between them. To address the above challenges, this paper presents a novel cross-view deformable transformer framework to model relations of critical representations between different views for non-displaced hip fracture classification. Specifically, we adopt a deformable self-attention module to localize discriminative task-related features for each X-ray view only with the image-level label. Moreover, the located discriminative features are further adopted to explore correlated representations across views by taking advantage of the query of the dominated view as guidance. Furthermore, we build a dataset including 768 hip cases, in which each case has paired hip X-ray images (Frontal/Lateral-view), to evaluate our framework for the non-displaced fracture and normal classification task. Experimental results demonstrate the effectiveness of the deformable self-attention module and the designed cross-view correlation exploration mechanism.",,,Computer Aided Diagnosis,other,,,,,,,,
CT Kernel Conversion Using Multi-Domain Image-to-Image Translation with Generator-Guided Contrastive Learning ,"Computed tomography (CT) image can be reconstructed by various types of kernels depending on what anatomical structure is evaluated. Also, even if the same anatomical structure is analyzed, the kernel being used differs depending on whether it is qualitative or quantitative evaluation. Thus, CT images reconstructed with different kernels would be necessary for accurate diagnosis. However, once CT image is reconstructed with a specific kernel, the CT raw data, sinogram is usually removed because of its large capacity and limited storage. To solve this problem, many methods have been proposed by using deep learning approach using generative adversarial networks in image-to-image translation for kernel conversion. Nevertheless, it is still challenging task that translated image should maintain the anatomical structure of source image in medical domain. In this study, we propose CT kernel conversion method using multi-domain image-to-image translation with generator-guided contrastive learning. Our proposed method maintains the anatomical structure of the source image accurately and can be easily utilized into other multi-domain image-to-image translation methods with only changing the discriminator architecture and without adding any additional networks. Experimental results show that our proposed method can translate CT image from sharp into soft kernels and from soft into sharp kernels compared to other image-to-image translation methods. Our code is available at https://github.com/cychoi97/GGCL.",https://github.com/cychoi97/GGCL,,Image Reconstruction,Lung,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,,,,,,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows ,"Mitigating the effects of image appearance due to variations in computed tomography (CT) acquisition and reconstruction parameters is a challenging inverse problem. We present CTFlow, a normalizing flows-based method for harmonizing CT scans acquired and reconstructed using different doses and kernels to a target scan. Unlike existing state-of-the-art image harmonization approaches that only generate a single output, flow-based methods learn the explicit conditional density and output the entire spectrum of plausible reconstruction, reflecting the underlying uncertainty of the problem. We demonstrate how normalizing flows reduces variability in image quality and the performance of a machine learning algorithm for lung nodule detection. We evaluate the performance of CTFlow by 1) comparing it with other techniques on a denoising task using the AAPM-Mayo Clinical Low-Dose CT Grand Challenge dataset, and 2) demonstrating consistency in nodule detection performance across 186 real-world low-dose CT chest scans acquired at our institution. CTFlow performs better in the denoising task for both peak signal-to-noise ratio and perceptual quality metrics. Moreover, CTFlow produces more consistent predictions across all dose and kernel conditions than generative adversarial network (GAN)-based image harmonization on a lung nodule detection task.",https://github.com/hsu-lab/ctflow,https://doi.org/10.7937/9NPB-2637,CT,Lung,,,,,,,,
"CT-guided, Unsupervised Super-resolution Reconstruction of Single 3D Magnetic Resonance Image ","Deep learning-based algorithms for single MR image (MRI) super-resolution have shown great potential in enhancing the resolution of low-quality images. However, many of these methods rely on supervised training with paired low-resolution (LR) and high-resolution (HR) MR images, which can be difficult to obtain in clinical settings. This is because acquiring HR MR images in clinical settings requires a significant amount of time. In contrast, HR CT images are acquired in clinical routine. In this paper, we propose a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation and super-resolution reconstruction, eliminating the requirement of high-resolution MRI for training. The proposed approach is validated on two datasets respectively acquired from two different clinical sites. Well-established metrics including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metrics (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) are used to assess the performance of the proposed method. Our method achieved an average PSNR of 32.23, an average SSIM of 0.90 and an average LPIPS of 0.14 when evaluated on data of the first site. An average PSNR of 30.58, an average SSIM of 0.88, and an average LPIPS of 0.10 were achieved by our method when evaluated on data of the second site.",,,Computational Anatomy and Physiology,Other,CT,MRI,,,,,,
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training ,"A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip",https://github.com/kakaobrain/cxr-clip,https://physionet.org/content/mimic-cxr/2.0.0/,Other,Data Efficient Learning,Text (clinical/radiology reports),,,,,,,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy ,"Feature detection and matching is a computer vision problem that underpins different computer assisted techniques in endoscopy, including anatomy and lesion recognition, camera motion estimation, and 3D reconstruction. This problem is made extremely challenging due to the abundant presence of specular reflections. Most of the solutions proposed in the literature are based on filtering or masking out these regions as an additional processing step. There has been little investigation into explicitly learning robustness to such artefacts with single-step end-to-end training. In this paper, we propose an augmentation technique (CycleSTTN) that adds temporally consistent and realistic specularities to endoscopic videos. Such videos can act as ground truth data with known texture occluded behind the added specularities. We demonstrate that our image generation technique produces better results than a standard CycleGAN model. Additionally, we leverage this data augmentation to re-train a deep-learning based feature extractor (SuperPoint) and show that it improves. CycleSTTN code is made available at https://github.com/RemaDaher/CycleSTTN.",https://github.com/RemaDaher/CycleSTTN,https://www.synapse.org/#!Synapse:syn26707219/wiki/615178,Image Registration,Abdomen,Image Reconstruction,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,Video,Surgical Scene Understanding,,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation ,"Nucleus segmentation is usually the first step in pathological image analysis tasks. Generalizable nucleus segmentation refers to the problem of training a segmentation model that is robust to domain gaps between the source and target domains. The domain gaps are usually believed to be caused by the varied image acquisition conditions, e.g., different scanners, tissues, or staining protocols. In this paper, we argue that domain gaps can also be caused by different foreground (nucleus)-background ratios, as this ratio significantly affects feature statistics that are critical to normalization layers. We propose a Distribution-Aware Re-Coloring (DARC) model that handles the above challenges from two perspectives. First, we introduce a re-coloring method that relieves dramatic image color variations between different domains. Second, we propose a new instance normalization method that is robust to the variation in foreground-background ratios. We evaluate the proposed methods on two H&E stained image datasets, named CoNSeP and CPM17, and two IHC stained image datasets, called DeepLIIF and BC-DeepLIIF. Extensive experimental results justify the effectiveness of our proposed DARC model. Codes are available at https://github.com/csccsccsccsc/DARC.",https://github.com/csccsccsccsc/DARC,,Histopathology,Image Segmentation,,,,,,,,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs ,"The adoption of Multi-Instance Learning (MIL) for classifying Whole-Slide Images (WSIs) has increased in recent years. Indeed, pixel-level annotation of gigapixel WSI is mostly unfeasible and time-consuming in practice. For this reason, MIL approaches have been profitably integrated with the most recent deep-learning solutions for WSI classification to support clinical practice and diagnosis. Nevertheless, the majority of such approaches overlook the multi-scale nature of the WSIs; the few existing hierarchical MIL proposals simply flatten the multi-scale representations by concatenation or summation of features vectors, neglecting the spatial structure of the WSI. Our work aims to unleash the full potential of pyramidal structured WSI; to do so, we propose a graph-based multi-scale MIL approach, termed DAS-MIL, that exploits message passing to let information flows across multiple scales. By means of a knowledge distillation schema, the alignment between the latent space representation at different resolutions is encouraged while preserving the diversity in the informative content. The effectiveness of the proposed framework is demonstrated on two well-known datasets, where we outperform SOTA on WSI classification, gaining a +1.9% AUC and +3.3% accuracy on the popular Camelyon16 benchmark. The source code is available at https://github.com/aimagelab/mil4wsi.",https://github.com/aimagelab/mil4wsi,https://camelyon16.grand-challenge.org/Data/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Breast,Lung,Oncology,Other,Uncertainty,Histopathology,Treatment Response and Outcome/Disease Prediction,,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation ,"Neural Architecture Search (NAS) has been widely used for medical image segmentation by improving both model performance and computational efficiency. Recently, the Visual Transformer (ViT) model has achieved significant success in computer vision tasks. Leveraging these two innovations, we propose a novel NAS algorithm, DAST, to optimize neural network models with transformer for 3D medical image segmentation. The proposed algorithm is able to search the global structure and local operations of the architecture with a GPU memory consumption constraint. The resulting architectures reveal an effective relationship between convolution and transformer layers in segmentation models. Moreover, we validate the proposed algorithm on large-scale medical image segmentation data sets, showing its superior performance over the baselines. The model achieves state-of-the-art performance in the public challenge of kidney CT segmentation (KiTS’19). The implementation will be publicly available at [LINK].",,https://kits19.grand-challenge.org/,Image Segmentation,Abdomen,,,,,,,,
Data AUDIT: Identifying Attribute Utility- and Detectability-Induced Bias in Task Models ,"To safely deploy deep learning-based computer vision models for computer-aided detection and diagnosis, we must ensure that they are robust and reliable. Towards that goal, algorithmic auditing has received substantial attention. To guide their audit procedures, existing methods rely on heuristic approaches or high-level objectives (e.g., non-discrimination in regards to protected attributes, such as sex, gender, or race). However, algorithms may show bias with respect to various attributes beyond the more obvious ones, and integrity issues related to these more subtle attributes can have serious consequences. To enable the generation of actionable, data-driven hypotheses which identify specific dataset attributes likely to induce model bias, we contribute a first technique for the rigorous, quantitative screening of medical image datasets. Drawing from literature in the causal inference and information theory domains, our procedure decomposes the risks associated with dataset attributes in terms of their detectability and utility (defined as the amount of information the attribute gives about a task label). To demonstrate the effectiveness and sensitivity of our method, we develop a variety of datasets with synthetically inserted artifacts with different degrees of association to the target label that allow evaluation of inherited model biases via comparison of performance against true counterfactual examples. Using these datasets and results from hundreds of trained models, we show our screening method reliably identifies nearly imperceptible bias-inducing artifacts. Lastly, we apply our method to the natural attributes of a popular skin-lesion dataset and demonstrate its success. Our approach provides a means to perform more systematic algorithmic audits and guide future data collection efforts in pursuit of safer and more reliable models. Full code is available at https://github.com/mpavlak25/data-audit.",https://github.com/mpavlak25/data-audit,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T,Other,Model Generalizability / Federated Learning,other,,,,,,,
DBTrans: A Dual-Branch Vision Transformer for Multi-modal Brain Tumor Segmentation ,"3D Spatially Aligned Multi-modal MRI Brain Tumor Segmentation (SAMM-BTS) is a crucial task for clinical diagnosis. While Transformer-based models have shown outstanding success in this field due to their ability to model global features using the self-attention mechanism, they still face two chal-lenges. Firstly, due to the high computational complexity and deficiencies in modeling local features, the traditional self-attention mechanism is ill-suited for SAMM-BTS tasks that require modeling both global and local volumetric features within an acceptable computation overhead. Secondly, existing models only stack spatially aligned multi-modal data on the channel dimen-sion, without any processing for such multi-channel data in the model’s in-ternal design. To address these challenges, we propose a Transformer-based model for the SAMM-BTS task, namely DBTrans, with dual-branch architec-tures for both the encoder and decoder. Specifically, the encoder implements two parallel feature extraction branches, including a local branch based on Shifted Window Self-attention and a global branch based on Shuffle Win-dow Cross-attention to capture both local and global information with linear computational complexity. Besides, we add an extra global branch based on Shifted Window Cross-attention to the decoder, introducing the key and val-ue matrices from the corresponding encoder block, allowing the segmented target to access a more complete context during up-sampling. Furthermore, the above dual-branch designs in the encoder and decoder are both integrat-ed with improved channel attention mechanisms to fully explore the contri-bution of features at different channels. Experimental results demonstrate the superiority of our DBTrans model in both qualitative and quantitative measures. Codes will be released at https://github.com/Aru321/DBTrans.",https://github.com/Aru321/DBTrans,https://www.med.upenn.edu/cbica/brats2021/,Image Segmentation,Neuroimaging - Brain Development,MRI,,,,,,,
DCAug: Domain-aware & Content-consistent Cross-cycle Framework for Tumor Augmentation ,"Existing tumor augmentation methods cannot deal with both domain and content information at the same time, causing a content distortion or domain gap (distortion problem) in the generated tumor. To address this challenge,  we propose a Domain-aware and Content-consistent Cross-cycle Framework, named DCAug, for tumor augmentation to eliminate the distortion problem and improve the diversity and quality of synthetic tumors. Specifically, DCAug consists of one novel Cross-cycle Framework and two novel contrastive learning strategies: 1) Domain-aware Contrastive Learning (DaCL) and 2) Cross-domain Consistency Learning (CdCL), which disentangles the image information into two solely independent parts: 1) Domain-invariant content information; 2) Individual-specific domain information. During new sample generation, DCAug maintains the consistency of domain-invariant content information while adaptively adjusting individual-specific domain information through the advancement of DaCL and CdCL. We analyze and evaluate DCAug on two challenging tumor segmentation tasks.  Experimental results (10.48% improvement in KiTS, 5.25% improvement in ATLAS)  demonstrate that DCAug outperforms current state-of-the-art tumor augmentation methods and significantly improves the quality of the synthetic tumors.",,,Image Segmentation,Neuroimaging - Others,Computer Aided Diagnosis,Attention models,Transfer learning,,,,,
Debiasing Medical Visual Question Answering via Counterfactual Training ,"Medical Visual Question Answering (Med-VQA) is expected to predict a convincing answer with the given medical image and clinical question, aiming to assist clinical decision-making. While today’s works have intention to rely on the superficial linguistic correlations as a shortcut, which may generate emergent dissatisfactory clinic answers. In this paper, we propose a novel DeBiasing Med-VQA model with CounterFactual training (DeBCF) to overcome language priors comprehensively.  Specifically, we generate counterfactual samples by masking crucial keywords and assigning irrelevant labels, which implicitly promotes the sensitivity of the model to the semantic words and visual objects for bias-weaken. Furthermore, to explicitly prevent the cheating linguistic correlations, we formulate the language prior into counterfactual causal effects and eliminate it from the total effect on the generated answers. Additionally, we initiatively present a newly splitting bias-sensitive Med-VQA dataset, Semantically-Labeled Knowledge-Enhanced under Changing Priors (SLAKE-CP) dataset through regrouping and re-splitting the train-set and test-set of SLAKE into the different prior distribution of answers, dedicating the model to learn interpretable objects rather than overwhelmingly memorizing biases. Experimental results on two public datasets and SLAKE-CP demonstrate that the proposed DeBCF outperforms existing state-of-the-art Med-VQA models and obtains significant improvement in terms of accuracy and interpretability. To our knowledge, it’s the first attempt to overcome language priors in Med-VQA and construct the bias-sensitive dataset for evaluating debiased ability.",,,Interpretability / Explainability,Attention models,Other,,,,,,,
Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition ,"Masked autoencoder (MAE) has attracted unprecedented attention and achieves remarkable performance in many vision tasks. It reconstructs random masked image patches (known as proxy task) during pretraining and learns meaningful semantic representations that can be transferred to downstream tasks. However, MAE has not been thoroughly explored in ultrasound imaging. In this work, we investigate the potential of MAE for ultrasound image recognition. Motivated by the unique property of ultrasound imaging in high noise-to-signal ratio, we propose a novel deblurring MAE approach that incorporates deblurring into the proxy task during pretraining. The addition of deblurring facilitates the pretraining to better recover the subtle details presented in the ultrasound images, thus improving the performance of the downstream classification task. Our experimental results demonstrate the effectiveness of our deblurring MAE, achieving state-of-the-art performance in ultrasound image classification. Overall, our work highlights the potential of MAE for ultrasound image recognition and presents a novel approach that incorporates deblurring to further improve its effectiveness.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Ultrasound,,,,,,,,
Decoupled Consistency for Semi-supervised Medical Image Segmentation ,"By fully utilizing unlabeled data, the semi-supervised learning (SSL) technique has recently produced promising results in the segmentation of medical images. Pseudo labeling and consistency regularization are two effective strategies for using unlabeled data. Yet, the traditional pseudo labeling method will filter out low-confidence pixels. The advantages of both high- and low-confidence data are not fully exploited by consistency regularization. Therefore, neither of these two methods can make full use of unlabeled data. We proposed a novel decoupled consistency semi-supervised medical image segmentation framework. First, the dynamic threshold is utilized to decouple the prediction data into consistent and inconsistent parts. For the consistent part, we use the method of cross pseudo supervision to optimize it. For the inconsistent part, we further decouple it into unreliable data that is likely to occur close to the decision boundary and guidance data that is more likely to emerge near the high-density area. Unreliable data will be optimized in the direction of guidance data. We refer to this action as directional consistency. Furthermore, in order to fully utilize the data, we incorporate feature maps into the training process and calculate the loss of feature consistency. A significant number of experiments have demonstrated the superiority of our proposed method.",https://github.com/wxfaaaaa/DCNet,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,,,,,,,,
DeDA: Deep Directed Accumulator ,"Chronic active multiple sclerosis lesions, also referred to as rim+ lesions, are characterized by a hyperintense rim observed at the lesion’s edge on quantitative susceptibility maps. 
Despite their geometrically simple structure, characterized by radially oriented gradients at the lesion edge with a greater gradient magnitude compared to non-rim+ (rim-) lesions, recent studies indicate that the identification performance for these lesions is subpar due to limited data and significant class imbalance.
In this paper, we propose a simple yet effective image processing operation, deep directed accumulator (DeDA), which provides a new perspective for injecting domain-specific inductive biases (priors) into neural networks for rim+ lesion identification.
Given a feature map and a set of sampling grids, DeDA creates and quantizes an accumulator space into finite intervals and accumulates corresponding feature values.
This DeDA operation can be regarded as a symmetric operation to the grid sampling within the forward-backward neural network framework, the process of which is order-agnostic, and can be efficiently implemented with the native CUDA programming.
Experimental results on a dataset with 177 rim+ and 3986 rim- lesions show that 10.1% of improvement in a partial (false positive rate < 0.1) area under the receiver operating characteristic curve (pROC AUC) and 10.2% of improvement in an area under the precision recall curve (PR AUC) can be achieved respectively comparing to other state-of-the-art methods.
The source code is available online at https://github.com/tinymilky/DeDA",https://github.com/tinymilky/DeDA,,Data Efficient Learning,Neuroimaging - Others,Computer Aided Diagnosis,MRI,,,,,,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology ,"Weakly supervised classification of whole slide images (WSIs) in digital pathology typically involves making slide-level predictions by aggregating predictions from embeddings extracted from multiple individual tiles. However, these embeddings can fail to capture valuable information contained within the individual cells in each tile. Here we describe an embedding extraction method that combines tile-level embeddings with a cell-level embedding summary. We validated the method using four hematoxylin and eosin stained WSI classification tasks: human epidermal growth factor receptor 2 status and estrogen receptor status in primary breast cancer, breast cancer metastasis in lymph node tissue, and cell of origin classification in diffuse large B-cell lymphoma. For all tasks, the new method outperformed embedding extraction methods that did not include cell-level representations. Using the publicly available HEROHE Challenge data set, the method achieved a state-of-the-art performance of 90% area under the receiver operating characteristic curve. Additionally, we present a novel model explainability method that could identify cells associated with different classification groups, thus providing supplementary validation of the classification model. This deep learning approach has the potential to provide morphological insights that may improve understanding of complex underlying tumor pathologies.",,https://ecdp2020.grand-challenge.org/,Computational (Integrative) Pathology,Oncology,Attention models,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning ,"In this work, we investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. A novel method is introduced that learns to augment a surgeon’s behavior in image space through object motion invariant image registration via homographies. Contrary to existing approaches, no geometric assumptions are made and no depth information is necessary, enabling immediate translation to a robotic setup.
Deviating from the dominant approach in the literature which consist of following a surgical tool, we do not handcraft the objective and no priors are imposed on the surgical scene, allowing the method to discover unbiased policies. In this new research field, significant improvements are demonstrated over two baselines on the Cholec80 and HeiChole datasets, showcasing an improvement of 47% over camera motion continuation. The method is further shown to indeed predict camera motion correctly on the public motion classification labels of the AutoLaparo dataset. All code is made accessible on GitHub.",https://github.com/RViMLab/homography_imitation_learning,http://camma.u-strasbg.fr/datasets,Guided Interventions and Surgery,Surgical Data Science,,,,,,,,
Deep Learning for Tumor-associated Stroma Identification in Prostate Histopathology Slides ,"The diagnosis of prostate cancer is driven by the histopathological appearance of epithelial cells and epithelial tissue architecture. Despite the fact that the appearance of the tumor-associated stroma contributes to diagnostic impressions, its assessment has not been standardized. Given the crucial role of the tumor microenvironment in tumor progression, it is hypothesized that the morphological analysis of stroma could have diagnostic and prognostic value. However, stromal alterations are often subtle and challenging to characterize through light microscopy alone. Emerging evidence suggests that computerized algorithms can be used to identify and characterize these changes. This paper presents a deep-learning approach to identify and characterize tumor-associated stroma in multi-modal prostate histopathology slides. The model achieved an average testing AUROC of 86.53% on a large curated dataset with over 1.1 million stroma patches. Our experimental results indicate that stromal alterations are detectable in the presence of prostate cancer and highlight the potential for tumor-associated stroma to serve as a diagnostic biomarker in prostate cancer. Furthermore, our research offers a promising computational framework for in-depth exploration of the field effect and tumor progression in prostate cancer.",https://github.com/zcwang0702/DeepFieldEffect_StromaNet,,Computational (Integrative) Pathology,Computer Aided Diagnosis,Histopathology,,,,,,,
Deep Learning-Based Air Trapping Quantification using Paired Inspiratory-Expiratory Ultra-Low Dose CT ,"Air trapping (AT) is a frequent finding in early cystic fibrosis (CF) lung disease detectable by imaging. The correct radiographic assessment of AT on paired inspiratory-expiratory computed tomography (CT) scans is laborious and prone to inter-reader variation. Conventional threshold-based methods for AT quantification are primarily designed for adults and less suitable for children. The administered radiation dose, in particular, plays an important role, especially for children. Low dose (LD) CT is considered established standard in pediatric lung CT imaging but also ultra-low dose (ULD) CT is technically feasible and requires comprehensive validation. We investigated a deep learning approach to quantify air trapping on ULDCT in comparison to LDCT and assessed structure-function relationships by cross-validation against multiple breath washout (MBW) lung function testing. A densely connected convolutional neural network (DenseNet) was trained on 2-D patches to segment AT. The mean threshold from radiographic assessments, performed by two trained radiologists, was used as ground truth. A grid search was conducted to find the best parameter configuration. Quantitative AT (QAT), defined as the percentage of AT in the lungs detected by our DenseNet models, correlated strongly between LD and ULD. Structure-function relationships were maintained. The best model achieved a patch-based DICE coefficient of 0.82 evaluated on the test set. AT percentages correlated strongly with MBW results (LD: R = 0.76, p < 0.001; ULD: R = 0.78, p < 0.001). A strong correlation between LD and ULD (R = 0.96, p < 0.001) and small ULD-LD differences (mean difference -1.04 ± 3.25%) were observed.",,,Other,Lung,CT,,,,,,,
Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy ,"Robust and reliable anonymization of chest radiographs constitutes an essential step before publishing large datasets of such for research purposes. The conventional anonymization process is carried out by obscuring personal information in the images with black boxes and removing or replacing meta-information. However, such simple measures retain biometric information in the chest radiographs, allowing patients to be re-identified by a linkage attack. Therefore, there is an urgent need to obfuscate the biometric information appearing in the images. We propose the first deep learning-based approach (PriCheXy-Net) to targetedly anonymize chest radiographs while maintaining data utility for diagnostic and machine learning purposes. Our model architecture is a composition of three independent neural networks that, when collectively used, allow for learning a deformation field that is able to impede patient re-identification. Quantitative results on the ChestX-ray14 dataset show a reduction of patient re-identification from 81.8% to 57.7% (AUC) after re-training with little impact on the abnormality classification performance. This indicates the ability to preserve underlying abnormality patterns while increasing patient privacy. Lastly, we compare our proposed anonymization approach with two other obfuscation-based methods (Privacy-Net, DP-Pix) and demonstrate the superiority of our method towards resolving the privacy-utility trade-off for chest radiographs.",https://github.com/kaipackhaeuser/PriCheXy-Net,https://nihcc.app.box.com/v/ChestXray-NIHCC,Other,Lung,other,,,,,,,
Deep Mutual Distillation for Semi-Supervised Medical Image Segmentation ,"In this paper, we focus on semi-supervised medical image segmentation. Consistency regularization methods such as initialization perturbation on two networks combined with entropy minimization are widely used to deal with the task. However, entropy minimization-based methods force networks to agree on all parts of the training data. For extremely ambiguous regions, which are common in medical images, such agreement may be meaningless and unreliable. To this end, we present a conceptually simple yet effective method, termed Deep Mutual Distillation (DMD), a high-entropy online mutual distillation process, which is more informative than a low-entropy sharpened process, leading to more accurate segmentation results on ambiguous regions, especially the outer branches. Furthermore, to handle the class imbalance and background noise problem, and learn a more reliable consistency between the two networks, we exploit the Dice loss to supervise the mutual distillation. Extensive comparisons with all state-of-the-art on LA and ACDC datasets show the superiority of our proposed DMD, reporting a significant improvement of up to 1.15% in terms of Dice score when only 10% of training data are labeled in LA. We compare DMD with other consistency-based methods with different entropy guidance to support our assumption. Extensive ablation studies on the chosen temperature and loss function further verify the effectiveness of our design. The code is publicly available at https://github.com/SilenceMonk/Dual-Mutual-Distillation",https://github.com/SilenceMonk/Dual-Mutual-Distillation,https://www.cardiacatlas.org/atriaseg2018-challenge/,CT,,,,,,,,,
Deep probability contour framework for tumour segmentation and dose painting in PET images ,"The use of functional imaging such as PET in radiotherapy (RT) is rapidly expanding with new cancer treatment techniques. A fundamental step in RT planning is the accurate segmentation of tumours based on clinical diagnosis. Furthermore, recent tumour control techniques such as intensity modulated radiation therapy (IMRT) dose painting requires the accurate calculation of multiple nested contours of intensity values to optimise dose distribution across the tumour. Recently, convolutional neural networks (CNNs) have achieved tremendous success in image segmentation tasks, most of which present the output map at a pixel-wise level. However, its ability to accurately recognize precise object boundaries is limited by the loss of information in the successive downsampling layers. In addition, for the dose painting strategy, there is a need to develop image segmentation approaches that reproducibly and accurately identify the high recurrent-risk contours. To address these issues, we propose a novel hybrid-CNN that integrates a kernel smoothing-based probability contour approach (KsPC) to produce contour-based segmentation maps, which mimic expert behaviours and provide accurate probability contours designed to optimise dose painting/IMRT strategies. Instead of user-supplied tuning parameters, our final model, named KsPC-Net, applies a CNN backbone to automatically learn the parameters and leverages the advantage of KsPC to simultaneously identify object boundaries and provide probability contour accordingly. The proposed model demonstrated promising performance in comparison to state-of-the-art models on the MICCAI 2021 challenge dataset (HECKTOR).",,https://www.aicrowd.com/challenges/miccai-2021-hecktor,Image Segmentation,Oncology,Interpretability / Explainability,Uncertainty,PET/SPECT,,,,,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing ,"Hyperspectral imaging (HSI) captures a greater level of spectral detail than traditional optical imaging, making it a potentially valuable intraoperative tool when precise tissue differentiation is essential. Hardware limitations of current optical systems used for handheld realtime video HSI result in a limited focal depth, thereby posing usability issues for integration of the technology into the operating room. This work integrates a focus-tunable liquid lens into a video HSI exoscope, and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing setup. We benchmarked
our proposed autofocus algorithm against traditional policies, and found our novel approach to perform significantly (p < 0.05) better than traditional techniques (0.070 ± .098 mean absolute focal error compared to 0.146 ± .148). In addition, we performed a blinded usability trial by having two neurosurgeons compare the system with different autofocus policies, and found our novel approach to be the most favourable, making our system a desirable addition for intraoperative HSI.",,,Reinforcement Learning,Oncology,Guided Interventions and Surgery,Interventional Imaging Systems,Video,,,,,
Deep unsupervised clustering for conditional identification of subgroups within a digital pathology image set ,"Consideration of subgroups or domains within medical image datasets is crucial for the development and evaluation of robust and generalizable machine learning systems. To tackle the domain identification problem, we examine deep unsupervised generative clustering approaches for representation learning and clustering. The Variational Deep Embedding (VaDE) model is trained to learn lower-dimensional representations of images based on a Mixture-of-Gaussians latent space prior distribution while optimizing cluster assignments. We propose the Conditionally Decoded Variational Deep Embedding (CDVaDE) model which incorporates additional variables of choice, such as the class labels, as conditioning factors to guide the clustering towards subgroup structures in the data which have not been known or recognized previously. We analyze the behavior of CDVaDE on multiple datasets and compare it to other deep clustering algorithms. Our experimental results demonstrate that the considered models are capable of separating digital pathology images into meaningful subgroups. We provide a general-purpose implementation of all considered deep clustering methods as part of the open source Python package DomId (https://github.com/DIDSR/DomId).",https://github.com/DIDSR/DomId,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,Microscopy,,,,,,,
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics ,"Functional brain dynamics is supported by parallel and overlapping functional network modes that are associated with specific neural circuits. Decomposing these network modes from fMRI data and finding their temporal characteristics is challenging due to their time-varying nature and the non-linearity of the functional dynamics. Dynamic Mode Decomposition (DMD) algorithms have been quite popular for solving this decomposition problem in recent years. In this work, we apply GraphDMD — an extension of the DMD for network data — to extract the dynamic network modes and their temporal characteristics from the fMRI time series in an interpretable manner. GraphDMD, however, regards the underlying system as a linear dynamical system that is sub-optimal for extracting the network modes from non-linear functional data. In this work, we develop a generalized version of the GraphDMD algorithm — DeepGraphDMD— applicable to arbitrary non-linear graph dynamical systems. DeepGraphDMD is an autoencoder-based deep learning model that learns Koopman eigenfunctions for graph data and embeds the non-linear graph dynamics into a latent linear space. We show the effectiveness of our method in both simulated data and the HCP resting-state fMRI data. In the HCP data, DeepGraphDMD provides novel insights into cognitive brain functions by discovering two major network modes related to fluid and crystallized intelligence.",https://github.com/mturja-vf-ic-bd/DeepGraphDMD,https://db.humanconnectome.org/data/projects/HCP_1200,Neuroimaging - Functional Brain Networks,Interpretability / Explainability,Other,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data ,"We propose a robust deep learning framework to simultane-
ously detect and localize seizure activity from multichannel scalp EEG.
Our model, called DeepSOZ, consists of a transformer encoder to gen-
erate global and channel-wise encodings. The global branch is combined
with an LSTM for temporal seizure detection. In parallel, we employ
attention-weighted multi-instance pooling of channel-wise encodings to
predict the seizure onset zone. DeepSOZ is trained in a supervised fash-
ion and generates high-resolution predictions on the order of each sec-
ond (temporal) and EEG channel (spatial). We validate DeepSOZ via
bootstrapped nested cross-validation on a large dataset of 120 patients
curated from the Temple University Hospital corpus. As compared to
baseline approaches, DeepSOZ provides robust overall performance in
our multi-task learning setup. We also evaluate the intra-seizure and
intra-patient consistency of DeepSOZ as a first step to establishing its
trustworthiness for integration into the clinical workflow for epilepsy.",https://github.com/deeksha-ms/DeepSOZ.git,https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml,EEG/ECG,Neuroimaging - Others,Computer Aided Diagnosis,Attention models,Uncertainty,,,,,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-empowered Learning ,"Multi-class cell segmentation in high-resolution Giga-pixel whole slide images (WSI) is critical for various clinical applications. Training such an AI model typically requires labor-intensive pixel-wise manual annotation from experienced domain experts (e.g., pathologists). Moreover, such annotation is error-prone when differentiating fine-grained cell types (e.g., podocyte and mesangial cells) via the naked human eye. In this study, we assess the feasibility of democratizing pathological AI deployment by only using lay annotators (annotators without medical domain knowledge). The contribution of this paper is threefold: (1) We proposed a molecular-empowered learning scheme for multi-class cell segmentation using partial labels from lay annotators; (2) The proposed method integrated Giga-pixel level molecular-morphology cross-modality registration, molecular-informed annotation, and molecular-oriented segmentation model, so as to achieve significantly superior performance via 3 lay annotators as compared with 2 experienced pathologists; (3) A deep corrective learning (learning with imperfect label) method is proposed to further improve the segmentation performance using partially annotated noisy data. From the experimental results, our learning method achieved F1 = 0.8496 using molecular-informed annotations from lay annotators, which is better than conventional morphology-based annotations (F1 = 0.7015) from experienced pathologists. Our method democratizes the development of a pathological segmentation deep model to the lay annotator level, which consequently scales up the learning process similar to a non-medical computer vision task. The official implementation and cell annotations are publicly available at https://github.com/hrlblab/MolecularEL.",https://github.com/hrlblab/MolecularEL,https://github.com/hrlblab/MolecularEL,Computational (Integrative) Pathology,Data Efficient Learning,Histopathology,Microscopy,,,,,,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction ,"CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network(DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence’s decoding process.  Extensive experiments and model discussions illustrate DTEC-Net’s effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.",,https://nihcc.app.box.com/v/DeepLesion,CT,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Deployment of Image Analysis Algorithms under Prevalence Shifts ,"Domain gaps are among the most relevant roadblocks in the clinical translation of machine learning (ML)-based solutions for medical image analysis. While current research focuses on new training paradigms and network architectures, little attention is given to the specific effect of prevalence shifts on an algorithm deployed in practice. Such discrepancies between class frequencies in the data used for a method’s development/validation and that in its deployment environment(s) are of great importance, for example in the context of artificial intelligence (AI) democratization, as disease prevalences may vary widely across time and location. Our contribution is twofold. First, we empirically demonstrate the potentially severe consequences of missing prevalence handling by analyzing (i) the extent of miscalibration, (ii) the deviation of the decision threshold from the optimum, and (iii) the ability of validation metrics to reflect neural network performance on the deployment population as a function of the discrepancy between development and deployment prevalence. Second, we propose a workflow for prevalence-aware image classification that uses estimated deployment prevalences to adjust a trained classifier to a new environment, without requiring additional annotated deployment data. Comprehensive experiments based on a diverse set of 30 medical classification tasks showcase the benefit of the proposed workflow in generating better classifier decisions and more reliable performance estimates compared to current practice.",https://github.com/IMSY-DKFZ/prevalence-shifts,www.kaggle.com/ahemateja19bec1025/covid-xray-dataset,Model Generalizability / Federated Learning,Interpretability / Explainability,Uncertainty,,,,,,,
Detecting domain shift in multiple instance learning for digital pathology using Fréchet Domain Distance ,"Multiple-instance learning (MIL) is an attractive approach for digital pathology applications as it reduces the costs related to data collection and labelling. However, it is not clear how sensitive MIL is to clinically realistic domain shifts, i.e., differences in data distribution that could negatively affect performance, and if already existing metrics for detecting domain shifts work well with these algorithms. We trained an attention-based MIL algorithm to classify whether a whole-slide image of a lymph node contains breast tumour metastases. The algorithm was evaluated on data from a hospital in a different country and various subsets of this data that correspond to different levels of domain shift. Our contributions include showing that MIL for digital pathology is affected by clinically realistic differences in data, evaluating which features from a MIL model are most suitable for detecting changes in performance, and proposing an unsupervised metric named Fréchet Domain Distance (FDD) for quantification of domain shifts. Shift measure performance was evaluated through the mean Pearson correlation to change in classification performance, where FDD achieved 0.70 on 10-fold cross-validation models. The baselines included Deep ensemble, Difference of Confidence, and Representation shift which resulted in 0.45, -0.29, and 0.56 mean Pearson correlation, respectively. FDD could be a valuable tool for care providers and vendors who need to verify if a MIL system is likely to perform reliably when implemented at a new site, without requiring any additional annotations from pathologists.",,,Computer Aided Diagnosis,Attention models,Model Generalizability / Federated Learning,Histopathology,,,,,,
Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery ,"In surgical oncology, it is challenging for surgeons to identify lymph nodes and completely resect cancer even with pre-operative imaging systems like PET and CT, because of the lack of reliable intraoperative visualization tools. Endoscopic radio-guided cancer detection and resection has recently been evaluated whereby a novel tethered laparoscopic gamma detector is used to localize a preoperatively injected radiotracer. This can both enhance the endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is non-imaging and it does not visibly indicate the activity origination on the tissue surface. Initial failed attempts used segmentation or geometric methods, but led to the discovery that it could be resolved by leveraging high-dimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through intensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at https://github.com/br0202/Sensing_area_detection.git.",https://github.com/br0202/Sensing_area_detection.git,https://github.com/br0202/Sensing_area_detection.git,Guided Interventions and Surgery,Interventional Imaging Systems,Surgical Scene Understanding,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,,,
Detection of basal cell carcinoma in whole slide images ,"Basal cell carcinoma (BCC) is a prevalent and increasingly diagnosed form of skin cancer that can benefit from automated whole slide image (WSI) analysis. However, traditional methods that utilize popular network structures designed for natural images, such as the ImageNet dataset, may result in reduced accuracy due to the significant differences between natural and pathology images. In this paper, we analyze skin cancer images using the optimal network obtained by neural architecture search (NAS) on the skin cancer dataset. Compared with traditional methods, our network is more applicable to the task of skin cancer detection. Furthermore, unlike traditional unilaterally augmented (UA) methods, the proposed supernet Skin-Cancer net (SC-net) considers the fairness of training and alleviates the effects of evaluation bias. We use the SC-net to fairly treat all the architectures in the search space and leveraged evolutionary search to obtain the optimal architecture for a skin cancer dataset. Our experiments involve 277,000 patches split from 194 slides. Under the same FLOPs budget (4.1G), our searched ResNet50 model achieves 96.2% accuracy and 96.5% area under the ROC curve (AUC), which are 4.8% and 4.7% higher than those with the baseline settings, respectively.",,,Computer Aided Diagnosis,Computational (Integrative) Pathology,Other,Histopathology,,,,,,
Detection-free Pipeline for Cervical Cancer Screening of Whole Slide Images ,"Cervical cancer is a significant health burden worldwide, and computer-aided diagnosis (CAD) pipelines have the potential to improve diagnosis efficiency and treatment outcomes. However, traditional CAD pipelines have limitations due to the requirement of a detection model trained on a large annotated dataset, which can be expensive and time-consuming. They also have a clear performance limit and low data utilization efficiency. To address these issues, we propose a two-stage detection-free pipeline that maximizes data utilization of whole slide images (WSIs), which levarages only sample-level diagnosis labels for training. The experimental results demonstrate the effectiveness of our approach, with performance scaling up as the amount of data increases. Overall, our novel pipeline has the potential to fully utilize massive data in WSI classification and can significantly improve cancer diagnosis and treatment. By reducing the reliance on expensive data labeling and detection models, our approach could enable more widespread and cost-effective implementation of CAD pipelines in clinical settings.",https://github.com/thebestannie/Detection-free-MICCAI2023,,Computer Aided Diagnosis,Computational (Integrative) Pathology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images ,"Early detection and diagnosis of breast cancer using ultrasound images are crucial for timely diagnostic decision and treatment in clinical application. However, the similarity between tumors and background and also severe shadow noises in ul-trasound images make accurate segmentation of breast tumor   challenging. In this paper, we propose a large pre-trained model for breast tumor segmentation, with robust performance when applied to new datasets. Specifically, our model is built upon UNet backbone with deep supervision for each stage of the decoder. Be-sides using Dice score, we also design discriminator-based loss on each stage of the decoder to penalize the distribution dissimilarity from multi-scales. Our pro-posed model is validated on a large clinical dataset with more than 10000 cases, and shows significant improvement than other representative models. Besides, we apply our large pretrained model to two public datasets without fine tuning, and obtain extremely good results. This indicates great generalizability of our large pre-trained model, as well as robustness to multi-site data. The code is publicly available at https://github.com/limy-ulab/US-SEG.",https://github.com/limy-ulab/US-SEG,,Breast,Image Segmentation,Other,Ultrasound,,,,,,
Development and Fast Transferring of General Connectivity-based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-learner ,"The accurate and automatic diagnosis of new brain disorders (BDs) is cru-cial in the clinical stage. However, previous deep learning based methods require training new models with large data from new BDs, which is often not practical. Recent neuroscience studies suggested that BDs could share commonness from the perspective of functional connectivity derived from fMRI. This potentially enables developing a connectivity-based general model that can be transferred to new BDs to address the difficulty of train-ing new models under data limitations. In this work, we demonstrate this possibility by employing the  meta-learning algorithm to develop a general adaptive graph meta-learner and transfer it to the new BDs. Specifically, we use an adaptive multi-view graph classifier to select the appropriate view for specific disease classification and a reinforcement learning based meta controller to alleviate the over-fitting when adapting to new datasets with small sizes. Experiments on 4,114 fMRI data from multiple datasets cover-ing a broad range of BDs demonstrate the effectiveness of modules in our framework and the advantages over other comparison methods. This work may pave the basis for fMRI-based deep learning models being widely used in clinical applications.",https://github.com/dasklarleo/fMRI_meta/tree/main,,Neuroimaging - Functional Brain Networks,Meta-learning,,,,,,,,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation ,"Deep learning-based medical image segmentation models suffer from performance degradation when deployed to a new healthcare center. To address this issue, unsupervised domain adaptation and multi-source domain generalization methods have been proposed, which, however, are less favorable for clinical practice due to the cost of acquiring target-domain data and the privacy concerns associated with redistributing the data from multiple source domains. In this paper, we propose a \textbf{C}hannel-level \textbf{C}ontrastive \textbf{S}ingle \textbf{D}omain \textbf{G}eneralization (\textbf{C$^2$SDG}) model for medical image segmentation. In C$^2$SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. Our method is novel in the contrastive perspective that enables channel-wise feature disentanglement using a single source domain. We evaluated C$^2$SDG against six SDG methods on a multi-domain joint optic cup and optic disc segmentation benchmark. Our results suggest the effectiveness of each module in C$^2$SDG and also indicate that C$^2$SDG outperforms the baseline and all competing methods with a large margin. The code is available at \url{https://github.com/ShishuaiHu/CCSDG}.",https://github.com/ShishuaiHu/CCSDG,https://zenodo.org/record/6325549,Image Segmentation,Model Generalizability / Federated Learning,other,,,,,,,
DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation ,"The volume-wise labeling of 3D medical images is expertisedemanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Cotraining (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC",https://github.com/xmed-lab/DHC,https://www.synapse.org/#!Synapse:syn3193805,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels ,"The soft Dice loss (SDL) has taken a pivotal role in numerous automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct utilization in scenarios involving soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be employed in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g.\ averaging, label smoothing, and knowledge distillation) over hard labels (e.g.\ majority voting and random selection). As a result, we obtain superior Dice scores and model calibration, which supports the wider adoption of DMLs in practice. The code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.",https://github.com/zifuwanggg/JDTLosses,,Image Segmentation,Uncertainty,,,,,,,,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model ,"Currently, deep learning (DL) has achieved the automatic prediction of dose distribution in radiotherapy planning, enhancing its efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L_1 or L_2 loss with posterior average calculations. To alleviate this limitation, we innovatively introduce a diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDP model contains a forward process and a reverse process. In the forward process, DiffDP gradually transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the noise added in each timestep. In the reverse process, it removes the noise from the original Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution map. To ensure the accuracy of the prediction, we further design a structure encoder to extract anatomical information from patient anatomy images and enable the noise predictor to be aware of the dose constraints within several essential organs, i.e., the planning target volume and organs at risk. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method.",https://github.com/scufzh/DiffDP,,Computer Aided Diagnosis,Neuroimaging - Others,CT,,,,,,,
Differentiable Beamforming for Ultrasound Autofocusing ,"Ultrasound images are distorted by phase aberration arising from local sound speed variations in the tissue, which lead to inaccurate time delays in beamforming and loss of image focus. Whereas state-of-the-art correction approaches rely on simplified physical models (e.g. phase screens), we propose a novel physics-based framework called differentiable beamforming that can be used to rapidly solve a wide range of imaging problems. We demonstrate the generalizability of differentiable beamforming by optimizing the spatial sound speed distribution in a heterogeneous imaging domain to achieve ultrasound autofocusing using a variety of physical constraints based on phase shift minimization, speckle brightness, and coherence maximization. The proposed method corrects for the effects of phase aberration in both simulation and in-vivo cases by improving image focus while simultaneously providing quantitative speed-of-sound distributions for tissue diagnostics, with accuracy improvements with respect to previously published baselines. Finally, we provide a broader discussion of applications of differentiable beamforming in other ultrasound domains.",https://github.com/waltsims/dbua,,Ultrasound,Image Reconstruction,,,,,,,,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification ,"Diffusion Probabilistic Models have recently shown remarkable performance in generative image modeling, attracting significant attention in the computer vision community. However, while a substantial amount of diffusion-based research has focused on generative tasks, few studies have applied diffusion models to general medical image classification.
In this paper, we propose the first diffusion-based model (named DiffMIC) to address general medical image classification by eliminating unexpected noise and perturbations in medical images and robustly capturing semantic representation. To achieve this goal, we devise a dual conditional guidance strategy that conditions each diffusion step with multiple granularities to improve step-wise regional attention. Furthermore, we propose learning the mutual information in each granularity by enforcing Maximum-Mean Discrepancy regularization during the diffusion forward process.
We evaluate the effectiveness of our DiffMIC on three medical classification tasks with different image modalities, including placental maturity grading on ultrasound images, skin lesion classification using dermatoscopic images, and diabetic retinopathy grading using fundus images. Our experimental results demonstrate that DiffMIC outperforms state-of-the-art methods by a significant margin, indicating the universality and effectiveness of the proposed model. Our code is
publicly available at https://github.com/scott-yjyang/DiffMIC.",https://github.com/scott-yjyang/DiffMIC,https://challenge.isic-archive.com/data/#2018,Computer Aided Diagnosis,Dermatology,Ophthalmology,Ultrasound,,,,,,
DiffMix: Diffusion Model-based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets ,"Nuclei segmentation and classification is a significant process in pathology image analysis. Deep learning-based approaches have greatly contributed to the higher accuracy of this task. However, those approaches suffer from the imbalanced nuclei data composition, which shows lower classification performance on the rare nuclei class. In this paper, we propose a realistic data synthesis method using a diffusion model. We generate two types of virtual patches to enlarge the training
data distribution, which is for balancing the nuclei class variance and for enlarging the chance to look at various nuclei. After that, we use a semantic-label-conditioned diffusion model to generate realistic and high-quality image samples. We demonstrate the efficacy of our method by experiment results on two imbalanced nuclei datasets, improving the state-of-the-art networks. The experimental results suggest that the proposed method improves the classification performance of the rare type nuclei classification, while showing superior segmentation and classification performance in imbalanced pathology nuclei datasets.",https://github.com/hvcl/DiffMix,CoNSeP: https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/,Other,Image Segmentation,Histopathology,,,,,,,
DiffULD: Diffusive Universal Lesion Detection ,"Universal Lesion Detection (ULD) in computed tomography (CT) plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by anchor-based detection designs, but they have inherent drawbacks due to the use of anchors: i) Insufficient training target and ii) Difficulties in anchor design. Diffusion probability models (DPM) have demonstrated outstanding capabilities in many vision tasks. Many DPM-based approaches achieve great success in natural image object detection without using anchors. But they are still ineffective in ULD due to the insufficient training targets.In this paper, we propose a novel ULD method, DiffULD, which utilizes DPM for lesion detection. To tackle the negative effect triggered by insufficient targets, we introduce a novel Center-aligned bounding box (BBox) padding strategy that provides additional high-quality training targets yet avoids significant performance deterioration. DiffULD is inherently advanced in locating lesions with diverse sizes andshapes since it can predict with arbitrary boxes. Experiments on the benchmark dataset DeepLesion show the superiority of DiffULD when compared to state-of-the-art ULD approaches.",https://github.com/momopusheen/DiffULD,https://nihcc.app.box.com/v/DeepLesion,Computer Aided Diagnosis,Image Segmentation,CT,,,,,,,
DiffuseIR: Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images ,"Three-dimensional microscopy is often limited by anisotropic spatial resolution, resulting in lower axial resolution than lateral resolution. Current State-of-The-Art (SoTA) isotropic reconstruction methods utilizing deep neural networks can achieve impressive super-resolution performance in fixed imaging settings. However, their generality in practical use is limited by degraded performance caused by artifacts and blurring when facing unseen anisotropic factors. To address these issues, we propose DiffuseIR, an unsupervised method for isotropic reconstruction based on diffusion models. First, we pre-train a diffusion model to learn the structural distribution of biological tissue from lateral microscopic images, resulting in generating naturally high-resolution images. Then we use low-axial-resolution microscopy images to condition the generation process of the diffusion model and generate high-axial-resolution reconstruction results. Since the diffusion model learns the universal structural distribution of biological tissues, which is independent of the axial resolution, DiffuseIR can reconstruct authentic images with unseen low-axial resolutions into a high-axial resolution without requiring re-training. The proposed DiffuseIR achieves SoTA performance in experiments on EM data and can even compete with supervised methods.",,,Image Reconstruction,Microscopy,,,,,,,,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI ,"Recent researches on cancer segmentation in dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) usually resort to the combination of temporal kinetic characteristics and deep learning to improve segmentation performance. However, the difficulty in accessing complete temporal sequences, especially post-contrast images, hinders segmentation performance, generalization ability and clinical application of existing methods. In this work, we propose a diffusion kinetic model (DKM) that implicitly exploits hemodynamic priors in DCE-MRI and effectively generates high-quality segmentation maps only requiring pre-contrast images. We specifically consider the underlying relation between hemodynamic response function (HRF) and denoising diffusion process (DDP), which displays remarkable results for realistic image generation. Our proposed DKM consists of a diffusion module (DM) and segmentation module (SM) so that DKM is able to learn cancer hemodynamic information and provide a latent kinetic code to facilitate segmentation performance. Once the DM is pretrained, the latent code estimated from the DM is simply incorporated into the SM, which enables DKM to automatically and accurately annotate cancers with pre-contrast images. To our best knowledge, this is the first work exploring the relationship between HRF and DDP for dynamic MRI segmentation. We evaluate the proposed method for tumor segmentation on public breast cancer DCE-MRI dataset. Compared to the existing state-of-the-art approaches with complete sequences, our method yields higher segmentation performance even with pre-contrast images. The source code will be available on https://github.com/Medical-AI-Lab-of-JNU/DKM.",https://github.com/Medical-AI-Lab-of-JNU/DKM,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=22513764,Image Segmentation,MRI,,,,,,,,
Diffusion Transformer U-Net for Medical Image Segmentation ,"Diffusion model has shown its power on various generation tasks. When applying the diffusion model in medical image segmentation, there are a few roadblocks to remove: the semantic features required for the conditioning of the diffusion process are not well aligned with the noise embedding; and the U-Net backbone employed in these diffusion models is not sensitive to contextual information that is essential during the reverse diffusion process for accurate pixel-level segmentation. To overcome these limitations, we present a cross-attention module to enhance the conditioning from source images, and a transformer based U-Net with multi-sized windows for the extraction of various scales of contextual information. Evaluated on five benchmark datasets with different imaging modalities including Kvasir-Seg, CVC Clinic DB, ISIC 2017, ISIC 2018, and Refuge, our diffusion transformer U-Net achieves great generalization ability and outperforms all the state-of-the-art models on these datasets.",,,Image Segmentation,Attention models,,,,,,,,
Diffusion-based Data Augmentation for Nuclei Image Segmentation ,"Nuclei segmentation is a fundamental but challenging task in the quantitative analysis of histopathology images. Although fully-supervised deep learning-based methods have made significant progress, a large number of labeled images are required to achieve great segmentation performance. Considering that manually labeling all nuclei instances for a dataset is inefficient, obtaining a large-scale human-annotated dataset is time-consuming and labor-intensive. Therefore, augmenting a dataset with only a few labeled images to improve the segmentation performance is of significant research and application value. In this paper, we introduce the first diffusion-based augmentation method for nuclei segmentation. The idea is to synthesize a large number of labeled images to facilitate training the segmentation model. To achieve this, we propose a two-step strategy. In the first step, we train an unconditional diffusion model to synthesize the Nuclei Structure that is defined as the representation of pixel-level semantic and distance transform. Each synthetic nuclei structure will serve as a constraint on histopathology image synthesis and is further post-processed to be an instance map. In the second step, we train a conditioned diffusion model to synthesize histopathology images based on nuclei structures. The synthetic histopathology images paired with synthetic instances maps will be added to the real dataset for training the segmentation model. The experimental results show that by augmenting 10% labeled real dataset with synthetic samples, one can achieve comparable segmentation results with the fully-supervised baseline.",,,Computer Aided Diagnosis,Data Efficient Learning,Histopathology,Microscopy,,,,,,
Diffusion-Based Hierarchical Multi-Label Object Detection to Analyze Panoramic Dental X-rays ,"Due to the necessity for precise treatment planning, the use of panoramic X-rays to identify different dental diseases has tremendously increased. Although numerous ML models have been developed for the interpretation of panoramic X-rays, there has not been an end-to-end model developed that can identify problematic teeth with dental enumeration and associated diagnoses at the same time. To develop such a model, we structure the three distinct types of annotated data hierarchically following the FDI system, the first labeled with only quadrant, the second labeled with quadrant-enumeration, and the third fully labeled with quadrant-enumeration-diagnosis. To learn from all three hierarchies jointly, we introduce a novel diffusion-based hierarchical multi-label object detection framework by adapting a diffusion-based method that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Specifically, to take advantage of the hierarchically annotated data, our method utilizes a novel noisy box manipulation technique by adapting the denoising process in the diffusion network with the inference from the previously trained model in hierarchical order. We also utilize a multi-label object detection method to learn efficiently from partial annotations and to give all the needed information about each abnormal tooth for treatment planning. Experimental results show that our method significantly outperforms state-of-the-art object detection methods, including RetinaNet, Faster R-CNN, DETR, and DiffusionDet for the analysis of panoramic X-rays, demonstrating the great potential of our method for hierarchically and partially annotated datasets. The code and the datasets are available at https://github.com/ibrahimethemhamamci/HierarchicalDet.",https://github.com/ibrahimethemhamamci/HierarchicalDet,https://github.com/ibrahimethemhamamci/DENTEX,Computer Aided Diagnosis,Data Efficient Learning,Other,Transfer learning,other,,,,,
DiMix: Disentangle-and-Mix based domain generalizable medical image segmentation ,"The rapid advancements in deep learning have revolutionized multiple domains, yet the significant challenge lies in effectively applying this technology to novel and unfamiliar environments, particularly in specialized and costly fields like medicine. Recent deep learning research has therefore focused on domain generalization, aiming to train models that can perform well on datasets from unseen environments. This paper introduces a novel framework that enhances generalizability by leveraging transformer-based disentanglement learning and style mixing. Our framework identifies features that are invariant across different domains. Through a combination of content-style disentanglement and image synthesis, the proposed method effectively learns to distinguish domain-agnostic features, resulting in improved performance when applied to unseen target domains. To validate the effectiveness of the framework, experiments were conducted on a publicly available Fundus dataset, and comparative analyses were performed against other existing approaches. The results demonstrated the power and efficacy of the proposed framework, showcasing its ability to enhance domain generalization performance.",,,Model Generalizability / Federated Learning,Computer Aided Diagnosis,,,,,,,,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration ,"Multimodal image registration is a challenging but essential step for numerous image-guided procedures. 
Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities.
Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings.
We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration.We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data.
Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one.
Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining.
We make our training code and data publicly available.",https://github.com/ImFusionGmbH/DISA-universal-multimodal-registration,https://doi.org/10.5281/zenodo.583096,Image Registration,Abdomen,Neuroimaging - Others,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI,Ultrasound,,,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms using Self-adversarial Learning ,"Asymmetry is a crucial characteristic of bilateral mammograms (Bi-MG) when abnormalities are developing. It is widely utilized by radiologists for diagnosis. The question of “what the symmetrical Bi-MG would look like when the asymmetrical abnormalities have been removed ?” has been not yet drawn strong attention in the development of algorithms on mammograms. Addressing this question could provide valuable insight into mammographic anatomy and aid in diagnostic interpretation. Hence, we propose a novel framework, DisAsymNet, which utilizes asymmetrical abnormality transformer guided self-adversarial learning for disentangling abnormalities and symmetric Bi-MG. At the same time, our proposed method is partially guided by randomly synthesized abnormalities. We conduct experiments on three public and one in-house dataset, and demonstrate that our method outperforms existing methods in abnormality classification, segmentation, and localization tasks. Additionally, reconstructed normal mammograms can provide insights toward better interpretable visual cues for clinical diagnosis. The code will be accessible to the public.",https://github.com/xinwangxinwang/DisAsymNet,https://pubmed.ncbi.nlm.nih.gov/22078258/,Breast,Image Reconstruction,Image Segmentation,Attention models,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution ,"Multi-contrast magnetic resonance imaging (MRI) is the most common management tool used to characterize neurological disorders based on brain tissue contrasts. However, acquiring high-resolution MRI scans is time-consuming and infeasible under specific conditions. Hence, multi-contrast super-resolution methods have been developed to improve the quality of low-resolution contrasts by leveraging complementary information from multi-contrast MRI. Current deep learning-based super-resolution methods have limitations in estimating restoration uncertainty and avoiding mode collapse. Although the diffusion model has emerged as a promising approach for image enhancement, capturing complex interactions between multiple conditions introduced by multi-contrast MRI super-resolution remains a challenge for clinical applications. In this paper, we propose a disentangled conditional diffusion model, DisC-Diff, for multi-contrast brain MRI super-resolution. It utilizes the sampling-based generation and simple objective function of diffusion models to estimate uncertainty in restorations effectively and ensure a stable optimization process. Moreover, DisC-Diff leverages a disentangled multi-stream network to fully exploit complementary information from multi-contrast MRI, improving model interpretation under multiple conditions of multi-contrast inputs. We validated the effectiveness of DisC-Diff on two datasets: the IXI dataset, which contains 578 normal brains, and a clinical dataset with 316 pathological brains. Our experimental results demonstrate that DisC-Diff outperforms other state-of-the-art methods both quantitatively and visually.",https://github.com/Yebulabula/DisC-Diff,https://drive.google.com/drive/folders/1i2nj-xnv0zBRC-jOtu079Owav12WIpDE,Image Reconstruction,Oncology,Attention models,Other,Uncertainty,MRI,,,,
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network ,"Previous studies have shown that neurodegenerative diseases, specifically Alzheimer’s disease (AD), primarily affect brain network function due to neuropathological burdens that spread throughout the network, similar to prion-like propagation. Therefore, identifying brain network alterations is crucial in understanding the pathophysiological mechanism of AD progression. Although recent graph neural network (GNN) analyses have provided promising results for early AD diagnosis, current methods do not account for the unique topological properties and high order information in complex brain networks. To address this, we propose a brain network-tailored hypergraph neural network (BrainHGNN) to identify the propagation patterns of neuropathological events in AD. Our BrainHGNN approach constructs a hypergraph using region of interest (ROI) position encoding and random-walk-based sampling strategy, preserving the unique identities of brain regions and characterizing the intrinsic properties of the brain-network organization. We then propose a self-learned weighted hypergraph convolution to iteratively update node and hyperedge messages and identify AD-related propagation patterns. We conducted extensive experiments on ADNI data, demonstrating that our BrainHGNN outperforms other state-of-the-art methods in classification performance and identifies significant propagation patterns with discriminative differences in group comparisons.",,,Computer Aided Diagnosis,Neuroimaging - Brain Development,MRI,PET/SPECT,,,,,,
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization ,"Modern multi-site neuroimaging studies are known to be biased by significant site effects observed in imaging data and their derived structural and functional features. Although many statistical models and deep learning methods have been proposed to eliminate the site effects while maintaining biological characteristics, they have two major drawbacks. First, statistical models are applicable for harmonizing regional-level data but are inherently not suitable to represent the complex non-linear mapping of vertex-wise cortical property maps. Second, existing deep learning methods can only harmonize data between two sites, which are practically less useful in multi-site data harmonization scenario and also ignore the rich information in the whole dataset. To address these issues, we develop a novel, flexible deep learning method to harmonize multi-site cortical surface property maps. Specifically, to detect and remove site effects, we employ a surface-based autoencoder and decompose the encoded cortical features into site-related and site-unrelated components and use an adversarial strategy to encourage the disentanglement. Then decoding the site-unrelated features with other site features can generate mappings across different sites. To learn more controllable and meaningful mappings, we also enforce the cycle consistency between forward and backward mappings. Our method can thus efficiently learn rich information from the whole dataset and generate realistic harmonized surface maps at the target site. Experiments on harmonizing infant cortical thickness maps of 2,342 scans from four sites with different scanners and imaging protocols validate the superior performance of our method on both site effects removal and biological variability preservation compared to other methods. To the best of our knowledge, this is the largest validation of different methods on infant cortical data harmonization.",,,Neuroimaging - Brain Development,Neuroimaging - Others,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Distilling BlackBox to Interpretable models for Efficient Transfer Learning ,"Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (e.g., scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this pa- per, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain- invariant. However, interpretable models typically underperform com- pared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a mixture of shallow interpretable models us- ing human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Further, we use the pseudo-labeling technique from semi-supervised learning (SSL) to learn the concept classifier in the target domain, followed by fine-tuning the interpretable models in the target domain. We evaluate our model using a real-life large-scale chest-X-ray (CXR) classification dataset. The code can be found at: https://github.com/annonymous-vision/miccai.",https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs,https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs#downloading-data,Interpretability / Explainability,Transfer learning,,,,,,,,
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI ,"Magnetic Resonance Imaging (MRI) acceleration techniques using k-space sub-sampling (KS) can greatly improve the efficiency of MRI-based stroke diagnosis. Although Deep Neural Networks (DNN) have shown great potential on stroke lesion recognition tasks when the MR images are reconstructed from the full k-space, they are vulnerable to the lower quality MR images generated by KS. In this paper, we propose a Distributionally Robust Learning (DRL) approach to improve the performance of stroke recognition DNN models when the MR images are reconstructed from the sub-sampled k-space. For Convolutional Neural Network (CNN) and Vision Transformer (ViT)-based models, our methods improve the stroke classification AUROC and AUPRC by up to 11.91% and 9.32% on the KS-perturbed brain MR images, respectively, compared against Empirical Risk Minimization (ERM) and other baseline defensive methods. We further show that DRL models can successfully recognize the stroke cases from highly perturbed MR images where clinicians may fail, which provides a solution for improved diagnosis in an accelerated MRI setting.",https://github.com/noc-lab/drl_mri,https://brain-development.org/ixi-dataset/,Computer Aided Diagnosis,MRI,Treatment Response and Outcome/Disease Prediction,,,,,,,
Diversity-preserving Chest Radiographs Generation from Reports in One Stage ,"Automating the analysis of chest radiographs based on deep learning algorithms has the potential to improve various steps of the radiology workflow. Such algorithms require large, labeled and domain-specific datasets, which are difficult to obtain due to privacy concerns and laborious annotations. Recent advances in generating X-rays from radiology reports provide a possible remedy for this problem. However, due to the complexity of medical images, existing methods synthesize low-fidelity X-rays and cannot guarantee image diversity. In this paper, we propose a diversity-preserving report-to-X-ray generation method with one-stage architecture, named DivXGAN. Specifically, we design a domain-specific hierarchical text encoder to extract medical concepts inherent in reports. This information is incorporated into a one-stage generator, along with the latent vectors, to generate diverse yet relevant X-ray images. Extensive experiments on two widely used datasets, namely Open-i and MIMIC-CXR, demonstrate the high fidelity and diversity of our synthesized chest radiographs. Furthermore, we demonstrate the efficacy of the generated X-rays in facilitating supervised downstream applications via a multi-label classification task.",,,Computer Aided Diagnosis,Other,other,Text (clinical/radiology reports),,,,,,
DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction ,"Accurate 3D cardiac reconstruction from cine magnetic resonance imaging (cMRI) is crucial for improved cardiovascular disease diagnosis and understanding of the heart’s motion. However, current cardiac MRI-based reconstruction technology used in clinical settings is 2D with limited through-plane resolution, resulting in low-quality reconstructed cardiac volumes. To better reconstruct 3D cardiac volumes from sparse 2D image stacks, we propose a morphology-guided diffusion model for 3D cardiac volume reconstruction, DMCVR, that synthesizes high-resolution 2D images and corresponding 3D reconstructed volumes. Our method outperforms previous approaches by conditioning the cardiac morphology on the generative model, eliminating the time-consuming iterative optimization process of the latent code, and improving generation quality. The learned latent spaces provide global semantics, local cardiac morphology and details of each 2D cMRI slice with highly interpretable value to reconstruct 3D cardiac shape. Our experiments show that DMCVR is highly effective in several aspects, such as 2D generation and 3D reconstruction performance. With DMCVR, we can produce high-resolution 3D cardiac MRI reconstructions, surpassing current techniques. Our proposed framework has great potential for improving the accuracy of cardiac disease diagnosis and treatment planning. Code can be accessed at https://github.com/hexiaoxiao-cs/DMCVR.",https://github.com/hexiaoxiao-cs/DMCVR,https://www.ukbiobank.ac.uk,Cardiac,Image Reconstruction,MRI,,,,,,,
Do we really need that skip-connection? Understanding its interplay with task complexity ,"The U-Net architecture has become the preferred model used for medical image segmentation tasks. Since its inception, several variants have been proposed. An important component of the U-Net architecture is the use of skip-connections, said to carry over image details on its decoder branch at different scales. However, beyond this intuition, not much is known as to what extent skip-connections of the U-Net are necessary, nor what their interplay is in terms of model robustness when they are subjected to different levels of task complexity. In this study we analyzed these questions using three variants of the UNet architecture (the standard U-Net, a “No-Skip” U-Net, and an Attention-Gated U-Net) using controlled experiments on varying synthetic texture images, and evaluated these findings on three medical image data sets. We measured task complexity as a function of texture-based similarities between foreground and background distributions.  Using this scheme, our findings suggest that the benefit of employing skip-connections is small for low-to-medium complexity tasks, and its benefit appear only when the task complexity becomes large. We report that such incremental benefit is non-linear, with the attention-gate U-Net yielding larger improvements.  Furthermore, we find that these benefits also bring along robustness degradations on clinical data sets, particularly in out-of-domain scenarios. These results suggest a dependency between task complexity and the choice/design of noise-resilient skip-connections, indicating the need for careful consideration while using these skip-connections.",https://github.com/amithjkamath/to_skip_or_not,https://scholar.cu.edu.eg/Dataset_BUSI.zip,Image Segmentation,Abdomen,Breast,Cardiac,Interpretability / Explainability,Model Generalizability / Federated Learning,CT,MRI,Ultrasound,
Domain Adaptation for Medical Image Segmentation using Transformation-Invariant Self-Training ,"Models capable of leveraging unlabelled data are crucial in overcoming large distribution gaps between the acquired datasets across different imaging devices and configurations. In this regard, self-training techniques based on pseudo-labeling have been shown to be highly effective for semi-supervised domain adaptation. However, the unreliability of pseudo labels can hinder the capability of self-training techniques to induce abstract representation from the unlabeled target dataset, especially in the case of large distribution gaps. 
Since the neural network performance should be invariant to image transformations, we look to this fact to identify uncertain pseudo labels. Indeed, we argue that transformation invariant detections can provide more reasonable approximations of ground truth. Accordingly, we propose a semi-supervised learning strategy for domain adaptation termed transformation-invariant self-training (TI-ST). The proposed method assesses pixel-wise pseudo-labels’ reliability and filters out unreliable detections during self-training. We perform comprehensive evaluations for domain adaptation using three different modalities of medical images, two different network architectures, and several alternative state-of-the-art domain adaptation methods. Experimental results confirm the superiority of our proposed method in mitigating the lack of target domain annotation and boosting segmentation performance in the target domain.",https://github.com/Negin-Ghamsarian/Transformation-Invariant-Self-Training-MICCAI23,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Ophthalmology,Image Segmentation,Video,,,,,,
Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion MRI ,"The human thalamus is a subcortical brain structure that comprises dozens of nuclei with different function and connectivity, which are affected differently by disease. For this reason, there is growing interest in studying the thalamic nuclei in vivo with MRI. Tools are available to segment the thalamus from 1 mm T1 scans, but the image contrast is too faint to produce reliable segmentations. Some tools have attempted to refine these boundaries using diffusion MRI information, but do not generalise well across diffusion MRI acquisitions. Here we present the first CNN that can segment thalamic nuclei from T1 and diffusion data of any resolution without retraining or fine tuning. Our method builds on our histological atlas of the thalamic nuclei and silver standard segmentations on high-quality diffusion data obtained with our recent Bayesian adaptive segmentation tool. We combine these with an approximate degradation model for fast domain randomisation during training. Our CNN produces a segmentation at 0.7 mm isotropic resolution, irrespective of the resolution of the input. Moreover, it uses a parsimonious model of the diffusion signal (fractional anisotropy and principal eigenvector) that is compatible with virtually any set of directions and b-values, including huge amounts of legacy data. We show results of our proposed method on three heterogeneous datasets acquired on dozens of different scanners. The method is publicly available at freesurfer.net/fswiki/ThalamicNucleiDTI.",https://github.com/htregidgo/joint_diffusion_structural_seg,https://www.humanconnectome.org/,Neuroimaging - DWI and Tractography,Neuroimaging - Others,Image Segmentation,MRI,,,,,,
DOMINO++: Domain-aware Loss Regularization for Deep Learning Generalizability ,"Out-of-distribution (OOD) generalization poses a serious chal-
lenge for modern deep learning (DL). OOD data consists of test data that
is significantly different from the model’s training data. DL models that
perform well on in-domain test data could struggle on OOD data. Over-
coming this discrepancy is essential to the reliable deployment of DL.
Proper model calibration decreases the number of spurious connections
that are made between model features and class outputs. Hence, cal-
ibrated DL can improve OOD generalization by only learning features
that are truly indicative of the respective classes. Previous work proposed
domain-aware model calibration (DOMINO) to improve DL calibration,
but it lacks designs for model generalizability to OOD data. In this
work, we propose DOMINO++, a dual-guidance and dynamic domain-
aware loss regularization focused on OOD generalizability. DOMINO++
integrates expert-guided and data-guided knowledge in its regulariza-
tion. Unlike DOMINO which imposed a fixed scaling and regularization
rate, DOMINO++ designs a dynamic scaling factor and an adaptive reg-
ularization rate. Comprehensive evaluations compare DOMINO++ with
DOMINO and the baseline model for head tissue segmentation from mag-
netic resonance images (MRIs) on OOD data. The OOD data consists of
synthetic noisy and rotated datasets, as well as real data using a different
MRI scanner from a separate site. DOMINO++’s superior performance
demonstrates its potential to improve the trustworthy deployment of DL
on real clinical data.",,,Image Segmentation,Model Generalizability / Federated Learning,Uncertainty,MRI,,,,,,
Dose Guidance for Radiotherapy-oriented Deep Learning Segmentation ,"Deep learning-based image segmentation for radiotherapy is intended to speed up the planning process and yield consistent results. However, most of these segmentation methods solely rely on distribution and geometry-associated training objectives without considering tumor control and the sparing of healthy tissues. To incorporate dosimetric effects into segmentation models, we propose a new training loss function that extends current state-of-the-art segmentation model training via a dose-based guidance method. We hypothesized that adding such a dose-guidance mechanism improves the robustness of the segmentation with respect to the dose (i.e., resolves distant outliers and focuses on locations of high dose/dose gradient). We demonstrate the effectiveness of the proposed method on Gross Tumor Volume segmentation for glioblastoma treatment. The obtained dosimetry-based results show reduced dose errors relative to the ground truth dose map using the proposed dosimetry-segmentation guidance, outperforming state-of-the-art distribution and geometry-based segmentation losses.",https://github.com/ruefene/doselo,,Image Segmentation,Guided Interventions and Surgery,MRI,,,,,,,
DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis ,"Multi-center positron emission tomography (PET) image synthesis aims at recovering low-dose PET images from multiple different centers. The generalizability of existing methods can still be suboptimal for a multi-center study due to domain shifts, which result from non-identical data distribution among centers with different imaging systems/protocols. While some approaches address domain shifts by training specialized models for each center, they are parameter inefficient and do not well exploit the shared knowledge across centers. To address this, we develop a generalist model that shares architecture and parameters across centers to utilize the shared knowledge. However, the generalist model can suffer from the center interference issue, i.e. the gradient directions of different centers can be inconsistent or even opposite owing to the non-identical data distribution. To mitigate such interference, we introduce a novel dynamic routing strategy with cross-layer connections that routes data from different centers to different experts. Experiments show that our generalist model with dynamic routing (DRMC) exhibits excellent generalizability across centers. Code and data are available at: https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.",https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis,,PET/SPECT,Image Reconstruction,,,,,,,,
Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI ,"Limited by the drawbacks of imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is an important problem in medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast super-resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. What’s more, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes the acquisition of reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed a dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and use a Implicit Decoding Function~(IDF) to obtain the final MRI SR results. Furthermore, we applied the curriculum learning to train our Dual-ArbNet in several stages. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential to be applied in clinical practice.",https://github.com/jmzhang79/Dual-ArbNet,http://brain-development.org/ixi-dataset/,Image Reconstruction,Computer Aided Diagnosis,Attention models,MRI,,,,,,
Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos ,"Out-of-distribution (OOD) detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dual- conditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-based OOD detection. This constrains the generative manifold of the model to generate images structurally and semantically similar to those within the in-distribution. The proposed model out- performs reference methods with a 12% improvement in accuracy, 22% higher precision, and an 8% better F1 score.",https://github.com/FetalUltrasound/DCDM,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Fetal Imaging,Ultrasound,,,,,,,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty ,"Magnetic resonance imaging (MRI) may degrade with motion artifacts in the reconstructed MR images due to the long acquisition time. In this paper, we propose a dual domain motion correction network (D2MC-Net) to correct the motion artifacts in 2D multi-slice MRI. Instead of explicitly estimating the motion parameters, we model the motion corruption by k-space uncertainty to guide the MRI reconstruction in an unfolded deep reconstruction network. Specifically, we model the motion correction task as a dual domain regularized model with an uncertainty-guided data consistency term. Inspired by its alternating iterative optimization algorithm, the D2MC-Net is composed of multiple stages, and each stage consists of a k-space uncertainty module (KUModule) and a dual domain reconstruction module (DDR-Module). The KU-Module quantifies the uncertainty of k-space corruption by motion. The DDR-Module reconstructs motion-free k-space data and MR image in both k-space and image domain, under the guidance of the k-space uncertainty. Extensive experiments on fastMRI dataset demonstrate that the proposed D2MC-Net outperforms state-of-the-art methods under different motion trajectories and motion severities.",https://github.com/Jiazhen-Wang/D2MC-Net-main,,Image Reconstruction,MRI,,,,,,,,
DULDA: Dual-domain Unsupervised Learned Descent Algorithm for PET image reconstruction ,"Deep learning based PET image reconstruction methods have achieved promising results recently. However, most of these methods follow a supervised learning paradigm, which rely heavily on the availability of high-quality training labels. In particular, the long scanning time required and high radiation exposure associated with PET scans make obtaining these labels impractical. In this paper, we propose a dual-domain unsupervised PET image reconstruction method based on learned descent algorithm, which reconstructs high-quality PET images from sinograms without the need for image labels. Specifically, we unroll the proximal gradient method with a learnable l2,1 norm for PET image reconstruction problem. The training is unsupervised, using measurement domain loss based on deep image prior as well as image domain loss based on rotation equivariance property. The experimental results demonstrate the superior performance of proposed method compared with maximum-likelihood expectation–maximization (MLEM), total-variation regularized EM (EM-TV) and deep image prior based method (DIP).",,,Image Reconstruction,PET/SPECT,,,,,,,,
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification ,"This paper presents an innovative approach to curriculum learning, which is a technique used to train learning models. Curriculum learning is inspired by the way humans learn, starting with simple examples and gradually progressing to more challenging ones. There are currently two main types of curriculum learning: fixed curriculum generated by transfer learning, and self-paced learning based on loss functions. However, these methods have limitations that can hinder their effectiveness. To overcome these limitations, this article proposes a new approach called Dynamic Curriculum Learning via in-domain uncertainty (DCLU), which is derived from uncertainty estimation. The proposed approach utilizes a Dirichlet distribution classifier to obtain prediction and uncertainty estimates from the network, which can be used as a metric to quantify the difficulty level of the data. An uncertainty-aware sampling pacing function is also introduced to adapt the curriculum according to the difficulty metric. This new approach has been evaluated on two medical image datasets, and the results show that it outperforms
other curriculum learning methods. The source code for this approach will be released at https://github.com/Joey2117/DCLU.",https://github.com/Joey2117/DCLU,,Computer Aided Diagnosis,Dermatology,Lung,Uncertainty,Treatment Response and Outcome/Disease Prediction,,,,,
Dynamic Functional Connectome Harmonics ,"Functional connectivity (FC)gradients'' enable the investigation of connection topography of cognitive hierarchies and yield the primary axes along which FC is organized. In this work, we employ a variant of thegradient’’ approach wherein we solve for the normal modes of FC, yielding functional connectome harmonics. Until now, research in this vein has only considered static FC, neglecting the possibility that the principal axes of FC may depend on the timescale at which they are computed. Recent work suggests that momentary activation patterns, or brain states, mediate the dominant components of functional connectivity, suggesting that the principal axes may be invariant to change in timescale. In light of this, we compute functional connectome harmonics using time windows of varying lengths and demonstrate that they are stable across timescales. Our connectome harmonics correspond to meaningful brain states. The activation strength of the brain states and their inter-relationships are found to be reproducible for individuals. Further, we utilize our time-varying functional connectome harmonics to formulate a simple and elegant method for computing cortical flexibility at vertex resolution. Finally, we demonstrate qualitative similarity between flexibility maps from our method and a method standard in the literature.",,,Neuroimaging - Functional Brain Networks,Other,MRI,,,,,,,
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases ,"The number of stroke patients is growing worldwide and half of them will suffer from cognitive impairment. Therefore, the prediction of PSCI becomes more and more important. However, the determinants and mechanisms of PSCI are still insufficiently understood, making this task challenging. In this paper, we propose a multi-modal fusion model to solve this task. First, dynamic graph neural representation is proposed to utilize and integrate multi-modal information, i.e. clinical tabular data and image data, which can divide them into node-level and global-level properties, instead of processing these data uniformly. Second, considering the variability of patient brain anatomy, a subject-specific undirected graph is constructed based on the connections among 131 brain anatomical regions segmented from image data, while first-order statistical features are extracted from each brain region and internal stroke lesions as node features. Meanwhile, a novel missing information compensation module is proposed to reduce the impact of missing elements in tabular data. In dynamic graph neural representation, two kinds of attention mechanisms are embedded, which encourage the model to automatically localize brain anatomical regions that are highly relevant to this task. One is node attention established between global tabular neural representation and nodes, the other is multi-head graph self-attention which changes the static undirected graph to several dynamic directed graphs and optimizes the broadcasting process of the graph. The proposed method achieves the best overall performance with a balanced accuracy score of 79.6%, outperforming the competing models.The code is publicly available at github.com/fightingkitty/MHGSA",https://github.com/fightingkitty/MHGSA,,Imaging Biomarkers,Neuroimaging - Others,Attention models,Other,MRI,Treatment Response and Outcome/Disease Prediction,,,,
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN using T1-MRI ,"Constructing structural brain networks using T1-weighted MRI (T1-MRI) presents a significant challenge due to the lack of direct regional connectivity. Current methods with T1-MRI rely on predefined regions or isolated pretrained modules to localize atrophy regions, which neglects individual specificity. Besides, existing methods capture global structural context only on the whole-image-level, which weaken correlation between regions and the hierarchical distribution nature of brain structure. We hereby propose a novel dynamic structural brain network construction method based on T1-MRI, which can dynamically localize critical regions and constrain the hierarchical distribution among them. Specifically, we first cluster spatially-correlated channel and generate several critical brain regions as prototypes. Then, we introduce a contrastive loss function to constrain the prototypes distribution, which embed the hierarchical brain semantic structure into the latent space. Self-attention and GCN are then used to dynamically construct hierarchical correlations of critical regions for brain network and explore the correlation, respectively. Our method is trained on ADNI-1 and tested on ADNI-2 databases for mild cognitive impairment (MCI) conversion prediction, and acheive the state-of-the-art (SOTA) performance.",https://github.com/Leng-10/DH-ProGCN,https://adni.loni.usc.edu/,Neuroimaging - Others,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Interpretability / Explainability,Other,MRI,,,,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms ,"The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle.
The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. 
However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss.  We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.",https://github.com/MasoudMo/echoglad,https://data.unityimaging.net/,Image Segmentation,Cardiac,Model Generalizability / Federated Learning,Ultrasound,Video,,,,,
ECL: Class-Enhancement Contrastive Learning for Long-tailed Skin Lesion Classification ,"Skin image datasets often suffer from imbalanced data distribution, exacerbating the difficulty of computer-aided skin disease diagnosis.  Some recent works exploit supervised contrastive learning (SCL) for this long-tailed challenge. Despite achieving significant performance, these SCL-based methods focus more on head classes, yet ignoring the utilization of information in tail classes. In this paper, we propose class-Enhancement Contrastive Learning (ECL), which enriches the information of minority classes and treats different classes equally. For information enhancement, we design a hybrid-proxy model to generate class-dependent proxies and propose a cycle update strategy for parameters optimization. A balanced-hybrid-proxy loss is designed to exploit relations between samples and proxies with different classes treated equally. Taking both “imbalanced data” and “imbalanced diagnosis difficulty” into account, we further present a balanced-weighted cross-entropy loss following curriculum learning schedule. Experimental results on the classification of imbalanced skin lesion data have demonstrated the superiority and effectiveness of our method. The codes can be publicly available from https://github.com/zylbuaa/ECL.git.",https://github.com/zylbuaa/ECL.git,,Other,Dermatology,Computer Aided Diagnosis,,,,,,,
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation ,"Active learning algorithms have become increasingly popular
for training models with limited data. However, selecting data for annotation remains a challenging problem due to the limited information available on unseen data. To address this issue, we propose EdgeAL, which
utilizes the edge information of unseen images as a priori information for
measuring uncertainty. The uncertainty is quantified by analyzing the di-
vergence and entropy in model predictions across edges. This measure
is then used to select superpixels for annotation. We demonstrate the
effectiveness of EdgeAL on multi-class Optical Coherence Tomography
(OCT) segmentation tasks, where we achieved a 99% Dice score while
reducing the annotation label cost to 12%, 2.3%, and 3%, respectively,
on three publicly available datasets (Duke, AROI, and UMN).",https://github.com/Mak-Ta-Reque/EdgeAL,https://ipg.fer.hr/ipg/resources/oct_image_database,Active Learning,Ophthalmology,Image Segmentation,CT,,,,,,
Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI ,"Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, current methods do not have an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, which makes these tasks challenging. To address these issues, this paper proposes a unified framework, called edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on multi-modality NCMRI. The EaMtNet is made edge-aware by using the newly designed edge-aware feature aggregation module (EaFA) for feature fusion and selection, which captures long-range dependency between feature and edge maps. Multi-tasking improves segmentation and quantification performance by leveraging prediction discrepancy to estimate uncertainty. Extensive experiments are performed on multi-modality NCMRI with 250 clinical subjects. The proposed model outperforms the state-of-the-art by a large margin, which demonstrate the potential of EaMtNet as a reliable clinical-aided tool for medical image analysis.",,,Image Segmentation,MRI,,,,,,,,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness ,"Lyme disease is a severe skin disease caused by tick bites, which affects hundreds of thousands of people. One task in diagnosing Lyme disease is lesion segmentation, i.e., separating benign skin from lesions, which can not only help clinicians to focus on lesions but also improve downstream tasks such as disease classification. However, it is challenging to segment Lyme disease lesions due to the lack of well-segmented, labeled Lyme datasets and the nature of Lyme, e.g., the typical bull’s eye lesion and its closeness to normal skin. In this paper, we design a simple yet novel data preprocessing and alteration method, called EdgeMixup, to help segment Lyme lesions on imbalanced training datasets. The key insight is to deploy a linear combination of lesion edge, either detected or computed, and the source image highlights the affected lesion area so that a learning model focuses more on the preserved lesion structure instead of skin tone, thus iteratively improving segmentation performance. Additionally, the improved edge from lesion segmentation can be further used for Lyme disease classification—e.g., in differentiating Lyme from other similar lesions including tinea corporis and herpes zoster—with improved model fairness on different subpopulations.",https://github.com/Haolin-Yuan/EdgeMixup,https://github.com/Haolin-Yuan/EdgeMixup,Image Segmentation,Dermatology,,,,,,,,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery ,"Phacoemulsification cataract surgery (PCS) is typically performed under a surgical microscope and adhering to standard procedures. The success of this surgery depends heavily on the seniority and experience of the ophthalmologist performing it. In this study, we developed an augmented reality (AR) guidance system to enhance the intraoperative skills of ophthalmologists by proposing a two-stage spatiotemporal learning network for surgical microscope video recognition. In the first stage, we designed a multi-task network that recognizes surgical phases and segments the limbus region to extract limbus-focused spatial features. In the second stage, we developed a temporal pyramid-based spatiotemporal feature aggregation (TP-SFA) module that uses causal and dilated temporal convolution for smooth and online surgical phase recognition. To provide phase-specific AR guidance, we designed several intraoperative visual cues based on the parameters of the fitted limbus ellipse and the recognized surgical phase. The comparison experiments results indicate that our method outperforms several strong baselines in surgical phase recognition. Furthermore, ablation experiments show the positive effects of the multi-task feature extractor and TP-SFA module. Our developed system has the potential for clinical application in PCS to provide real-time intraoperative AR guidance.",,,Ophthalmology,Guided Interventions and Surgery,Microscopy,,,,,,,
Efficient Subclass Segmentation in Medical Images ,"As research interests in medical image analysis become increasingly fine-grained, the cost for extensive annotation also rises. One feasible way to reduce the cost is to annotate with coarse-grained superclass labels while using limited fine-grained annotations as a complement. In this way, fine-grained data learning is assisted by ample coarse annotations. Recent studies in classification tasks have adopted this method to achieve satisfactory results. However, there is a lack of research on efficient learning of fine-grained subclasses in semantic segmentation tasks. In this paper, we propose a novel approach that leverages the hierarchical structure of categories to design network architecture. Meanwhile, a task-driven data generation method is presented to make it easier for the network to recognize different subclass categories. Specifically, we introduce a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superclass classifier, a Separate Normalization module that stretches the intra-class distance within the same superclass to facilitate subclass segmentation, and a HierarchicalMix model that generates high-quality pseudo labels for unlabeled samples by fusing only similar superclass regions from labeled and unlabeled images. Our experiments on the Brats2021 and ACDC datasets demonstrate that our approach achieves comparable accuracy to a model trained with full subclass annotations, with limited subclass annotations and sufficient superclass annotations. Our approach offers a promising solution for efficient fine-grained subclass segmentation in medical images. Our code is publicly available here.",https://github.com/OvO1111/EfficientSubclassLearning,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,Data Efficient Learning,Image Segmentation,MRI,,,,,,,
EGE-UNet: an Efficient Group Enhanced UNet for skin lesion segmentation ,"Transformer and its variants have been widely used for medical image segmentation. However, the large number of parameter and computational load of these models make them unsuitable for mobile health applications. To address this issue, we propose a more efficient approach, the Efficient Group Enhanced UNet (EGE-UNet). We incorporate a Group multi-axis Hadamard Product Attention module (GHPA) and a Group Aggregation Bridge module (GAB) in a lightweight manner. The GHPA groups input features and performs Hadamard Product Attention mechanism (HPA) on different axes to extract pathological information from diverse perspectives. The GAB effectively fuses multi-scale information by grouping low-level features, high-level features, and a mask generated by the decoder at each stage. Comprehensive experiments on the ISIC2017 and ISIC2018 datasets demonstrate that EGE-UNet outperforms existing state-of-the-art methods. In short, compared to the TransFuse, our model achieves superior segmentation performance while reducing parameter and computation costs by 494x and 160x, respectively. Moreover, to our best knowledge, this is the first model with a parameter count limited to just 50KB. Our code is available at https://github.com/JCruan519/EGE-UNet.",https://github.com/JCruan519/EGE-UNet,ISIC2017: https://challenge.isic-archive.com/data/#2017,Image Segmentation,Computer Aided Diagnosis,Attention models,,,,,,,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-aware Network ,"Robust and accurate segmentation for elongated physiological structures is challenging, especially in the ambiguous region, such as  the corneal endothelium microscope image with uneven illumination or the fundus image with disease interference. In this paper, we present a  spatial and scale uncertainty-aware network (SSU-Net) that fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we estimate epistemic and aleatoric spatial uncertainty maps using Monte Carlo dropout to approximate Bayesian networks. Based on these spatial uncertainty maps, we propose the gated soft uncertainty-aware (GSUA) module to guide the model to focus on ambiguous regions. Second, we extract the uncertainty under different scales and propose the multi-scale uncertainty-aware (MSUA) fusion module to integrate  structure contexts from hierarchical predictions, strengthening the final prediction. Finally, we visualize the uncertainty map of final prediction, providing interpretability for segmentation results. Experiment results show that the SSU-Net performs best on cornea endothelial cell and retinal vessel segmentation tasks. Moreover, compared with counterpart uncertainty-based methods, SSU-Net is more accurate and robust.",,,Image Segmentation,Ophthalmology,Uncertainty,Microscopy,other,,,,,
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation ,"Epipolar geometry is exploited in several applications in the field 
of Cone-Beam Computed Tomography (CBCT). By leveraging consistency 
conditions between multiple views of the same scene, motion artifacts can 
be minimized, the effects of beam hardening can be reduced, and segmentation 
masks can be refined. In this work, we explore the idea of enabling deep 
learning models to access the known geometrical relations between views. 
This implicit 3D information can potentially enhance various projection 
domain algorithms such as segmentation, detection, or inpainting. We 
introduce a differentiable feature translation operator, which uses available 
projection matrices to calculate and integrate over the epipolar line in a 
second view. As an example application, we evaluate the effects of the operator 
on the task of projection domain metal segmentation. By re-sampling a stack of 
projections into orthogonal view pairs, we segment each projection image 
jointly with a second view acquired roughly 90° apart. The comparison 
with an equivalent single-view segmentation model reveals an improved 
segmentation performance of 0.95 over 0.91 measured by the dice 
coefficient. By providing an implementation of this operator as an 
open-access differentiable layer, we seek to enable future research.",https://github.com/maxrohleder/FUME,,Other,Image Segmentation,CT,,,,,,,
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning ,"Recently, spatiotemporal graphs have emerged as a concise and elegant manner of representing video clips in an object-centric fashion, and have shown to be useful for downstream tasks such as action recognition. In this work, we investigate the use of latent spatiotemporal graphs to represent a surgical video in terms of the constituent anatomical structures and tools and their evolving properties over time. To build the graphs, we first predict frame-wise graphs using a pre-trained model, then add temporal edges between nodes based on spatial coherence and visual and semantic similarity. Unlike previous approaches, we incorporate long-term temporal edges in our graphs to better model the evolution of the surgical scene and increase robustness to temporary occlusions. We also introduce a novel graph-editing module that incorporates prior knowledge and temporal coherence to correct errors in the graph, enabling improved downstream task performance. Using our graph representations, we evaluate two downstream tasks, critical view of safety prediction and surgical phase recognition, obtaining strong results that demonstrate the quality and flexibility of the learned representations.",https://github.com/CAMMA-public/SurgLatentGraph,,Surgical Scene Understanding,Guided Interventions and Surgery,Data Efficient Learning,Video,Surgical Data Science,Surgical Skill and Work Flow Analysis,,,,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos ,"Reconstructing soft tissues from stereo endoscope videos is an essential prerequisite for many medical applications. Previous methods struggle to produce high-quality geometry and appearance due to their inadequate representations of 3D scenes. To address this issue, we propose a novel neural-field-based method, called EndoSurf, which effectively learns to represent a deforming surface from an RGBD sequence. In EndoSurf, we model surface dynamics, shape, and texture with three neural fields. First, 3D points are transformed from the observed space to the canonical space using the deformation field. The signed distance function (SDF) field and radiance field then predict their SDFs and colors, respectively, with which RGBD images can be synthesized via differentiable volume rendering. We constrain the learned shape by tailoring multiple regularization strategies and disentangling geometry and appearance. Experiments on public endoscope datasets demonstrate that EndoSurf significantly outperforms existing solutions, particularly in reconstructing high-fidelity shapes. Code is available at \url{https://github.com/Ruyi-Zha/endosurf.git}.",https://github.com/Ruyi-Zha/endosurf.git,,Surgical Visualization and Mixed/Augmented/Virtual Reality,Video,,,,,,,,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions between Amyloid Cascade and Tau Propagation ,"Amyloid-beta (A$\beta$) deposition and tau neurofibrillary tangles (tau) are important hallmarks of Alzheimer’s disease (AD). Although converging evidence shows that the interaction between A$\beta$ and tau is the gateway to understanding the etiology of AD, these two AD hallmarks are often treated as independent variables in the current state-of-the-art early diagnostic model for AD, which might be partially responsible for the issue of lacking explainability. Inspired by recent progress in systems biology, we formulate the evolving biological process of A$\beta$ cascade and tau propagation into a closed-loop feedback system where the system dynamics are constrained by region-to-region white matter fiber tracts in the brain. On top of this, we conceptualize that A$\beta$-tau interaction, following the principle of optimal control, underlines the pathophysiological mechanism of AD. In this context, we propose a deep reaction-diffusion model that leverages the capital of deep learning and insights into systems biology, which allows us to (1) enhance the prediction accuracy of developing AD and (2) uncover the latent control mechanism of A$\beta$-tau interactions. We have evaluated our novel explainable deep model on the neuroimaging data in Alzheimer’s Disease Neuroimaging Initiative (ADNI), where we achieve not only a higher prediction accuracy for disease progression but also a better understanding of disease etiology than conventional (“black-box”) deep models.",,,Other,Neuroimaging - DWI and Tractography,Interpretability / Explainability,MRI,,,,,,
Enhancing Automatic Placenta Analysis through Distributional Feature Recomposition in Vision-Language Contrastive Learning ,"The placenta is a valuable organ that can aid in understanding adverse events during a pregnancy and predicting adverse events after birth. However, manual pathological examination and report generation is laborious and resource-intensive. Limitations in diagnostic performance and model efficiency have impeded previous attempts to automate placenta analysis. This study presents a novel framework for the automatic analysis of placenta images that aims to improve accuracy and efficiency. Building on previous vision-language contrastive learning (VLC) methods, we propose two enhancements, namely Pathology Report Feature Recomposition and Distributional Feature Recomposition, which increase representation robustness and mitigate feature suppression. In addition, we deploy efficient neural networks as image encoders to achieve model compression and inference acceleration. Experiments demonstrate that the proposed approach outperforms previous work in both performance and efficiency by significant margins. The benefits of our method, including enhanced efficacy and deployability, may have significant implications for reproductive healthcare, particularly in rural areas or low- and middle-income countries.",,,Computer Aided Diagnosis,Data Efficient Learning,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,Histopathology,Text (clinical/radiology reports),,,
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images ,"Recently, deep learning models have shown the potential to predict breast cancer risk and enable targeted screening strategies, but current models do not consider the change in the breast over time. In this paper, we present a new method, PRIME+, for breast cancer risk prediction that leverages prior mammograms using a transformer decoder, outperforming a state-of-the-art risk prediction method that only uses mammograms from a single time point. We validate our approach on a dataset with 16,113 exams and further demonstrate that it effectively captures patterns of changes from prior mammograms, such as changes in breast density, resulting in improved short-term and long-term breast cancer risk prediction. Experimental results show that our model achieves a statistically significant improvement in performance over the state-of-the-art based model, with a C-index increase from 0.68 to 0.73 (p < 0.05) on held-out test sets.",,,Computer Aided Diagnosis,Breast,,,,,,,,
EoFormer: Edge-oriented Transformer for Brain Tumor Segmentation ,"Accurate segmentation of brain tumors in MRI images re- quires precise detection of the edges. However, this crucial information has been overlooked by existing methods. In this paper, we introduce the Edge-oriented Transformer (EoFormer) which specifically captures and enhances edge information for brain tumor segmentation. Our approach incorporates a CNN-Transformer encoder to comprehensively improve the feature representation capability. The CNN structure captures low- level local features in the image, while the Transformer structure estab- lishes long-range dependencies between features to generate high-level global features. Additionally, the decoder of our approach utilizes two edge sharpening modules, the Edge-oriented Sobel and Laplacian mod- ules, which enhance the edge information. We also introduce efficient attention and re-parameterization techniques that make EoFormer com- putationally efficient. Experimental results on the BraTS 2020 dataset and a private medulloblastoma dataset demonstrate the superiority of our approach compared with existing state-of-the-art methods. More- over, our method achieves this with limtied model parameters and lower FLOPs, making it a promising approach for future research. The code is available at https://github.com/sd0809/EoFormer.",https://github.com/sd0809/EoFormer,https://www.med.upenn.edu/cbica/brats2020/data.html,Image Segmentation,MRI,,,,,,,,
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition ,"Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (e.g., dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A domain mixup strategy is additionally devised to reduce the co-occurring artifacts in each domain, which allows for more flexible decision margins and mitigates the issue of incorrectly assigned domain labels. Experiments on four out-of-distribution datasets and six different bias ISIC datasets demonstrate the superior generalization ability of EPVT in skin lesion recognition across various environments.",https://github.com/SiyuanYan1/EPVT,https://github.com/alceubissoto/artifact-generalization-skin,Dermatology,Computer Aided Diagnosis,Model Generalizability / Federated Learning,,,,,,,
Estimated time to surgical procedure completion: An exploration of video analysis methods ,"An accurate estimation of a surgical procedure’s time to completion (ETC) is a valuable capability that has significant impact on operating room efficiency, and yet remains challenging to predict due to significant variability in procedure duration. This paper studies the ETC task in depth; rather than focusing on introducing a novel method or a new application, it provides a methodical exploration of key aspects relevant to training machine learning models to automatically and accurately predict ETC. We study four major elements related to training an ETC model: evaluation metrics, data, model architectures, and loss functions. The analysis was performed on a large-scale dataset of approximately 4,000 surgical videos including three surgical procedures: Cholecystectomy, Appendectomy, and Robotic-Assisted Radical Prostatectomy (RARP). This is the first demonstration of ETC performance using video datasets for Appendectomy and RARP. Even though AI-based applications are ubiquitous in many domains of our lives, some industries are still lagging behind. Specifically, today, ETC is still done by a mere average of a surgeon’s past timing data without considering the visual data captured in the surgical video in real time. We hope this work will help bridge the technological gap and provide important information and experience to promote future research in this space. The source code for models and loss functions is available at: https://github.com/theator/etc.",https://github.com/theator/etc,,Surgical Data Science,,,,,,,,,
Estimation of 3T MR images from 1.5T images regularized with Physics based Constraint ,"Limited accessibility to high field MRI scanners (7T,
11T) has motivated the development of post-processing methods to im-
prove low field images. Existing post-processing methods have
shown the feasibility to improve 3T images to produce 7T-like images [3]. It has been observed that improving lower field (LF, ≤ 1.5T) images
comes with additional challenges due to poor image quality such as the
function mapping 1.5T and higher field (HF, 3T) images is more com-
plex than the function relating 3T and 7T images. Except for [10], no
method has been addressed to improve ≤1.5T MRI images. Further, most
of the existing methods, including [10] require example images, and
also often rely on pixel to pixel correspondences between LF and HF im-
ages which are usually inaccurate for ≤1.5T images. The focus of this
paper is to address the unsupervised framework for quality improvement
of 1.5T images and avoid the expensive requirements of example images
and associated image registration. The LF and HF images are assumed
to be related by a linear transformation (LT).The unknown HF image
and unknown LT are estimated in alternate minimization framework.
Further, a physics based constraint is proposed that provides an addi-
tional non-linear function relating LF and HF images in order to achieve
the desired high contrast in estimated HF image. This constraint exploits
the fact that the T1 relaxation time of tissues increases with increase in
field strength, and if it is incorporated in the LF acquisition the HF con-
trast can be simulated. The experimental results demonstrate that the
proposed approach provides processed 1.5T images with improved image quality, and is comparably better than
the existing methods addressing similar problems. The improvement in
image quality is also shown to provide better tissue segmentation and
volume quantification as compared to scanner acquired 1.5T images. Its application on improving 0.25T images has proved its advantages.",https://drive.google.com/drive/folders/1WbzkBJS1BWAje8aF0ty2SWYTQ9i0B7Yr?usp=sharing,,Image Reconstruction,,,,,,,,,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images ,"Near Out-of-Distribution (OOD) detection is a crucial issue in medical applications, as misdiagnosis caused by the presence of rare diseases inevitablely poses a significant risk. Recently, several deep learning-based methods for OOD detection with uncertainty estimation, such as the Evidential Deep Learning (EDL) and its variants, have shown remarkable performance in identifying outliers that significantly differ from training samples. Nevertheless, few studies focus on the great challenge of near OOD detection problem, which involves detecting outliers that are close to the training distribution, as commonly encountered in medical image application. To address this limitation and reduce the risk of misdiagnosis, we propose an Evidence Reconciled Neural Network (ERNN). Concretely, we reform the evidence representation obtained from the evidential head with the proposed Evidential Reconcile Block (ERB), which restricts the decision boundary of the model and further improves the performance in near OOD detection. Compared with the state-of-the-art uncertainty-based methods for OOD detection, our method reduces the evidential error and enhances the capability of near OOD detection in medical applications. The experiments on both the ISIC2019 dataset and an in-house pancreas tumor dataset validate the robustness and effectiveness of our approach. Code for ERNN has been released at https://github.com/KellaDoe/ERNN.",https://github.com/KellaDoe/ERNN,https://challenge.isic-archive.com/data/#2019,Uncertainty,Computer Aided Diagnosis,,,,,,,,
Evolutionary normalization optimization boosts semantic segmentation network performance ,"Semantic segmentation is an important task in medical imaging. Typically, encoder-decoder architectures, such as the U-Net, are used in various variants to approach this task. Normalization methods, such as Batch or Instance Normalization are used throughout the architectures to adapt to data-specific noise. However, it is barely investigated which normalization method is most suitable for a given dataset and if a combination of those is beneficial for the overall performance. In this work, we show that by using evolutionary algorithms we can fully automatically select the best set of normalization methods, outperforming any competitive single normalization method baseline. We provide insights into the selection of normalization and how this compares across imaging modalities and datasets. Overall, we propose that normalization should be managed carefully during the development of the most recent semantic segmentation models as it has a significant impact on medical image analysis tasks, contributing to a more efficient analysis of medical data. Our code is openly available after peer review.",https://github.com/neuluna/ga-unet,https://www.bagls.org/,Image Segmentation,Other,CT,MRI,other,,,,,
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation ,"The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For the surgeon to trust the model, explainability of the generated predictions needs to be provided. For image classification models, pixel attribution (PA) and risk estimation are popular methods to infer explainability. However, the former method lacks trustworthiness while the latter can not provide visual explanation of the model’s attention. In this paper, we propose the first approach which incorporates risk estimation into a PA method for improved and more trustworthy image classification explainability. The proposed method iteratively applies a classification model with a PA method to create a volume of PA maps. We introduce a
method to generate an enhanced PA map by estimating the expectation values of
the pixel-wise distributions. In addition, the coefficient of variation (CV) is used
to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data verifies that our improved explainability method outperforms the state-of-the-art.",https://github.com/alfieroddan/Explainable-Image-Classification,,Interpretability / Explainability,Computer Aided Diagnosis,Guided Interventions and Surgery,Uncertainty,Microscopy,Visualization in Biomedical Imaging,,,,
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task through Visualizing Functions within the Models ,"In this study, we proposed a novel explainable artificial intelligence (XAI) technique to explain massive-training artificial neural networks (MTANNs). Firstly, we optimized the structure of an MTANN to find a compact model that performs equivalently well to the original one. This en-ables to “condense” functions in a smaller number of hidden units in the network by removing “redundant” units. Then, we applied an unsupervised hierarchical clustering algorithm to the function maps in the hidden layers with the single-linkage method. From the clustering and visualization re-sults, we were able to group the hidden units into those with similar func-tions together and reveal the behaviors and functions of the trained MTANN models. We applied this XAI technique to explain the MTANN model trained to segment liver tumors in CT. The original MTANN model with 80 hidden units (F1=0.6894, Dice=0.7142) was optimized to the one with nine hidden units (F1=0.6918, Dice=0.7005) with almost equivalent performance. The nine hidden units were clustered into three groups, and we found the following three functions: 1) enhancing liver area, 2) suppressing non-tumor area, and 3) suppressing the liver boundary and false enhance-ment. The results shed light on the “black-box” problem with deep learning (DL) models; and we demonstrated that our proposed XAI technique was able to make MTANN models “transparent”.",,,Interpretability / Explainability,Computer Aided Diagnosis,Image Segmentation,CT,,,,,,
Exploring Brain Function-Structure Connectome Skeleton via Self-Supervised Graph-Transformer Approach ,"Understanding the relationship between brain functional connectivity and structural connectivity is important in the field of brain imaging, and it can help us better comprehend the working mechanisms of the brain. Much effort has been made on this issue, but it is still far from satisfactory. The brain transmits information through a network architecture, which means that the regions and connections of the brain are significant. The main difficulties with this issue are currently at least two aspects. On the one hand, the importance of different brain regions in structural and functional integration has not been fully addressed; on the other hand, the connectome skeleton of the brain,  plays the role of common and key connections in the brain network, has not been clearly studied. To alleviate the above problems, this paper proposes a transformer-based self-supervised graph reconstruction framework (TSGR). The framework uses the graph neural net-work (GNN) to fuse functional and structural information of the brain, and uses the self-supervised model to identify contribution scores of regions for the reconstruction task. Regions with high scores are considered as key connectome regions which play an essential role in the communication connectivity of the net-work in the brain. Based on key brain regions, the connectome skeleton can be obtained. Experimental results demonstrate the effectiveness of the proposed method, which obtains key regions and connectome skeleton in the brain net-work. This provides a new angle of view to explore the relationship between brain function and structure.",https://github.com/kang105/TSGR,,MRI,Neuroimaging - Brain Development,Neuroimaging - DWI and Tractography,Neuroimaging - Functional Brain Networks,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps ,"The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have
managed to reduce the dependency on labels. However, considering the
large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for
cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation
network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior
self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo
masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell
detection). Compared with other fully-supervised and weakly-supervised
methods, our method can achieve competitive performance without any
manual annotations. Our simple but effective framework can also achieve
multi-class cell detection which can not be done by existing unsupervised
methods. The results show the potential of PSMs that might inspire other
research to deal with the hunger for labels in medical area.",https://github.com/cpystan/PSM,,Microscopy,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen ,"Multi-organ segmentation of the abdominal region plays a vital role in clinical such as organ quantification, surgical planning, and disease diagnosis. Due to the dense distribution of abdominal organs and the close connection between each organ, the accuracy of the label is highly required. However, the dense and complex structure of abdominal organs necessitates highly professional medical expertise to manually annotate the organs, leading to significant costs in terms of time and effort. We found a cheap and easily accessible form of supervised information. Recording the areas by the eye tracker where the radiologist focuses while reading abdominal images, gaze information is able to force the network model to focus on relevant objects or features required for the segmentation task. Therefore how to effectively integrate image information with gaze information is a problem to be solved. To address this issue, we propose a novel network for abdominal multi-organ segmentation, which incorporates radiologists’ gaze information to boost high-precision segmentation and weaken the demand for high-cost manual labels. Our network includes three special designs: 1) a dual-path encoder to further integrate gaze information; 2) a cross-attention transformer module (CATM) that embeds human cognitive information about the image into the network model; and 3) multi-feature skip connection (MSC), which combines spatial information during down-sampling to offset the internal details of segmentation. Additionally, our network utilizes discrete wavelet transform (DWT) to further provide information on organ location and edge in different directions. Extensive experiments performed on the publicly available Synapse dataset demonstrate that our proposed method can integrate effectively gaze information and achieves Dice similarity coefficient (DSC) up to 81.87% and Hausdorff distance(HD) reduction to 11.96%, as well as gain high-quality readable visualizations.",https://github.com/code-Porunacabeza/gaze_seg/,,Abdomen,Computer Aided Diagnosis,,,,,,,,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation ,"Medical Hyperspectral Imaging (MHSI) brings opportunities for computational pathology and precision medicine. Since MHSI is a 3D hypercube, building a 3D segmentation network is the most intuitive way for MHSI segmentation. But, high spatiospectral dimensions make it difficult to perform efficient and effective segmentation. In this study, in light of information correlation in MHSIs, we present a computationally efficient, plug-and-play space and spectrum factorization strategy based on 2D architectures. Drawing inspiration from the low-rank prior of MHSIs, we propose spectral matrix decomposition and low-rank decomposition modules for removing redundant spatiospectral information. By plugging our dual-stream strategy into 2D backbones, we can achieve state-of-the-art MHSI segmentation performances with 3~13 times faster compared with existing 3D networks in terms of inference speed. Experiments show our strategy leads to remarkable performance gains in different 2D architectures, reporting an improvement up to 7.7% compared with its 2D counterpart in terms of DSC on a public Multi-Dimensional Choledoch dataset. Code will be released.",https://github.com/boxiangyun/Dual-Stream-MHSI,https://www.kaggle.com/datasets/ethelzq/multidimensional-choledoch-database,Image Segmentation,Computational (Integrative) Pathology,Biophotonics,Histopathology,other,,,,,
FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification ,"Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called \textbf{model unfairness}, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attributes. This simple but effective design can be adapted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performance and fairness, we propose a new metric, named Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness improvement over accuracy drop. Experiments on two dermatological datasets show that our proposed method outperforms other methods on fairness criteria and FATE. Our code is available at https://github.com/XuZikang/FairAdaBN.",https://github.com/XuZikang/FairAdaBN,,Other,Dermatology,,,,,,,,
Faithful Synthesis of Low-dose Contrast-enhanced Brain MRI Scans using Noise-preserving Conditional GANs ,"Today Gadolinium-based contrast agents (GBCA) are indispensable in magnetic resonance imaging (MRI) for diagnosing various diseases.
However, GBCAs are expensive and may accumulate in patients with potential side effects, thus dose-reduction is recommended.
Still, it is unclear to which extent the GBCA dose can be reduced while preserving the diagnostic value – especially in pathological regions. 
To address this issue, we collected brain MRI scans at numerous non-standard GBCA dosages and developed a conditional GAN model for synthesizing corresponding images at fractional dose levels.
Along with the adversarial loss, we advocate a novel content loss function based on the Wasserstein distance of locally paired patch statistics for the faithful preservation of noise.
Our numerical experiments show that conditional GANs are suitable for generating images at different GBCA dose levels and can be used to augment datasets for dose reduction models.
Moreover, our model can be transferred to openly available datasets such as BraTS, where non-standard GBCA dosage images do not exist.",https://github.com/tpinetz/low-dose-gadolinium-mri-synthesis,https://www.med.upenn.edu/cbica/brats2020/data.html,Data Efficient Learning,Neuroimaging - Others,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images ,"In medical image analysis, anomaly detection in weakly supervised settings has gained significant interest due to the high cost associated with expert-annotated pixel-wise labeling. Current methods primarily rely on auto-encoders and flow-based healthy image reconstruction to detect anomalies. However, these methods have limitations in terms of high-fidelity generation and suffer from complicated training processes and low-quality reconstructions. Recent studies have shown promising results with diffusion models in image generation. However, their practical value in medical scenarios is restricted due to their weak detail-retaining ability and low inference speed. To address these limitations, we propose a fast non-Markovian diffusion model (FNDM) with hybrid-condition guidance to detect high-precision anomalies in the brain MR images. A non-Markovian diffusion process is designed to enable the efficient transfer of anatomical information across diffusion steps. Additionally, we introduce new hybrid pixel-wise conditions as more substantial guidance on hidden states, which enables the model to concentrate more efficiently on the anomaly regions. Furthermore, to reduce computational burden during clinical applications, we have accelerated the encoding and sampling procedures in our FNDM using multi-step ODE solvers. As a result, our proposed FNDM method outperforms the previous state-of-the-art diffusion model, achieving a 9.56\% and 19.98\% improvement in Dice scores on the BRATS 2020 and ISLES datasets, respectively, while requiring only six times less computational cost.",,https://www.kaggle.com/datasets/awsaf49/brats2020-training-data,MRI,Computer Aided Diagnosis,,,,,,,,
Fast Reconstruction for Deep Learning PET Head Motion Correction ,"Head motion correction is an essential component of brain PET imaging, in which even motion of small magnitude can greatly degrade image quality and introduce artifacts. Building upon previous work, we propose a new head motion correction framework taking fast reconstructions as input. The main characteristics of the proposed method are: (i) the adoption of a high-resolution short-frame fast reconstruction workflow; (ii) the development of a novel encoder for PET data representation extraction; and (iii) the implementation of data augmentation techniques. Ablation studies are conducted to assess the individual contributions of each of these design choices. Furthermore, multi-subject studies are conducted on an 18F-FPEB dataset, and the method performance is qualitatively and quantitatively evaluated by MOLAR reconstruction study and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation. Additionally, we also compared our method with a conventional intensity-based registration method. Our results demonstrate that the proposed method outperforms other methods on all subjects, and can accurately estimate motion for subjects out
of the training set. All code is publicly available on GitHub: https://github.com/OnofreyLab/dl-hmc_fast_recon_miccai2023.",https://github.com/OnofreyLab/dl-hmc_fast_recon_miccai2023,,PET/SPECT,Image Reconstruction,Image Registration,,,,,,,
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis ,"Image synthesis is expected to provide value for the translation of machine learning methods into clinical practice. Fundamental problems like model robustness, domain transfer, causal modelling, and operator training become approachable through synthetic data. Especially, heavily operator-dependant modalities like Ultrasound imaging require robust frameworks for image and video generation. So far, video generation has only been possible by providing input data that is as rich as the output data, e.g., image sequence plus conditioning in, video out. However, clinical documentation is usually scarce and only single images are reported and stored, thus retrospective patient-specific analysis or the generation of rich training data becomes impossible with current approaches. 
In this paper, we extend elucidated diffusion models for video modelling to generate plausible video sequences from single images and arbitrary conditioning with clinical parameters. 
We explore this idea within the context of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We use the publicly available EchoNet-Dynamic dataset for all our experiments.
Our image to sequence approach achieves an R2 score of 93%, which is 38 points higher than recently proposed sequence to sequence generation methods. Code and weights are available at https://github.com/HReynaud/EchoDiffusion.",https://github.com/HReynaud/EchoDiffusion,https://echonet.github.io/dynamic/,Image Reconstruction,Cardiac,Ultrasound,Video,,,,,,
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-aware Aggregation ,"Federated learning is a promising strategy for performing privacy-preserving, distributed learning for medical image segmentation. However, the data-level heterogeneity as well as system-level heterogeneity makes it challenging to optimize. In this paper, we propose to improve Federated optimization via local Contrastive learning and Global Process-aware Aggregation (referred as FedContrast-GPA), aiming to jointly address both data-level and system-level heterogeneity issues. In specific, To address data-level heterogeneity, we 
propose to learn a unified latent feature space via an intra-client and inter-client local prototype based contrastive learning scheme. Among which, intra-client contrastive learning is adopted to improve the discriminative ability of learned feature embedding at each client, while inter-client contrastive learning is introduced to achieve cross-client distribution perception and alignment in a privacy preserving manner. To address system-level heterogeneity, we further propose a simple yet effective process-aware aggregation scheme to achieve effective straggler mitigation. Experimental results on six prostate segmentation datasets demonstrate large performance boost  over existing state-of-the-art methods.",,,Model Generalizability / Federated Learning,Abdomen,Image Segmentation,MRI,,,,,,
"FEDD - Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification ","Skin diseases affect millions of people worldwide, across all ethnicities. Increasing diagnosis accessibility requires fair and accurate segmentation and classification of dermatology images. However, the scarcity of annotated medical images, especially for rare diseases and underrepresented skin tones, poses a challenge to the development of fair and accurate models. In this study, we introduce a Fair, Efficient, and Diverse Diffusion-based framework for skin lesion segmentation and malignancy classification. FEDD leverages semantically meaningful feature embeddings learned through a denoising diffusion probabilistic backbone and processes them via linear probes to achieve state-of-the-art performance on Diverse Dermatology Images (DDI). We achieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07 while using only 5%, 10%, 15%, and 20% labeled samples, respectively. Additionally, FEDD trained on 10% of DDI demonstrates malignancy classification accuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high efficiency in data-constrained scenarios while providing fair performance for diverse skin tones and rare malignancy conditions. Our newly annotated DDI segmentation masks and training code can be found on https://github.com/hectorcarrion/fedd.",https://github.com/hectorcarrion/fedd,https://ddi-dataset.github.io/,Dermatology,Computer Aided Diagnosis,Image Segmentation,Data Efficient Learning,Treatment Response and Outcome/Disease Prediction,,,,,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning ,"The harmful radiation dose associated with CT imaging is a major concern because it can cause genetic diseases.  Acquiring CT data at low radiation doses has become a pressing goal. Deep learning (DL)-based methods have proven to suppress noise-induced artifacts and promote image quality in low-dose CT imaging. However, it should be noted that most of the DL-based methods are constructed based on the CT data from a specific condition, i.e., specific imaging geometry and specific dose level. Then these methods might generalize poorly to the other conditions, i.e., different imaging geometries and other radiation doses, due to the big data heterogeneity. In this study, to address this issue, we propose a condition generalization method under a federated learning framework (FedCG) to reconstruct CT images on two conditions: three different dose levels and different samplings at three different geometries. Specifically, the proposed FedCG method leverages a cross-domain learning approach: individual-site sinogram learning and cross-site image reconstruction for condition generalization. In each individual site, the sinogram at each condition is processed similarly to that in the iRadonMAP. Then the CT images at each site are learned via a condition generalization network in the server which considers latent common characteristics in the CT images at all conditions and preserves the site-specific characteristics in each condition. Experiments show that the proposed FedCG outperforms the other competing methods on two imaging conditions in terms of qualitative and quantitative assessments.",,,CT,Attention models,Model Generalizability / Federated Learning,,,,,,,
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging ,"Deep learning models have shown promising performance in the field of diabetic retinopathy (DR) staging. However, collaboratively training a DR staging model across multiple institutions remains a challenge due to non-iid data, client reliability, and confidence evaluation of the prediction. To address these issues, we propose a novel federated uncertainty-aware aggregation paradigm (FedUAA), which considers the reliability of each client and produces a confidence estimation for the DR staging. In our FedUAA, an aggregated encoder is shared by all clients for learning a global representation of fundus images, while a novel temperature-warmed uncertainty head (TWEU) is utilized for each client for local personalized staging criteria. Our TWEU employs an evidential deep layer to produce the uncertainty score with the DR staging results for client reliability evaluation. Furthermore, we developed a novel uncertainty-aware weighting module (UAW) to dynamically adjust the weights of model aggregation based on the uncertainty score distribution of each client. In our experiments, we collect five publicly available datasets from different institutions to conduct a dataset for federated DR staging to satisfy the real non-iid condition. The experimental results demonstrate that our FedUAA achieves better DR staging performance with higher reliability compared to other federated learning methods. Our proposed FedUAA paradigm effectively addresses the challenges of collaboratively training DR staging models across multiple institutions, and provides a robust and reliable solution for the deployment of DR diagnosis models in real-world clinical scenarios.",,,Model Generalizability / Federated Learning,Ophthalmology,Uncertainty,,,,,,,
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation ,"With the increasingly strengthened data privacy acts and the difficult data centralization, Federated Learning (FL) has become an effective solution to collaboratively train the model while preserving each client’s privacy. FedAvg is a standard aggregation algorithm that makes the proportion of the dataset size of each client an aggregation weight. However, it can’t deal with non-independent and identically distributed (non-IID) data well because of its fixed aggregation weights and the neglect of data distribution. The paper presents a new aggregation strategy called FedGrav, which is designed to handle non-IID datasets and is inspired by the law of universal gravitation in physics. FedGrav can dynamically adjust the aggregation weights based on the training condition of local models throughout the entire training process, making it an effective solution for non-IID data. The model affinity is creatively proposed by considering both the differences of sample size on the client and the discrepancies among local models. It considers the client sample size as the mass of the local model and defines the model graph distance based on neural network topology. By calculating the affinity among local models, FedGrav can explore internal correlations of them and improve the aggregation weights. The proposed FedGrav has been applied to the CIFAR-10 and the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2021 datasets, and the validation results show that our method outperforms the previous state-of-the-art by 1.54 mean DSC and 2.89 mean HD95. The source code will be available on Github.",,,Model Generalizability / Federated Learning,Image Segmentation,MRI,,,,,,,
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification ,"Federated learning (FL), training deep models from decentralized data without privacy leakage, has shown great potential in medical image computing recently. However, considering the ubiquitous class imbalance in medical data, FL can exhibit performance degradation, especially for minority classes (e.g. rare diseases). Existing methods towards this problem mainly focus on training a  balanced classifier to eliminate class prior bias among classes, but neglect to explore better representation to facilitate classification performance. In this paper, we present a privacy-preserving FL method named FedIIC to combat class imbalance from two perspectives: feature learning and classifier learning. In feature learning, two levels of contrastive learning are designed to extract better class-specific features with imbalanced data in FL. In classifier learning, per-class margins are dynamically set according to real-time difficulty and class priors, which helps the model learn classes equally. Experimental results on publicly-available datasets demonstrate the superior performance of FedIIC in dealing with both real-world and simulated multi-source medical imaging data under class imbalance. Code is available at https://github.com/wnn2000/FedIIC.",https://github.com/wnn2000/FedIIC,https://www.fc.up.pt/addi/ph2%20database.html,Model Generalizability / Federated Learning,Computer Aided Diagnosis,CT,,,,,,,
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation ,"Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data.  In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model’s generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization.",https://github.com/ubc-tea/FedSoup,,Model Generalizability / Federated Learning,,,,,,,,,
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis ,"Brain connectivity patterns such as functional connectivity (FC) and effective connectivity (EC), describing complex spatio-temporal dynamic interactions in the brain network, are highly desirable for mild cognitive impairment (MCI) diagnosis. Major FC methods are based on statistical dependence, usually evaluated in terms of correlations, while EC generally focuses on directional causal influences between brain regions. Therefore, comprehensive integration of FC and EC with complementary information can further extract essential biomarkers for characterizing brain abnormality. This paper proposes Spatio-Temporal Graph Neural Network with Dynamic Functional and Effective Connectivity Fusion (FE-STGNN) for MCI diagnosis using resting-state fMRI (rs-fMRI). First, dynamic FC and EC networks are constructed to encode the functional brain networks into multiple graphs. Then, spatial graph convolution is employed to process spatial structural features and temporal dynamic characteristics. Finally, we design the position encoding-based cross-attention mechanism, which utilizes the causal linkage of EC during time evolution to guide the fusion of FC networks for MCI classification. Qualitative and quantitative experimental results demonstrate the significance of the proposed FE-STGNN method and the benefit of fusing FC and EC, which achieves $82\%$ of MCI classification accuracy and outperforms state-of-the-art methods.",https://github.com/haijunkenan/FE-STGNN,https://adni.loni.usc.edu/,Neuroimaging - Functional Brain Networks,Attention models,Other,,,,,,,
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling ,"Data scarcity is a significant obstacle hindering the learning of powerful machine learning models in critical healthcare applications. Data-sharing mechanisms among multiple entities (e.g., hospitals) can accelerate model training and yield more accurate predictions. Recently, approaches such as Federated Learning (FL) and Split Learning (SL) have facilitated collaboration without the need to exchange private data. In this work, we propose a framework for medical imaging classification tasks called Federated Split learning of Vision transformer with Block
Sampling (FeSViBS). The FeSViBS framework builds upon the existing federated split vision transformer and introduces a block sampling module, which leverages intermediate features extracted by the Vision Transformer (ViT) at the server. This is achieved by sampling features (patch tokens) from an intermediate transformer block and distilling their information content into a pseudo class token before passing them back to the client. These pseudo class tokens serve as an effective feature augmentation strategy and enhances the generalizability of the learned
model. We demonstrate the utility of our proposed method compared to other SL and FL approaches on three publicly available medical imaging datasets: HAM1000, BloodMNIST, and Fed-ISIC2019, under both IID and non-IID settings. Code: https://github.com/faresmalik/FeSViBS",https://github.com/faresmalik/FeSViBS,,Model Generalizability / Federated Learning,Histopathology,other,,,,,,,
Few Shot Medical Image Segmentation with Cross Attention Transformer ,"Medical image segmentation has made significant progress in recent years. Deep learning-based methods are recognized as data-hungry techniques, requiring large amounts of data with manual annotations. However, manual annotation is expensive in the field of medical image analysis, which requires domain-specific expertise. To address this challenge, few-shot learning has the potential to learn new classes from only a few examples. In this work, we propose a novel framework for few-shot medical image segmentation, termed CAT-Net, based on cross masked attention Transformer. Our proposed network mines the correlations between the support image and query image, limiting them to focus only on useful foreground information and boosting the representation capacity of both the support prototype and query features. We further design an iterative refinement framework that refines the query image segmentation iteratively and promotes the support feature in turn. We validated the proposed method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental results demonstrate the superior performance of our method compared to state-of-the-art methods and the effectiveness of each component. Upon publication of this paper, we will release the source code for our method.",git@github.com:hust-linyi/CAT-Net.git,,Data Efficient Learning,Abdomen,,,,,,,,
Few-Shot Medical Image Segmentation via a Region-enhanced Prototypical Transformer ,"Automated segmentation of large volumes of medical images is often plagued by the limited availability of fully annotated data and the diversity of organ surface properties resulting from the use of different acquisition protocols for different patients. In this paper, we introduce a more promising few-shot learning-based method named Region-enhanced Prototypical Transformer (RPT) to mitigate the effects of large intra-class diversity/bias. First, a subdivision strategy is introduced to produce a collection of regional prototypes from the foreground of the support prototype. Second, a self-selection mechanism is proposed to incorporate into the Bias-alleviated Transformer (BaT) block to suppress or remove interferences present in the query prototype and regional support prototypes. By stacking BaT blocks, the proposed RPT can iteratively optimize the generated regional prototypes and finally produce rectified and more accurate global prototypes for Few-Shot Medical Image Segmentation (FSMS). Extensive experiments are conducted on three publicly available medical image datasets, and the obtained results show consistent improvements compared to state-of-the-art FSMS methods.",https://github.com/YazhouZhu19/RPT,https://chaos.grand-challenge.org/,Image Segmentation,CT,MRI,,,,,,,
Fine-grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-constraint Loss ,"Ultrasound imaging is a promising tool for clinical hand examination due to its radiation-free and cost-effective nature. To mitigate the impact of ultrasonic imaging defects on accurate clinical diagnosis, automatic fine-grained hand bone segmentation is highly desired. However, existing ultrasound image segmentation methods face difficulties in performing this task due to the presence of numerous categories and insignificant inter-class differences. To address these challenges, we propose a novel Adaptive Multi-dimensional Convolutional Network (AMCNet) for fine-grained hand bone segmentation. It is capable of dynamically adjusting the weights of 2D and 3D convolutional features at different levels via an adaptive multi-dimensional feature fusion mechanism. We also design an anatomy-constraint loss to encourage the model to learn anatomical relationships and effectively mine hard samples. Experiments demonstrate that our method outperforms other comparison methods and effectively addresses the task of fine-grained hand bone segmentation in ultrasound volume. We have developed a user-friendly and extensible module on the 3D Slicer platform based on the proposed method and will release it globally to promote greater value in clinical applications. The source code is available at https://github.com/BL-Zeng/AMCNet.",https://github.com/BL-Zeng/AMCNet,,Image Segmentation,Musculoskeletal,Ultrasound,,,,,,,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis ,"Federated learning (FL) has emerged as a promising technique in the field of medical diagnosis. 
By distributing the same task through deep networks on mobile devices, FL has proven effective in diagnosing dermatitis, a common and easily recognizable skin disease. 
However, in skin disease diagnosis, FL presents challenges related to (1) generalization rather than personalization and (2) limited utilization of mobile devices. 
Despite its improved comprehensive diagnostic performance, skin disease diagnosis should aim for personalized diagnosis rather than centralized and generalized diagnosis, due to personal diversities and variability, such as skin color, wrinkles, and aging. 
To this end, we propose a novel deep learning network for personalized diagnosis in an adaptive manner, utilizing personal characteristics in diagnosing dermatitis in a mobile- and FL-based environment. 
Our framework, dubbed as APD-Net, achieves adaptive and personalized diagnosis using a new model design and a genetic algorithm (GA)-based fine-tuning method. 
APD-Net incorporates a novel architectural design that leverages personalized and centralized parameters, along with a fine-tuning method based on a modified GA to identify personal characteristics. We validate APD-Net on clinical datasets and demonstrate its superior performance compared to state-of-the-art approaches. 
Experimental results demonstrate that APD-Net improved personalized diagnostic accuracy by 9.9\% in dermatitis diagnosis, making it a promising tool for clinical practice.",,,Model Generalizability / Federated Learning,Dermatology,Computer Aided Diagnosis,,,,,,,
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps ,"Comprehensive, contiguous visualizations of the main cerebral arteries and the surrounding parenchyma offer considerable potential for improving diagnostic workflows in cerebrovascular disease, e.g., for fast assessment of vascular topology and lumen in stroke patients.
Unfolding the brain vasculature into a 2D overview is, however, infeasible using common Curved Planar Reformation (CPR) due to the circular structure of the Circle of Willis (CoW) and the spatial configuration of the vessels typically rendering them unsuitable for mapping onto simple geometric primitives. 
We propose CeVasMap, extending the As-Rigid-As-Possible (ARAP) deformation by a smart initialization of the required mesh to map the CoW as well as a merging of neighboring vessels depending on the resulting degree of distortion.
Otherwise, vessels are unfolded and attached individually, creating a textbook-style overview image. 
We provide an extensive distortion analysis, comparing the vector fields of individual and merged unfoldings of each vessel to their CPR results.
In addition to enabling unfolding of circular structures, our method is 
on par in terms of incurred distortions to optimally oriented CPRs for individual vessels and comparable to unfavorable CPR orientations when merging the complete CoW with a median distortion of 65 µm/mm.",,,Visualization in Biomedical Imaging,Neuroimaging - Others,Vascular,CT,,,,,,
FLIm-based In Vivo Classification of Residual Cancer in the Surgical Cavity during Transoral Robotic Surgery ,"Incomplete surgical resection with residual cancer left in the surgical cavity is a potential sequelae of Transoral Robotic Surgery (TORS). To minimize such risk, surgeons rely on intraoperative frozen section analysis (IFSA) to locate and remove the remaining tumor. This process may lead to false negatives and is time-consuming. Mesoscopic fluorescence lifetime imaging (FLIm) of tissue fluorophores (i.e., collagen and metabolic co-factors NADH and FAD) emission has demonstrated the potential to demarcate the extent of head and neck cancer in patients undergoing surgical procedures of the oral cavity and the oropharynx. Here, we demonstrate the first label-free FLIm-based classification using a novel-ty detection model to identify residual cancer in the surgical cavity of the oro-pharynx. Due to highly imbalanced label representation in the surgical cavity, the model employed solely FLIm data from healthy surgical cavity tissue for training and classified the residual tumors as an anomaly. FLIm data from N=22 patients undergoing upper aerodigestive oncologic surgery were used to train and validate the classification model using leave-one-patient-out cross-validation. Our approach identified all patients with positive surgical margins (N=3) confirmed by pathology. Furthermore, the proposed method reported a point-level sensitivity of 0.75 and a specificity of 0.78 across optically interrogated tissue surface for all N=22 patients. The results indicate that the FLIm-based classification model can identify residual cancer by directly imaging the surgical cavity, potentially enabling intraoperative surgical guidance for TORS.",,,Guided Interventions and Surgery,Oncology,Other,Biophotonics,Surgical Visualization and Mixed/Augmented/Virtual Reality,Visualization in Biomedical Imaging,,,,
Flow-based Geometric Interpolation of Fiber Orientation Distribution Functions ,"The fiber orientation distribution function (FOD) is an advanced
model for high angular resolution diffusion MRI representing complex fiber
geometry. However, the complicated mathematical structures of the FOD function
pose challenges for FOD image processing tasks such as interpolation,
which plays a critical role in the propagation of fiber tracts in tractography. In
FOD-based tractography, linear interpolation is commonly used for numerical
efficiency, but it is prone to generate false artificial information, leading to anatomically incorrect fiber tracts. To overcome this difficulty, we propose a flowbased and geometrically consistent interpolation framework that considers
peak-wise rotations of FODs within the neighborhood of each location. Our
method decomposes a FOD function into multiple components and uses a
smooth vector field to model the flows of each peak in its neighborhood. To
generate the interpolated result along the flow of each vector field, we develop
a closed-form and efficient method to rotate FOD peaks in neighboring voxels
and realize geometrically consistent interpolation of FOD components. By
combining the interpolation results from each peak, we obtain the final interpolation of FODs. Experimental results on Human Connectome Project (HCP) data demonstrate that our method produces anatomically more meaningful FOD
interpolations and significantly enhances tractography performance.",,,Neuroimaging - DWI and Tractography,Computational Anatomy and Physiology,,,,,,,,
FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery ,"In brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment. However, intra-operative tissue deformation (called brain shift) can move the surgical target and render the pre-surgical plan invalid. Intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan. Quality control for the registration results during surgery is important to avoid adverse outcomes, but manual verification faces great challenges due to difficult 3D visualization and the low contrast of iUS. Automatic algorithms are urgently needed to address this issue, but the problem was rarely attempted. Therefore, we propose a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery. Developed and validated with the public RESECT clinical database, the resulting algorithm can achieve an estimation error of 0.59±0.57 mm.",,,Guided Interventions and Surgery,Image Registration,MRI,Ultrasound,,,,,,
FocalUNETR: A Focal Transformer for Boundary-aware Prostate Segmentation using CT Images ,"Computed Tomography (CT) based precise prostate segmentation for treatment planning is challenging due to (1) the unclear boundary of the prostate derived from CT’s poor soft tissue contrast and (2) the limitation of convolutional neural network-based models in capturing long-range global context. Here we propose a novel focal transformer-based image segmentation architecture to effectively and efficiently extract local visual features and global context from CT images. Additionally, we design an auxiliary boundary-induced label regression task coupled with the main prostate segmentation task to address the issue of unclear boundaries from poor soft tissue contrast CT images. We demonstrate that these designs can significantly improve the quality of the CT-based prostate segmentation task over other competing methods, resulting in substantially improved Dice Similarity Coefficient and reduced Hausdorff Distance and Average Symmetric Surface Distance on both private and public CT image datasets.",,,Image Segmentation,CT,,,,,,,,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-Supervised Contrastive Learning ,"Forensic pathology is critical in analyzing death manner and time from the microscopic aspect to assist in the establishment of reliable factual bases for criminal investigation. In practice, even the manual differentiation between different postmortem organ tissues is challenging and relies on expertise, considering that changes like putrefaction and autolysis could significantly change typical histopathological appearance. Developing AI-based computational pathology techniques to assist forensic pathologists is practically meaningful, which requires reliable discriminative representation learning to capture tissues’ fine-grained postmortem patterns. To this end, we propose a framework called FPath, in which a dedicated self-supervised contrastive learning strategy and a context-aware multiple-instance learning (MIL) block are designed to learn discriminative representations from postmortem histopathological images acquired at varying magnification scales. Our self-supervised learning step leverages multiple complementary contrastive losses and regularization terms to train a double-tier backbone for fine-grained and informative patch/instance embedding. Thereafter, the context-aware MIL adaptively distills from the local instances a holistic bag/image-level representation for the recognition task. On a large-scale database of 19, 607 experimental rat postmortem images and 3, 378 real-world human decedent images, our FPath led to state-of-the-art accuracy and reliable cross-domain generalization in recognizing seven different postmortem tissues.",https://github.com/ladderlab-xjtu/forensic_pathology,,Computational (Integrative) Pathology,Computer Aided Diagnosis,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,Microscopy,,,,
Forward-solution aided deep-learning framework for patient-specific noninvasive cardiac ectopic pacing localization ,"Accurate localization of the ectopic pacing is the key to effective catheter ablation for curing cardiac diseases such as premature ventricular contraction (PVC) and tachycardia. Invasive localization method can achieve high precision but has disadvantages of high risk, high cost and its time-consuming process, therefore a non-invasive and convenient localization method is in demand. Noninvasive methods have been developed to utilize electrophysiological information provided by 12-lead electrocardiogram (ECG), and most of them are purely based on end-to-end data-driven architecture. This architecture generally needs a large and comprehensive labeled dataset, which is very difficult to obtain for whole ventricular ectopic beats in clinical setting. To address this issue, we propose a framework that combines cardiac forward-solution simulation and deep learning network for patient-specific noninvasive ectopic pacing localization. For each patient, it only requires his/her own CT images to establish specific heart-torso model and to simulate various ECG data from different ectopic pacing locations, and uses this simulated ECG data as training dataset for our designed network. The network mainly contains time-frequency fusion module and local-global feature extraction module. Five PVC patient ECG data are tested with high precision and accuracy for ectopic pacing localization, which shows its high-potential in clinical setting.",,,Cardiac,EEG/ECG,,,,,,,,
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance ,"Deep learning nowadays offers expert-level and sometimes even super-expert-level performance, but achieving such performance demands massive annotated data for training (e.g., Google’s proprietary CXR Foundation Model (CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays (CXRs)). Numerous datasets are publicly available in medical imaging but individually small and heterogeneous in expert labels. We envision a powerful and robust foundation model that can be trained by aggregating numerous small public datasets. To realize this vision, we have developed Ark, a framework that accrues and reuses knowledge from heterogeneous expert annotations in various datasets. As a proof of concept, we have trained two Ark models on 335,484 and 704,363 CXRs, respectively, by merging several datasets including ChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range of imaging tasks covering both classification and segmentation via fine-tuning, linear-probing, and gender-bias analysis, and demonstrated our Ark’s superior and robust performance over the state-of-the-art (SOTA) fully/self-supervised baselines and Google’s proprietary CXR-FM. This enhanced performance is attributed to our simple yet powerful observation that aggregating numerous public datasets diversifies patient populations and accrues knowledge from diverse experts, yielding unprecedented performance yet saving annotation cost. With all codes and pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark exerts an important impact on open science, as accruing and reusing knowledge from expert annotations in public datasets can potentially surpass the performance of proprietary models trained on unusually large data, inspiring many more researchers worldwide to share codes and datasets to build open foundation models, accelerate open science, and democratize deep learning for medical imaging.",GitHub.com/JLiangLab/Ark,,Transfer learning,Data Efficient Learning,,,,,,,,
Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train ,"Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downtream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.",https://github.com/med-air/Endo-FM,https://mycuhk-my.sharepoint.com/:f:/g/personal/1155167044_link_cuhk_edu_hk/EmB8iuYtsGdDrpIQSO6AMHEBtaSW-DY-dRfHCmfd96kCTg?e=KWUWsd,Video,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Fourier Test-time Adaptation with Multi-level Consistency for Robust Classification ,"Deep classifiers may encounter significant performance degradation when processing unseen testing data from varying centers, vendors, and protocols. Ensuring the robustness of deep models against these domain shifts is crucial for their widespread clinical application. In this study, we propose a novel approach called Fourier Test-time Adaptation (FTTA), which employs a dual-adaptation design to integrate input and model tuning, thereby jointly improving the model robustness. The main idea of FTTA is to build a reliable multi-level consistency measurement of paired inputs for achieving self-correction of prediction. Our contribution is two-fold. First, we encourage consistency in global features and local attention maps between the two transformed images of the same input. Here, the transformation refers to Fourier-based input adaptation, which can transfer one unseen image into source style to reduce the domain gap. Furthermore, we leverage style-interpolated images to enhance the global and local features with learnable parameters, which can smooth the consistency measurement and accelerate convergence. Second, we introduce a regularization technique that utilizes style interpolation consistency in the frequency domain to encourage self-consistency in the logit space of the model output. This regularization provides strong self-supervised signals for robustness enhancement. FTTA was extensively validated on three large classification datasets with different modalities and organs. Experimental results show that FTTA is general and outperforms other strong state-of-the-art methods.",,,Model Generalizability / Federated Learning,Fetal Imaging,Continual Learning,Reinforcement Learning,Ultrasound,,,,,
FreeSeed: Frequency-band-aware and Self-guided Network for Sparse-view CT Reconstruction ,"Sparse-view computed tomography (CT) is a promising solution for expediting the scanning process and mitigating radiation exposure to patients, the reconstructed images, however, contain severe streak artifacts, compromising subsequent screening and diagnosis. Recently, deep learning-based image post-processing methods along with their dual-domain counterparts have shown promising results. However, existing methods usually produce over-smoothed images with loss of details due to i) the difficulty in accurately modeling the artifact patterns in the image domain, and ii) the equal treatment of each pixel in the loss function. To address these issues, we concentrate on the image post-processing and propose a simple yet effective FREquency-band-awarE and SElf-guidED network, termed FreeSeed, which can effectively remove artifacts and recover missing details from the contaminated sparse-view CT images. Specifically, we first propose a frequency-band-aware artifact modeling network (FreeNet), which learns artifact-related frequency-band attention in the Fourier domain for better modeling the globally distributed streak artifact on the sparse-view CT images. We then introduce a self-guided artifact refinement network (SeedNet), which leverages the predicted artifact to assist FreeNet in continuing to refine the severely corrupted details. Extensive experiments demonstrate the superior performance of FreeSeed and its dual-domain counterpart over the state-of-the-art sparse-view CT reconstruction methods. Source code is made available at https://github.com/Masaaki-75/freeseed.",https://github.com/Masaaki-75/freeseed,https://ctcicblog.mayo.edu/2016-low-dose-ct-grand-challenge/,Image Reconstruction,CT,,,,,,,,
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation ,"It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks.  Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model’s performance on clean and adversarial samples.",https://github.com/asif-hanif/vafa,https://www.synapse.org/#!Synapse:syn3193805/wiki/217789,Other,Image Segmentation,Interpretability / Explainability,,,,,,,
Frequency-mixed Single-source Domain Generalization for Medical Image Segmentation ,"The annotation scarcity of medical image segmentation causes challenges in collecting sufficient training data for deep learning models. This obstacle means that models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models in handling unseen domains. However, the DG setup requires multiple source domains, which can impede the efficient deployment of segmentation algorithms in real clinical scenarios. To address this challenge and improve the segmentation model’s generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency’s effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to seamlessly learn and inject robust context-aware representations into the segmentation task. Our experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model’s ability to perform well on unseen domains. Therefore, our approach provides a promising solution for enhancing the generalization ability of medical image segmentation models, especially when annotated data is scarce.",https://github.com/liamheng/Non-IID_Medical_Image_Segmentation,http://www.isi.uu.nl/Research/Databases/DRIVE/,Image Segmentation,Computer Aided Diagnosis,Model Generalizability / Federated Learning,other,,,,,,
From Mesh Completion to AI Designed Crown ,"Designing a dental crown is a time-consuming and labor-intensive process. Our goal is to simplify crown design and minimize the tediousness of making manual adjustments while still ensuring the highest level of accuracy and consistency. To this end, we present a new end-to-end deep learning approach, coined Dental Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud context. The dental context includes the tooth prepared to receive a crown and its surroundings, namely the two adjacent teeth and the three closest teeth in the opposing jaw. We formulate crown generation in terms of completing this point cloud context. A feature extractor first converts the input point cloud into a set of feature vectors that represent local regions in the point cloud. The set of feature vectors is then fed into a transformer to predict a new set of feature vectors for the missing region (crown). Subsequently, a point reconstruction head, followed by a multi-layer perceptron, is used to predict a dense set of points with normals. Finally, a differentiable point-to-mesh layer serves to reconstruct the crown surface mesh. We compare our DMC method to a graph-based convolutional neural network which learns to deform a crown mesh from a generic crown shape to the target geometry. Extensive experiments on our dataset demonstrate the effectiveness of our method, which attains an average of 0.062 Chamfer Distance. 
The code is available at: https://github.com/Golriz-code/DMC.git
Keywords:  Mesh completion · Transformer · 3D shape generation.",https://github.com/Golriz-code/DMC.git,,Attention models,Computer Aided Diagnosis,Surgical Planning and Simulation,,,,,,,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation ,"Accurate and safe catheter ablation procedures for atrial fibrillation require precise segmentation of cardiac structures in Intracardiac Echocardiography (ICE) imaging. Prior studies have suggested methods that employ 3D geometry information from the ICE transducer to create a sparse ICE volume by placing 2D frames in a 3D grid, enabling the training of 3D segmentation models. However, the resulting 3D masks from these models can be inaccurate and may lead to serious clinical complications due to the sparse sampling in ICE data, frames misalignment, and cardiac motion. To address this issue, we propose an interactive editing framework that allows users to edit segmentation output by drawing scribbles on a 2D frame.  The user interaction is mapped to the 3D grid and utilized to execute an editing step that modifies the segmentation in the vicinity of the interaction while preserving the previous segmentation away from the interaction. Furthermore, our framework accommodates multiple edits to the segmentation output in a sequential manner without compromising previous edits. This paper presents a novel loss function and a novel evaluation metric specifically designed for editing. Cross-validation and testing results indicate that, in terms of segmentation quality and following user input, our proposed loss function outperforms standard losses and training strategies. We demonstrate quantitatively and qualitatively that subsequent edits do not compromise previous edits when using our method, as opposed to standard segmentation losses. Our approach improves segmentation accuracy while avoiding undesired changes away from user interactions and without compromising the quality of previously edited regions, leading to better patient outcomes.",,,Image Segmentation,Cardiac,Other,Ultrasound,,,,,,
From Tissue to Sound: Model-based Sonification of Medical Imaging ,"We introduce a general design framework for the interactive sonification of multimodal medical imaging data. The proposed approach operates on a physical model that is generated based on the structure of anatomical tissues. The model generates unique acoustic profiles in response to external interactions, enabling the user to learn about how the tissue characteristics differ from rigid to soft, dense to sparse, structured to scattered. The acoustic profiles are attained by leveraging the topological structure of the model with minimal preprocessing, making this approach applicable to a diverse array of applications. Unlike conventional methods that directly transform low-dimensional data into global sound features, this approach utilizes unsupervised mapping of features between an anatomical data model and a sound model, allowing for the processing of high-dimensional data. We verified the feasibility of the proposed method with an abdominal CT volume. The results show that the method can generate perceptually discernible acoustic signals in accordance with the underlying anatomical structure. In addition to improving the directness and richness of interactive sonification models, the proposed framework provides enhanced possibilities for designing multisensory applications for multimodal imaging data.",,,Surgical Visualization and Mixed/Augmented/Virtual Reality,Guided Interventions and Surgery,,,,,,,,
FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images ,"Unsupervised deformable image registration is one of the challenging tasks in medical imaging. Obtaining a high-quality deformation field while preserving deformation topology remains demanding amid a series of deep-learning-based solutions. Meanwhile, the diffusion model’s latent feature space shows potential in modeling the deformation semantics. To fully exploit the diffusion model’s ability to guide the registration task, we present two modules: Feature-wise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG). Specifically, FDG uses the diffusion model’s multi-scale semantic features to guide the generation of the deformation field. SDG uses the diffusion score to guide the optimization process for preserving deformation topology with barely any additional computation. Experiment results on the 3D medical cardiac image registration task validate our model’s ability to provide refined deformation fields with preserved topology effectively. Code is available at: https://github.com/xmed-lab/FSDiffReg.git.",https://github.com/xmed-lab/FSDiffReg.git,https://acdc.creatis.insa-lyon.fr/description/databases.html,Image Registration,Cardiac,Other,MRI,,,,,,
Full Image-index Remainder based Single Low-dose DR/CT Self-supervised Denoising ,"Low-dose digital radiography (DR) and computed tomography (CT) play a crucial role in minimizing health risks during clinical examinations and diagnoses. However, reducing the radiation dose often leads to lower signal-to-noise ratio measurements, resulting in degraded image quality. Existing supervised and self-supervised reconstruction techniques have been developed with noisy and clean image pairs or noisy and noisy image pairs, implying they cannot be adapted to single DR and CT image denoising. In this study, we introduce the Full Image-Index Remainder (FIRE) method. Our method begins by dividing the entire high-dimensional image space into multiple low-dimensional sub-image spaces using a full image-index remainder technique. By leveraging the data redundancy present within these sub-image spaces, we identify similar groups of noisy sub-images for training a self-supervised denoising network. Additionally, we establish a sub-space sampling theory specifically designed for self-supervised denoising networks. Finally, we propose a novel regularization optimization function that effectively reduces the disparity between self-supervised and supervised denoising networks, thereby enhancing denoising training. Through comprehensive quantitative and qualitative experiments conducted on both clinical low-dose CT and DR datasets, we demonstrate the remarkable effectiveness and advantages of our FIRE method compared to other state-of-the-art approaches.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Musculoskeletal,Image Reconstruction,CT,other,Visualization in Biomedical Imaging,,,,
Fully Bayesian VIB-DeepSSM ,"Statistical shape modeling (SSM) enables population-based quantitative analysis of anatomical shapes, informing clinical diagnosis. Deep learning approaches predict correspondence-based SSM directly from unsegmented 3D images but require calibrated uncertainty quantification, motivating Bayesian formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an effective, principled framework for predicting probabilistic shapes of anatomy from images with aleatoric uncertainty quantification. However, VIB is only half-Bayesian and lacks epistemic uncertainty inference. We derive a fully Bayesian VIB formulation and demonstrate the efficacy of two scalable implementation approaches: concrete dropout and batch ensemble. Additionally, we introduce a novel combination of the two that further enhances uncertainty calibration via multimodal marginalization. Experiments on synthetic shapes and left atrium data demonstrate that the fully Bayesian VIB network predicts SSM from images with improved uncertainty reasoning without sacrificing accuracy.",https://github.com/jadie1/BVIB-DeepSSM,https://drive.google.com/file/d/1oegp1RZVFJfx8s7aL5GgX55UKGhRgAZn/view,Uncertainty,Computational Anatomy and Physiology,,,,,,,,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images ,"Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities.  Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification.",https://github.com/xmed-lab/FDDM,https://github.com/li-xirong/mmc-amd,Ophthalmology,Computer Aided Diagnosis,,,,,,,,
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization ,"Gadolinium-based contrast agents are commonly used in cardiac magnetic resonance (CMR) imaging to characterize myocardial scar tissue. Recent works using deep learning have shown the promise of contrast-free short-axis cine images to detect scars based on wall motion abnormalities (WMA) in ischemic patients. However, WMA can occur in patients without a scar. Moreover, the presence of a scar may not always be accompanied by WMA, particularly in non-ischemic heart disease, posing a significant challenge in detecting scars in such cases. To overcome this limitation, we propose a novel deep spatiotemporal residual attention network (ST-RAN) that leverages temporal and spatialinformation at different scales to detect scars in both ischemic and non-ischemic heart diseases. Our model comprises three primary components. First, we develop a novel factorized 4D (3D+time) convolutional layer that extracts 3D spatial features of the heart and a deep 1D kernel in the temporal direction to extract heart motion. Secondly, we enhance the power of the 4D (3D+time) layer with spatiotemporal attention to extract rich whole-heart features while tracking the long-range temporal relationship between the frames. Lastly, we introduce a residual attention block that extracts spatial and temporal features at different scales to obtain global and local motion features and to detect subtle changes in contrast related to scar. We train and validate our model on a large dataset of 3000 patients who underwent clinical CMR with various indications and different field strengths (1.5T, 3T) from multiple vendors (GE, Siemens) to demonstrate the generalizability and robustness of our model. We show that our model works on both ischemic and non-ischemic heart diseases outperforming state-of-the-art methods. Our code is available at https://github.com/HMS-CardiacMR/Myocardial_Scar_Detection.",https://github.com/HMS-CardiacMR/Myocardial_Scar_Detection,,Attention models,Cardiac,MRI,,,,,,,
Gall Bladder Cancer Detection from US Images with Only Image Level Labels ,"Automated detection of Gallbladder Cancer (GBC) from Ultrasound (US) images is an important problem, which has drawn increased interest from researchers. However, most of these works use difficult-to-acquire information such as bounding box annotations or additional US videos. In this paper, we focus on GBC detection using only image-level labels. Such annotation is usually available based on the diagnostic report of a patient, and do not require additional annotation effort from the physicians. However, our analysis reveals that it is difficult to train a standard image classification model for GBC detection. This is due to the low inter-class variance (a malignant region usually occupies only a small portion of a US image), high intra-class variance (due to the US sensor capturing a 2D slice of a 3D object leading to large viewpoint variations), and low training data availability. We posit that even when we have only the image level label, still formulating the problem as object detection (with bounding box output) helps a deep neural network (DNN) model focus on the relevant region of interest. Since no bounding box annotations is available for training, we pose the problem as weakly supervised object detection (WSOD). Motivated by the recent success of transformer models in object detection, we train one such model, DETR, using multi-instance-learning (MIL) with self-supervised instance selection to suit the WSOD task. Our proposed method demonstrates an improvement of AP and detection sensitivity over the SOTA transformer-based and CNN-based WSOD methods. Project page is at https://gbc-iitd.github.io/wsod-gbc.",https://gbc-iitd.github.io/wsod-gbc,https://gbc-iitd.github.io/gbcu,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Ultrasound,,,,,,,,
Gene-induced Multimodal Pre-training for Image-omic Classification ,"Histology analysis of the tumor micro-environment integrated with genomic assays is the gold standard for most cancers in modern medicine. This paper proposes a Gene-induced Multimodal Pre-training (GiMP) framework, which jointly incorporates genomics and Whole Slide Images (WSIs) for classification tasks. Our work aims at dealing with the main challenges of multi-modality image-omic classification w.r.t. (1) the patient-level feature extraction difficulties from gigapixel WSIs and tens of thousands of genes, and (2) effective fusion considering high-order relevance modeling. Concretely, we first propose a group multi-head self-attention gene encoder to capture global structured features in gene expression cohorts. We design a masked patch modeling paradigm (MPM) to capture the latent pathological characteristics of different tissues. The mask strategy is randomly masking a fixed-length contiguous subsequence of patch embeddings of a WSI. Finally, we combine the classification tokens of paired modalities and propose a triplet learning module to learn high-order relevance and discriminative patient-level information. After pre-training, a simple fine-tuning can be adopted to obtain the classification results. Experimental results on the TCGA dataset show the superiority of our network architectures and our pre-training framework, achieving 99.47% in accuracy for image-omic classification. The code is
publicly available at https://github.com/huangwudiduan/GIMP.",https://github.com/huangwudiduan/GIMP,https://portal.gdc.cancer.gov/,Histopathology,Computational (Integrative) Pathology,,,,,,,,
Generating High-Resolution 3D CT with 12-bit Depth using a Diffusion Model with Adjacent Slice and Intensity Calibration Network ,"Since the advent of generative models, deep learning-based methods for generating high-resolution, photorealistic 2D images have made significant successes. However, it is still difficult to create precise 3D image data with 12-bit depth used in clinical settings that capture the anatomy and pathology of CT and MRI scans. Using a score-based diffusion model, we propose a slice-based method that generates 3D images from previous 2D CT slices along the inferior direction. We call this method stochastic differential equations with adjacent slice-based conditional iterative inpainting (ASCII). We also propose an intensity calibration network (IC-Net) that adjusts the among slices intensity mismatch caused by 12-bit depth image generation. As a result, Frechet Inception Distance (FIDs) scores of FID-Ax, FID-Cor and FID-Sag of ASCII(2) with IC-Net were 14.993, 19.188 and 19.698, respectively. Anatomical continuity of the generated 3D image along the inferior direction was evaluated by an expert radiologist with more than 15 years of experience. In the analysis of eight anatomical structures, our method was evaluated to be continuous for seven of the structures.",,,Image Reconstruction,CT,,,,,,,,
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model ,"As acquiring MRIs is expensive, neuroscience studies struggle to attain a sufficient number of them for properly training deep learning models. This challenge could be reduced by MRI synthesis, for which Generative Adversarial Networks (GANs) are popular. GANs, however, are commonly unstable and struggle with creating diverse and high-quality data. A more stable alternative is Diffusion Probabilistic Models (DPMs) with a fine-grained training strategy. To overcome their need for extensive computational resources, we propose a conditional DPM (cDPM) with a memory-efficient process that generates realistic-looking brain MRIs. To this end, we train a 2D cDPM to generate an MRI subvolume conditioned on another subset of slices from the same MRI. By generating slices using arbitrary combinations between condition and target slices, the model only requires limited computational resources to learn interdependencies between slices even if they are spatially far apart. After having learned these dependencies via an attention network, a new anatomy-consistent 3D brain MRI is generated by repeatedly applying the cDPM. Our experiments demonstrate that our method can generate high-quality 3D MRIs that share a similar distribution to real MRIs while still diversifying the training set. The code is available at https://github.com/xiaoiker/mask3DMRI_diffusion and also will be released as part of MONAI, at https://github.com/Project-MONAI/GenerativeModels .",,,MRI,Neuroimaging - Others,Image Reconstruction,Other,,,,,,
Geometric Ultrasound Localization Microscopy ,"Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and robustness while only utilizing a portion of the available transducer data.",,https://zenodo.org/record/4343435,Ultrasound,Vascular,Visualization in Biomedical Imaging,,,,,,,
Geometry-adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders ,"Placenta accreta spectrum (PAS) is a high-risk obstetric disorder associated with significant morbidity and mortality. Since the abnormal invasion usually occurs near the uteroplacental interface, there is a large geometry variation in the lesion bounding boxes, which considerably degrades the detection performance. In addition, due to the confounding visual representations of PAS, the diagnosis highly depends on the clinical experience of radiologists, which easily results in inaccurate bounding box annotations. In this paper, we propose a geometry adaptive network for robust PAS detection. Specifically, to deal with the geometric prior missing problem, we design a Geometry-adaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module. The GA-LA strategy dynamically selects positive PAS candidates (RoIs) for each lesion according to its shape information. The GA-RF module aggregates the multi-scale RoI features based on the geometry distribution of proposals. Moreover, we develop a Lesion-aware Detection Head (LA-Head) to leverage high-quality predictions to iteratively refine inaccurate annotations with a novel multiple instance learning paradigm. Experimental results under both clean and noisy labels indicate that our method achieves state-of-the-art performance and demonstrate promising assistance for PAS diagnosis in clinical applications.",https://github.com/Meteorary/GALA,,MRI,Abdomen,Other,,,,,,,
Geometry-invariant abnormality detection ,"Cancer is a highly heterogeneous condition best visualised in positron emission tomography. Due to this heterogeneity, a general purpose cancer detection model can be built using unsupervised learning anomaly detection models. While prior work in this field has showcased the efficacy of abnormality detection methods (e.g. Transformer-based), these have shown significant vulnerabilities to differences in data geometry. Changes in image resolution or observed field of view can result in inaccurate predictions, even with significant data pre-processing and augmentation. We propose a new spatial conditioning mechanism that enables models to adapt and learn from varying data geometries, and apply it to a state-of-the-art Vector-Quantized Variational Autoencoder + Transformer abnormality detection model. We showcase that this spatial conditioning mechanism statistically-significantly improves model performance on whole-body data compared to the same model
without conditioning, while allowing the model to perform inference at varying data geometries.",,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=93258287,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,Attention models,Model Generalizability / Federated Learning,PET/SPECT,,,,,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation ,"Cardiac structure segmentation from echocardiogram videos plays a crucial role in diagnosing heart disease. The combination of multi-view echocardiogram data is essential to enhance the accuracy and robustness of automated methods. However, due to the visual disparity of the data, deriving cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. In this study, we propose a novel global-local fusion (GL-Fusion) network to utilize both global-based and local-based multi-view information to improve the accuracy of echocardiogram analysis. Specifically, a multi-view global-based fusion module (MGFM) is proposed to mine global context information and to constrain feature pairs in the same phase of different cardiac cycles. Additionally, a multi-view local-based fusion module (MLFM) is designed to extract local features’ correlations of cardiac structures in different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 81.57% average dice score, which demonstrates a 7.11% improvement over the baseline method, and outperforms other existing state-of-the-art methods. To our knowledge, this is the first exploration of a multi-view method for echocardiogram video segmentation.",https://github.com/xmed-lab/GL-Fusion,,Ultrasound,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling ,"In dynamic Magnetic Resonance (MR) imaging, k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked-image-modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.",,,Image Reconstruction,Cardiac,Attention models,MRI,,,,,,
"GLSFormer : Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos ","Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer",https://github.com/nisargshah1999/GLSFormer,http://ftp.itec.aau.at/datasets/ovid/cat-101/,Surgical Skill and Work Flow Analysis,Video,,,,,,,,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging ,"Federated learning has been extensively explored in privacy-preserving medical image analysis. However, the domain shift widely existed in real-world scenarios still greatly limits its practice, which requires to consider both generalization and personalization, namely generalized and personalized federated learning (GPFL). 
Previous studies almost focus on the partial objective of GPFL: personalized federated learning mainly cares about its local performance, which cannot guarantee a generalized global model for unseen clients; federated domain generalization only considers the out-of-domain performance, ignoring the performance of the training clients. To achieve both objectives effectively, we propose a novel GRAdient CorrEction (GRACE) method. GRACE incorporates a feature alignment regularization under a meta-learning framework on the client side to correct the personalized gradients from overfitting. Simultaneously, GRACE employs a consistency-enhanced re-weighting aggregation to calibrate the uploaded gradients on the server side for better generalization. Extensive experiments on two medical image benchmarks demonstrate the superiority of our method under various GPFL settings. Code available at https://github.com/MediaBrain-SJTU/GPFL-GRACE.",https://github.com/MediaBrain-SJTU/GPFL-GRACE,,Model Generalizability / Federated Learning,Meta-learning,Transfer learning,,,,,,,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels ,"Noisy annotations are inevitable in clinical practice due to the requirement of labeling efforts and expert domain knowledge. Therefore, medical image classification with noisy labels is an important topic. A recently advanced paradigm in learning with noisy labels (LNL) first selects clean data with small-loss criterion, then formulates the LNL problem as semi-supervised learning (SSL) task and employs Mixup to augment the dataset. However, the small-loss criterion is vulnerable to noisy labels and the Mixup operation is prone to accumulate errors in pseudo labels. To tackle these issues, this paper presents a two-stage framework with novel criteria for clean data selection and a more advanced Mixup method for SSL. In the clean data selection stage, based on the observation that gradient space reflects optimization dynamics and feature space is more robust to noisy labels, we propose two novel criteria, i.e., Gradient Conformity-based Selection (GCS) and Feature Conformity-based Selection (FCS), to select clean samples. Specifically, the GCS and FCS criteria identify clean data that better aligns with the class-wise optimization dynamics in the gradient space and principal eigenvector in the feature space. In the SSL stage, to effectively augment the dataset while mitigating the disturbance of unreliable pseudo-labels, we propose a Sample Reliability-based Mixup (SRMix) method which selects mixup partners based on their spatial reliability, temporal stability, and prediction confidence. Extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods on two medical datasets with synthetic and real-world label noise.",,,Computer Aided Diagnosis,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification ,"There is significant interest in using neuroimaging data for schizophrenia classification. Graph convolutional networks (GCNs) provide great potential to improve schizophrenia classification using brain graphs derived from neuroimaging data. However, accurate classification of schizophrenia is still challenging due to the heterogeneity of schizophrenia and their subtle differences in neuroimaging features. This paper presents a new graph convolutional framework for population-based schizophrenia classification that leverages graph-theoretical measures of morphometric similarity networks inferred from structural MRI scans and incorporates variational edges to reinforce the learning process. Specifically, we construct individual morphometric similarity networks based on inter-regional similarity of multiple morphometric features (cortical thickness, surface area, gray matter volume, mean curvature, and Gaussian curvature) extracted from T1-weighted MRI. We then formulate an adaptive population graph where each node is represented by the topological features of individual morphometric similarity networks and each edge models the similarity between the topological features of the subjects and incorporates the phenotypic information. An encode module is devised to estimate the associations between phenotypic data of the subjects and to adaptively optimize the edge weights. Our proposed method is evaluated on a large dataset collected from nine sites, resulting in a total sample of 366 patients with schizophrenia and 590 healthy individuals. Experimental results demonstrate that our proposed method improves the classification performance over traditional machine learning algorithms, with a mean classification accuracy of 81.8%. The most salient regions contributing to classification are primarily identified in the middle temporal gyrus and superior temporal gyrus.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - Others,MRI,Treatment Response and Outcome/Disease Prediction,,,,,,
Graph-theoretic automatic lesion tracking and detection of patterns of lesion changes in longitudinal CT studies ,"Radiological follow-up of oncological patients requires the analysis and comparison of multiple unregistered scans acquired every few months. This process is currently often partial, time-consuming and subject to variability. We present a new, generic, graph-based method for tracking individual lesion changes and detecting patterns in the evolution of lesions over time. The tasks are formalized as graph-theoretic problems in which lesions are vertices and edges are lesion pairings computed by overlap-based lesion matching. We define seven individual lesion change classes and five lesion change patterns that fully summarize the evolution of lesions over time. They are directly computed from the graph properties and its connected components with graph-based methods. Experimental results on lung (83 CTs from 19 patients) and liver (77 CECTs from 18 patients) datasets with more than two scans per patient yielded an individual lesion change class accuracy of 98% and 85%, and identification of patterns of lesion change with an accuracy of 96% and 76%, respectively. Highlighting unusual lesion labels and lesion change patterns in the graph helps radiologists identify overlooked or faintly visible lesions. Automatic lesion change classification and pattern detection in longitudinal studies may improve the accuracy and efficiency of radiological interpretation and disease status evaluation.",,,Treatment Response and Outcome/Disease Prediction,CT,,,,,,,,
GSDG: Exploring A Global Semantic-guided Dual-stream Graph Model for Automated Volume Differential Diagnosis and Prognosis ,"Three-dimensional medical images are crucial for the early screening and prognosis of numerous diseases. However, constructing an accurate computer-aided prediction model is challenging when dealing with volumes of different sizes due to numerous slices (native nodes) in a single case and variable-length slice sequence. We propose a Global Semantic-guided Dual-stream Graph model to address this issue. Our approach differs from the existing solution that aligns volumes with varying numbers of slices through downsampling. Instead, we leverage global semantic vectors to guide the grouping of native nodes, construct super-nodes, and build dual-stream graphs by incorporating the sequential association of each volume’s unique slices and the feature association of global semantic vectors. Specifically, we propose a shared global semantic vectors-based grouping method that aligns the number of nodes and the semantic distribution of nodes among different volumes without discarding slices. Furthermore, we construct a dual-stream graph module that enables Graph Convolutional Networks (GCN) to make clinical predictions from computer tomography (CT) volumes through the natural sequence association between native nodes and, simultaneously, the latent feature association between semantic vectors. We provide interpretability by visualizing the distribution of native nodes within each group and weakly-supervised slice localization. The results demonstrate that our method outperforms previous work in diagnostic (96.74%, +2.81%) and prognostic accuracy (84.56%, +1.86%) while being more interpretable, making it a promising approach for medical image analysis scenarios with limited fine-grained annotation.",,http://ncov-ai.big.ac.cn/download?lang=en,Computer Aided Diagnosis,Lung,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,,,,,
GSMorph: Gradient Surgery for cine-MRI Cardiac Deformable Registration ,"Deep learning-based deformable registration methods have been widely investigated in diverse medical applications.
Learning-based deformable registration relies on weighted objective functions trading off registration accuracy and smoothness of the deformation field. Therefore, they inevitably require tuning the hyperparameter for optimal registration performance. Tuning the hyperparameters is highly computationally expensive and introduces undesired dependencies on domain knowledge. In this study, we construct a registration model based on the gradient surgery mechanism, named GSMorph, to achieve a hyperparameter-free balance on multiple losses. In GSMorph, we reformulate the optimization procedure by projecting the gradient of similarity loss orthogonally to the plane associated with the smoothness constraint, rather than additionally introducing a hyperparameter to balance these two competing terms. Furthermore, our method is model-agnostic and can be merged into any deep registration network without introducing extra parameters or slowing down inference. In this study, We compared our method with state-of-the-art (SOTA) deformable registration approaches over two publicly available cardiac MRI datasets. GSMorph proves superior to five SOTA learning-based registration models and two conventional registration techniques, SyN and Demons, on both registration accuracy and smoothness.",https://github.com/wulalago/GSMorph,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,Image Registration,Cardiac,MRI,,,,,,,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images ,"Interactive segmentation reduces the annotation time of medical images and allows annotators to iteratively refine labels with corrective interactions, such as clicks. While existing interactive models transform clicks into user guidance signals, which are combined with images to form (image, guidance) pairs, the question of how to best represent the guidance has not been fully explored. To address this, we conduct a comparative study of existing guidance signals by training interactive models with different signals and parameter settings to identify crucial parameters for the model’s design. Based on our findings, we design a guidance signal that retains the benefits of other signals while addressing their limitations. We propose an adaptive Gaussian heatmap guidance signal that utilizes the geodesic distance transform to dynamically adapt the radius of each heatmap when encoding clicks. We conduct our study on the MSD Spleen and the AutoPET datasets to explore the segmentation of both anatomy (spleen) and pathology (tumor lesions). Our results show that choosing the guidance signal is crucial for interactive segmentation as we improve the performance by 14% Dice with our adaptive heatmaps on the challenging AutoPET dataset when compared to non-interactive models. This brings interactive models one step closer to deployment in clinical workflows. We will make our code publicly available.",https://github.com/Zrrr1997/Guiding-The-Guidance,http://medicaldecathlon.com/,Image Segmentation,Data Efficient Learning,CT,PET/SPECT,,,,,,
H2GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment ,"Novel brain gyrus landmarks, the gyral hinges (GHs), have been identified and demonstrated to be of functional importance, and a subset of them have correspondences across subjects and species. A precise GH alignment is crucial for understanding the relationship between brain structure and function. However, accurate and robust GH alignment is challenging due to the massive cortical morphological variations of GHs between subjects. Previous studies typically construct a single-scale graph to model the GHs relations and deploy the graph matching algorithms for GH alignment but suffer from two overlooked deficiencies. First, they consider only pairwise relations between GHs, ignoring that their relations are highly complex. Second, they only consider the point scale for graph-based GH alignment, which inevitably introduces several alignment errors on small-scaled regions. To overcome these deficiencies, we propose a Hierarchical HyperGraph Matching (H2GM) framework for GH alignment, consisting of a Multi-scale Hypergraph Establishment (MsHE) module, a Multi-scale Hypergraph Matching (MsHM) module, and an Inter-Scale Consistency (ISC) constraint. Specifically, the MsHE module constructs multi-scale hypergraphs by utilizing abundant biological evidence and models high-order relations between GHs at different scales. The MsHM module matches hypergraph pairs at each scale to entangle a robust GH alignment with multi-scale high-order cues. And the ISC constraint incorporates inter-scale semantic consistency to encourage the agreement of multi-scale knowledge. Experimental results demonstrate that H2GM improves GH alignment remarkably and outperforms state-of-the-art methods. Our code will make public later.",https://github.com/CUHK-AIM-Group/HHGM,,Image Registration,Other,MRI,,,,,,,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis ,"Placenta Accreta Spectrum (PAS) can lead to high risks like excessive blood loss at the delivery. Therefore, prenatal screening with MRI is essential for delivery planning that ensures better clinical outcomes. For computer-aided PAS diagnosis, existing work mostly extracts radiomics features directly from ROI while ignoring the context information, or learns global semantic features with limited awareness of the focal area. Moreover, they usually select single or few MRI slices to represent the whole sequences, which can result in biased decisions. To deal with these issues, a novel end-to-end Hierarchical Attention and Contrastive Learning Network (HACL-Net) is proposed under the formulation of a multi-instance problem. Slice-level attention module is first designed to extract context-aware deep semantic features. These slice-wise features are then aggregated via the patient-level attention module into task-specific patient-wise representation for PAS prediction. A plug-and-play contrastive learning module is introduced to further improve the discriminating power of extracted features. Extensive experiments with ablation studies on a real clinical dataset show that HACL-Net can achieve state-of-the-art prediction performance with the effectiveness of each module.",https://github.com/LouieBHLu/HACL-Net,,Computer Aided Diagnosis,Fetal Imaging,Attention models,MRI,,,,,,
HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation ,"With the introduction of Transformers, different attention-based models have been proposed for image segmentation with promising results. Although self-attention allows capturing of long-range dependencies, it suffers from a quadratic complexity in the image size especially in 3D. To avoid the out-of-memory error during training, input size reduction is usually required for 3D segmentation, but the accuracy can be suboptimal when the trained models are applied on the original image size. To address this limitation, inspired by the Fourier neural operator (FNO), we introduce the HartleyMHA model which is robust to training image resolution with efficient self-attention. FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the appealing properties of zero-shot super-resolution and global receptive field. We modify the FNO by using the Hartley transform with shared parameters to reduce the model size by orders of magnitude, and this allows us to further apply self-attention in the frequency domain for more expressive high-order feature combination with improved efficiency. When tested on the BraTS’19 dataset, it achieved superior robustness to training image resolution than other tested models with less than 1% of their model parameters.",,,Image Segmentation,Attention models,,,,,,,,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis ,"Accurate periodontal disease classification from panoramic X-ray images is of great significance for efficient clinical diagnosis and treatment. It has been a challenging task due to the subtle evidence in radiography. Recent methods attempt to estimate bone loss on these images to classify periodontal diseases, relying on the radiographic manual annotations to supervise segmentation or keypoint detection.
However, these radiographic annotations are inconsistent with clinical golden standard of probing measurements, and thus can lead to measurement errors and unstable classifications. In this paper, we propose a novel hybrid classification framework, HC-Net, for accurate periodontal disease classification from X-ray images, which consists of three components, i.e., tooth-level classification, patient-level classification, and a learnable adaptive noisy-OR gate. Specifically, in the tooth-level classification, we first introduce instance segmentation to capture each tooth, and then classify the periodontal disease in the tooth level. As for the patient-level, we exploit a multi-task strategy to jointly learn patient-level classification and classification activation map (CAM) that reflects confidence of local lesion areas upon the panoramic X-ray image. Eventually, the adaptive noisy-OR gate obtains a hybrid classification by integrating predictions from both levels.
Extensive experiments on the dataset collected from real-world clinics demonstrate that our proposed HC-Net achieves state-of-the-art performance in periodontal disease classification and shows great application potential.",https://github.com/ShanghaiTech-IMPACT/Periodental_Disease,,Computer Aided Diagnosis,Treatment Response and Outcome/Disease Prediction,,,,,,,,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation ,"Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https://github.com/shijun18/H-DenseFormer.",https://github.com/shijun18/H-DenseFormer,PI-CAI22: https://pi-cai.grand-challenge.org/,Image Segmentation,CT,MRI,PET/SPECT,,,,,,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images ,"Pulmonary vessel segmentation in computerized tomography (CT) images is essential for pulmonary vascular disease and surgical navigation. However, the existing methods were generally designed for contrast-enhanced images, their performance is limited by the low contrast and the non-uniformity of Hounsfield Unit (HU) in non-contrast CT images, meanwhile, the varying size of the vessel structures are not well considered in current pulmonary vessel segmentation methods. To address this issue, we propose a hierarchical enhancement network (HENet) for better image- and feature-level vascular representation learning in the pulmonary vessel segmentation task. Specifically, we first design an Auto Contrast Enhancement (ACE) module to adjust the vessel contrast dynamically. Then, we propose a Cross-Scale Non-local Block (CSNB) to effectively fuse multi-scale features by utilizing both local and global semantic information. Experimental results show that our approach achieves better pulmonary vessel segmentation outcomes compared to other state-of-the-art methods, demonstrating the efficacy of the proposed ACE and CSNB module. Our code is available at https://github.com/CODESofWenqi/HENet.",https://github.com/CODESofWenqi/HENet,,CT,Lung,Vascular,Image Segmentation,,,,,,
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images ,"Chest radiography is a commonly used diagnostic imaging exam for monitoring disease progression and treatment effectiveness. While machine learning has made significant strides in tasks such as image segmentation, disease diagnosis, and automatic report generation,  more intricate tasks such as disease progression  monitoring remain fairly underexplored. 
This task presents a formidable challenge because of the complex and intricate nature of disease appearances on chest X-ray images, which makes distinguishing significant changes from irrelevant variations between images challenging.
Motivated by these challenges, this work proposes CheXRelFormer, an end-to-end siamese Transformer disease progression model that takes a pair of images as input and detects whether the patient’s condition has improved, worsened, or remained unchanged. The model comprises two hierarchical Transformer encoders, a difference module that compares feature differences across images, and a final classification layer that predicts the change in the patient’s condition. Experimental results demonstrate that CheXRelFormer outperforms previous counterparts. Code is available at https://github.com/PLAN-Lab/CheXRelFormer",https://github.com/PLAN-Lab/CheXRelFormer,https://physionet.org/content/chest-imagenome/1.0.0/,Computer Aided Diagnosis,Attention models,Other,,,,,,,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights ,"Occlusion-free video generation is challenging due to surgeons’ obstructions in the camera field of view. Prior work has addressed this issue by installing multiple cameras on a surgical light, hoping some cameras will observe the surgical field with less occlusion. However, this special camera setup poses a new imaging challenge since camera configurations can change every time surgeons move the light, and manual image alignment is required. This paper proposes an algorithm to automate this alignment task. The proposed method detects frames where the lighting system moves, realigns them, and selects the camera with the least occlusion. This algorithm results in a stabilized video with less occlusion. Quantitative results show that our method outperforms conventional approaches. A user study involving medical doctors also confirmed the superiority of our method.",https://github.com/isogawalab/SingleViewSurgicalVideo,,Surgical Visualization and Mixed/Augmented/Virtual Reality,Image Reconstruction,Surgical Scene Understanding,,,,,,,
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers ","Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable.
The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation.
However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.",https://github.com/MWod/DeepImplant_MICCAI_2023,https://www.sciencedirect.com/science/article/pii/S2352340921001864,Image Segmentation,Neuroimaging - Others,Computational Anatomy and Physiology,Guided Interventions and Surgery,Interventional Imaging Systems,Data Efficient Learning,Other,other,Surgical Data Science,Surgical Planning and Simulation
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis ,"In computation pathology, the pyramid structure of gigapixel Whole Slide Images (WSIs) has recently been studied for capturing various information from individual cell interactions to tissue microenvironments. This hierarchical structure is believed to be beneficial for cancer diagnosis and prognosis tasks. However, most previous hierarchical WSI analysis works (1) only characterize local or global correlations within the WSI pyramids and (2) use only unidirectional interaction between different resolutions, leading to an incomplete picture of WSI pyramids. To this end, this paper presents a novel Hierarchical Interaction Graph-Transformer (i.e., HIGT) for WSI analysis. With Graph Neural Network and Transformer as the building commons, HIGT can learn both short-range local information and long-range global representation of the WSI pyramids. Considering that the information from different resolutions is complementary and can benefit each other during the learning process, we further design a novel Bidirectional Interaction module to establish communication between different levels within the WSI pyramids. Finally, we aggregate both coarse-grained and fine-grained features learned from different levels together for slide-level prediction. We evaluated our methods on two public WSI datasets from TCGA projects, i.e., kidney carcinoma (KICA) and esophageal carcinoma (ESCA). Experimental results show that our HIGT outperforms both hierarchical and non-hierarchical state-of-the-art methods on both tumor subtyping and staging tasks.",https://github.com/HKU-MedAI/HIGT,,Histopathology,Computational (Integrative) Pathology,,,,,,,,
Histopathology Image Classification using Deep Manifold Contrastive Learning ,"Contrastive learning has gained popularity due to its robustness with good feature representation performance. However, cosine distance, the commonly used similarity metric in contrastive learning, is not well suited to represent the distance between two data points, especially on a nonlinear feature manifold. Inspired by manifold learning, we propose a novel extension of contrastive learning that leverages geodesic distance between features as a similarity metric for histopathology whole slide image classification. To reduce the computational overhead in manifold learning, we propose geodesic-distance-based feature clustering for efficient contrastive loss evaluation using prototypes without time-consuming pairwise feature similarity comparison. The efficacy of the proposed method is evaluated on two real-world histopathology image datasets. Results demonstrate that our method outperforms stateof-the-art cosine-distance-based contrastive learning methods.",https://github.com/hvcl/Deep-Manifold-Contrastive-Learning,,Computational (Integrative) Pathology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers? ,"Pruning has emerged as a powerful technique for compressing deep neural networks, reducing memory usage and inference time without significantly affecting overall performance. However, the nuanced ways in which pruning impacts model behavior are not well understood, particularly for long-tailed, multi-label datasets commonly found in clinical settings. This knowledge gap could have dangerous implications when deploying a pruned model for diagnosis, where unexpected model behavior could impact patient well-being. To fill this gap, we perform the first analysis of pruning’s effect on neural networks trained to diagnose thorax diseases from chest X-rays (CXRs). On two large CXR datasets, we examine which diseases are most affected by pruning and characterize class “forgettability” based on disease frequency and co-occurrence behavior. Further, we identify individual CXRs where uncompressed and heavily pruned models disagree, known as pruning-identified exemplars (PIEs), and conduct a human reader study to evaluate their unifying qualities. We find that radiologists perceive PIEs as having more label noise, lower image quality, and higher diagnosis difficulty. This work represents a first step toward understanding the impact of pruning on model behavior in deep long-tailed, multi-label medical image classification. All code, model weights, and data access instructions can be found at https://github.com/VITA-Group/PruneCXR.",https://github.com/VITA-Group/PruneCXR,https://nihcc.app.box.com/v/ChestXray-NIHCC,Computer Aided Diagnosis,Lung,Interpretability / Explainability,Other,CT,other,,,,
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? ,"Deep Neural Networks (DNNS) have been successful in various computer vision tasks, but are known to be uncalibrated, and make overconfident mistakes. This erodes a user’s confidence in the model and is a major concern in their applicability for critical tasks like medical imaging. In the last few years, researchers have proposed various metrics to measure miscalibration, and techniques to calibrate DNNS. However, our investigation shows that for small datasets, typical for medical imaging tasks, the common metrics for calibration, have a large bias as well as variance. It makes these metrics highly unreliable, and unusable for medical imaging. Similarly, we show that state-of-the-art (SOTA) calibration techniques while effective on large natural image datasets, are ineffective on small medical imaging datasets. We discover that the reason for failure is large variance in the density estimation using a small sample set. We propose a novel evaluation metric that incorporates the inherent uncertainty in the predicted confidence, and regularizes the density estimation using a parametric prior model. We call our metric, Robust Expected Calibration Error (RECE), which gives a low bias, and low variance estimate of the expected calibration error, even on the small datasets. In addition, we propose a novel auxiliary loss - Robust Calibration Regularization (RCR) which rectifies the above issues to calibrate the model at train time. We demonstrate the effectiveness of our RECE metric as well as the RCR loss on several medical imaging datasets and achieve SOTA calibration results on both standard calibration metrics as well as RECE. We also show the benefits of using our loss on general classification datasets. The source code and all trained models have been released.",https://github.com/MayankGupta73/Robust-Calibration,,Uncertainty,Computer Aided Diagnosis,,,,,,,,
Identification of Disease-sensitive Brain Imaging Phenotypes and Genetic Factors using GWAS Summary Statistics ,"Brain imaging genetics is a rapidly growing neuroscience area that integrates genetic variations and brain imaging phenotypes to investigate the genetic underpinnings of brain disorders. In this field, using multi-modal imaging data can leverage complementary information and thus stands a chance of identifying comprehensive genetic risk factors. Due to privacy and copyright issues, many imaging and genetic data are unavailable, and thus existing imaging genetic methods cannot work. In this paper, we proposed a novel multi-modal brain imaging genetic learning method that can study the associations between imaging phenotypes and genetic variations using genome-wide association study (GWAS) summary statistics. Our method leverages the powerful multi-modal of brain imaging phenotypes and GWAS. More importantly, it does not need to access the imaging and genetic data of each individual. Experimental results on both Alzheimer’s Disease Neuroimaging Initiative (ADNI) database and GWAS summary statistics suggested that our method has the same learning ability, including identifying associations between genetic biomarkers and imaging phenotypes and selecting relevant biomarkers, as those counterparts depending on the individual data. Therefore, our learning method provides a novel methodology for brain imaging genetics without individual data.",,,Population Imaging and Imaging Genetics,Meta-learning,,,,,,,,
IIB-MIL: Integrated instance-level and bag-level multiple instances learning with label disambiguation for pathological image analysis ,"Digital pathology plays a pivotal role in the diagnosis and interpretation of diseases and has drawn increasing attention in modern healthcare. Due to the huge gigapixel-level size and diverse nature of whole-slide images (WSIs), analyzing them through multiple instance learning (MIL) has become a widely-used scheme, which, however, faces the challenges that come with the weakly supervised nature of MIL. Conventional MIL methods mostly either utilized instance-level or bag-level supervision to learn informative representations from WSIs for downstream tasks. In this work, we propose a novel MIL method for pathological image analysis with integrated instance-level and bag-level supervision (termed IIB-MIL). More importantly, to overcome the weakly supervised nature of MIL, we design a label-disambiguation-based instance-level supervision for MIL using Prototypes and Confidence Bank to reduce the impact of noisy labels. Extensive experiments demonstrate that IIB-MIL outperforms state-of-the-art approaches in both benchmarking datasets and addressing the challenging practical clinical task. The code is available at https://github.com/TencentAILabHealthcare/IIB-MIL.",https://github.com/TencentAILabHealthcare/IIB-MIL,,Computational (Integrative) Pathology,Computer Aided Diagnosis,,,,,,,,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions ,"Statistical shape modeling (SSM) is an essential tool for analyzing variations in anatomical morphology. In a typical SSM pipeline, 3D anatomical images, gone through segmentation and rigid registration, are represented using lower-dimensional shape features, on which statistical analysis can be performed. Various methods for constructing compact shape representations have been proposed, but they involve laborious and costly steps. We propose Image2SSM, a novel deep-learning-based approach for SSM that leverages image-segmentation pairs to learn a radial-basis-function (RBF)-based representation of shapes directly from images. This RBF-based shape representation offers a rich self-supervised signal for the network to estimate a continuous, yet compact representation of the underlying surface that can adapt to complex geometries in a data-driven manner. Image2SSM can characterize populations of biological structures of interest by constructing statistical landmark-based shape models of ensembles of anatomical shapes while requiring minimal parameter tuning and no user assistance. Once trained, Image2SSM can be used to infer low-dimensional shape representations from new unsegmented images, paving the way toward scalable approaches for SSM, especially when dealing with large cohorts. Experiments on synthetic and
real datasets show the efficacy of the proposed method compared to the
state-of-art correspondence-based method for SSM.",,,Other,Musculoskeletal,Vascular,Computational Anatomy and Physiology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI,Population Imaging and Imaging Genetics,,
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure ,"High-level cognitive assistance, such as predicting dissection trajectories in Endoscopic Submucosal Dissection (ESD), can potentially support and facilitate surgical skills training. However, it has rarely been explored in existing studies. Imitation learning has shown its efficacy in learning skills from expert demonstrations, but it faces challenges in predicting uncertain future movements and generalizing to various surgical scenes. In this paper, we introduce imitation learning to the formulated task of learning how to suggest dissection trajectories from expert video demonstrations. We propose a novel method with implicit diffusion policy imitation learning (iDiff-IL) to address this problem. Specifically, our approach models the expert behaviors using a joint state-action distribution in an implicit way. It can capture the inherent stochasticity of future dissection trajectories, therefore allows robust visual representations for various endoscopic views. By leveraging the diffusion model in policy learning, our implicit policy can be trained and sampled efficiently for accurate predictions and good generalizability. To achieve conditional sampling from the implicit policy, we devise a forward-process guided action inference strategy that corrects the state mismatch. We collected a private ESD video dataset with 1032 short clips to validate our method. Experimental results demonstrate that our solution outperforms SOTA imitation learning methods on our formulated task. To the best of our knowledge, this is the first work applying imitation learning for surgical skill learning with respect to dissection trajectory prediction.",,,Guided Interventions and Surgery,Reinforcement Learning,,,,,,,,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts ,"Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation prediction with the ambiguous coordinate-based point representations and aggregate these features to adaptively refine the boundary region. To parallelly optimize multi-scale pixel-level features, we leverage the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a stochastic gating mechanism. Our experiments demonstrate that MORSE can work well with different medical segmentation backbones, consistently achieving competitive performance improvements in both 2D and 3D supervised medical segmentation methods. We also theoretically analyze the superiority of MORSE.",https://github.com/charlesyou999648/MORSE,,Image Segmentation,Other,Uncertainty,,,,,,,
Implicit neural representations for joint decomposition and registration of gene expression images in the marmoset brain ,"We propose a novel image registration method based on implicit neural representations that addresses the challenging problem of registering a pair of brain images with similar anatomical structures, but where one image contains additional features or artifacts that are not present in the other image. To demonstrate its effectiveness, we use 2D microscopy in situ hybridization gene expression images of the marmoset brain. Accurately quantifying gene expression requires image registration to a brain template, which is difficult due to the diversity of patterns causing variations in visible anatomical brain structures. Our approach uses implicit networks in combination with an image exclusion loss to jointly perform the registration and decompose the image into a support and residual image. The support image aligns well with the template, while the residual image captures individual image characteristics that diverge from the template. In experiments, our method provided excellent results and outperformed other registration techniques.",https://github.com/BrainImageAnalysis/ImpRegDec,https://gene-atlas.brainminds.jp/,Image Registration,Neuroimaging - Others,,,,,,,,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior ,"The variational registration model takes advantage of explaining uncertainties of registration results. However, most existing variational registration models are based on convolutional neural networks (CNNs), which cannot capture distant information in images. Besides, the evidence lower bound (ELBO) and the commonly used standard prior cannot close the gap between the real posterior and the variational posterior in the vanilla variational registration model. This paper proposes a network in a variational image registration model for cardiac motion estimation to effectively capture the spatial correspondence of long-distance images and solve the shortcomings of CNNs. Our proposed network comprises a Transformer with a T2T module and the cross attention between the moving and the fixed images. To close the gap between the real posterior and the variational posterior, the importance-weighted evidence lower bound (iwELBO) is introduced into the variational registration model with an implicit prior. The coefficients of a parametric transformation using multi-supports CSRBFs are latent variables in our variational registration model, which improve registration accuracy significantly. Experimental results show that the proposed method outperforms state-of-arts research on public cardiac datasets.",,,Image Registration,Cardiac,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,MRI,,,,
Improved flexibility and interpretability of large vessel stroke prognostication using image synthesis and multi-task learning ,"While acute ischemic stroke due to large vessel occlusion (LVO) may be life-threatening or permanently disabling, timely intervention with endovascular thrombectomy (EVT) can prove life-saving for affected patients. Appropriate patient selection based on prognostic prediction is vital for this costly and invasive procedure, as not all patients will benefit from EVT. Accurate prognostic prediction for LVO presents a significant challenge. Computed Tomography Perfusion (CTP) maps can provide additional information for clinicians to make decisions. 
However, CTP maps are not always available due to variations in available equipment, funding, expertise and image quality. To address these gaps, we test (i) the utility of acquired CTP maps in a deep learning prediction model, (ii) the ability to improve flexibility of this model through image synthesis, and (iii) the added benefits of including multi-task learning with a simple clinical task to focus the synthesis on key clinical features. Our results demonstrate that network architectures utilising a full set of images can still be flexibly deployed if CTP maps are unavailable as their benefits can be effectively synthesized from more widely available images (NCCT and CTA). Additionally, such synthesized images may help with interpretability and building a clinically trusted model.",,,Treatment Response and Outcome/Disease Prediction,Neuroimaging - Others,Image Reconstruction,CT,,,,,,
Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction ,"Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2* relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and dividing sampled data into three groups, the proposed approach achieves superior results compared to the state-of-the-art parallel imaging method, as demonstrated in an in-vivo experiment.",https://github.com/jaejin-cho/miccai2023,https://www.dropbox.com/s/2rteu3vmtbj15kx/example.mat?dl=0,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - DWI and Tractography,Image Reconstruction,Data Efficient Learning,MRI,,,,,
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer ,"Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method with existing methods in the multi-center (n=4) dataset with 1,070 patients with PDAC, and statistical analysis confirmed its clinical effectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and it has the potential to be combined with established clinical factors to select patients at higher risk who might benefit from neoadjuvant therapy.",,,Computer Aided Diagnosis,Oncology,CT,Treatment Response and Outcome/Disease Prediction,,,,,,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function ,"The measurement of fetal thalamus diameter (FTD) and fetal head circumference (FHC) are crucial in identifying abnormal fetal thalamus development as it may lead to certain neuropsychiatric disorders in later life. However, manual measurements from 2D-US images are laborious, prone to high inter-observer variability, and complicated by the high signal-to-noise ratio nature of the images. Deep learning-based landmark detection approaches have shown promise in measuring biometrics from US images, but the current state-of-the-art (SOTA) algorithm, BiometryNet, is inadequate for FTD and FHC measurement due to its inability to account for the fuzzy edges of these structures and the complex shape of the FTD structure. To address these inadequacies, we propose a novel Swoosh Activation Function (SAF) designed to enhance the regularization of heatmaps produced by landmark detection algorithms. Our SAF serves as a regularization term to enforce an optimum mean squared error (MSE) level between predicted heatmaps, reducing the dispersiveness of hotspots in predicted heatmaps. Our experimental results demonstrate that SAF significantly improves the measurement performances of FTD and FHC with higher intraclass correlation coefficient scores in FTD and lower mean difference scores in FHC measurement than those of the current SOTA algorithm BiometryNet. Moreover, our proposed SAF is highly generalizable and architecture-agnostic. The SAF’s coefficients can be configured for different tasks, making it highly customizable. Our study demonstrates that the SAF activation function is a novel method that can improve measurement accuracy in fetal biometry landmark detection. This improvement has the potential to contribute to better fetal monitoring and improved neonatal outcomes.",https://github.com/DasuberVetLeonidas/SwooshActivationFunction,https://hc18.grand-challenge.org,Fetal Imaging,Ultrasound,,,,,,,,
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models ,"Image-based precision medicine aims to personalize treatment decisions based on an individual’s unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of the uncertainty estimate with the factual error, and, given the lack of ground truth counterfactual outcomes, demonstrate how uncertainty for the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate how knowledge of uncertainty could modify clinical decision-making to improve individual patient and clinical trial outcomes.",,,Treatment Response and Outcome/Disease Prediction,Neuroimaging - Others,Uncertainty,,,,,,,
Improving Outcome Prediction of Pulmonary Embolism by De-Biased Multi-Modality Model ,"Bias in healthcare negatively impacts marginalized populations with lower socioeconomic status and contributes to healthcare inequalities. Eliminating bias in AI models is crucial for fair and precise medical implementation. The development of a holistic approach to reducing bias aggregation in multimodal medical data and promoting equity in healthcare is highly demanded. Racial disparities exist in the presentation and development of algorithms for pulmonary embolism (PE), and deep survival prediction model can be de-biased with multimodal data. In this paper, we present a novel survival prediction (SP) framework with demographic bias disentanglement for PE. The CTPA images and clinical reports are encoded by the state-of-the-art backbones pretrained with large-scale medical-related tasks. The proposed de-biased SP modules effectively disentangle latent race-intrinsic attributes from the survival features, which provides a fair survival outcome through the survival prediction head. We evaluate our method using a multimodal PE dataset with time-to-event labels and race identifications. The comprehensive results show an effective de-biased performance of our framework on outcome predictions.",,,Treatment Response and Outcome/Disease Prediction,Computer Aided Diagnosis,Text (clinical/radiology reports),,,,,,,
Improving Pathology Localization: Multi-Series Joint Attention Takes the Lead ,"Automated magnetic resonance imaging (MRI) pathology
localization can significantly reduce inter-reader variability and the time
expert radiologists need to make a diagnosis. Many automated localization pipelines only operate on a single series at a time and are unable to capture inter-series relationships of pathology features. However, some pathologies require the joint consideration of multiple series to be accurately located in the face of highly anisotropic volumes and unique anatomies. To efficiently and accurately localize a pathology, we propose a Multi-series jOint ATtention localization framework (MOAT)
for MRI, which shares information among different MRI series to jointly predict the pathological location(s) in each MRI series. The framework allows different MRI series to share latent representations with each other allowing each series to get location guidance from the others and enforcing consistency between the predicted locations. Extensive experiments on three knee MRI pathology datasets, including medial compartment cartilage (MCC) high-grade defects, medial meniscus (MM) tear and displaced fragment/flap (DF) with 2729, 2355, and 4608 studies respectively, show that our proposed method outperforms the state of the art approaches by 3.4 to 8.0 mm on L1 distance, 6 to 27 percent on specificity and 5 to 14 percent on sensitivity across different pathologies.",,,Computer Aided Diagnosis,Attention models,MRI,,,,,,,
Incomplete Multimodal Learning for Visual Acuity Prediction after Cataract Surgery Using Masked Self-Attention ,"As the primary treatment option for cataracts, it is estimated
that millions of cataract surgeries are performed each year globally.
Predicting the Best Corrected Visual Acuity (BCVA) in cataract
patients is crucial before surgeries to avoid medical disputes. However,
accurate prediction remains a challenge in clinical practice. Traditional
methods based on patient characteristics and surgical parameters have
limited accuracy and often underestimate postoperative visual acuity. In
this paper, we propose a novel framework for predicting visual acuity
after cataract surgery using masked self-attention. Especially different
from existing methods, which are based on monomodal data, our proposed
method takes preoperative images and patient demographic data
as input to leverage multimodal information. Furthermore, we expand
our method to a more complex and challenging clinical scenario, i.e., the
incomplete multimodal data. Firstly, we apply efficient Transformers to
extract modality-specific features. Then, an attentional fusion network
is utilized to fuse the multimodal information. To address the modalitymissing
problem, an attention mask mechanism is proposed to improve
the robustness. We evaluate our method on a collected dataset of 1960
patients who underwent cataract surgery and compare its performance
with other state-of-the-art approaches. The results show that our proposed
method outperforms other methods and achieves a mean absolute
error of 0.112 logMAR. The percentages of the prediction errors within ±
0.10 logMAR are 94.3%. Besides, extensive experiments are conducted
to investigate the effectiveness of each component in predicting visual
acuity.",https://github.com/liyiersan/MSA,,Ophthalmology,Computer Aided Diagnosis,Uncertainty,other,Text (clinical/radiology reports),Ultrasound,,,,
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI ,"Deep learning (DL) models for segmenting various anatomical structures have achieved great success via a static DL model that is trained in a single source domain. Yet, the static DL model is likely to perform poorly in a continually evolving environment, requiring appropriate model updates. In a continual learning setting, we would expect that well-trained static models are updated, following continually evolving target domain data—e.g., additional lesions or structures of interest—collected from different sites, without catastrophic forgetting. This, however, poses challenges, due to distribution shifts, additional structures not seen during the initial model training, and the absence of training data in a source domain. To address these challenges, in this work, we seek to progressively evolve an ``off-the-shelf” trained segmentation model to diverse datasets with additional anatomical categories in a unified manner. Specifically, we first propose a divergence-aware dual-flow module with balanced rigidity and plasticity branches to decouple old and new tasks, which is guided by continuous batch renormalization. Then, a complementary pseudo-label training scheme with self-entropy regularized momentum MixUp decay is developed for adaptive network optimization. We evaluated our framework on a brain tumor segmentation task with continually changing target domains—i.e., new MRI scanners/modalities with incremental structures. Our proposed framework demonstrated superior segmentation performance, by efficiently learning new anatomical structures from different types of MRI data. Our framework was able to well retain the discriminability of previously learned structures, hence enabling the realistic life-long segmentation model extension along with the widespread accumulation of big medical data.",,,Continual Learning,Model Generalizability / Federated Learning,Transfer learning,MRI,,,,,,
Inflated 3D Convolution-Transformer for Weakly-supervised Carotid Stenosis Grading with Ultrasound Videos ,"Localization of the narrowest position of the vessel and corresponding vessel and remnant vessel delineation in carotid ultrasound (US) are essential for carotid stenosis grading (CSG) in clinical practice. However, the pipeline is time-consuming and tough due to the ambiguous boundaries of plaque and temporal variation. To automatize this procedure, a large number of manual delineations are usually required, which is not only laborious but also not reliable given the annotation difficulty. In this study, we present the first video classification framework for automatic CSG. Our contribution is three-fold. First, to avoid the requirement of laborious and unreliable annotation, we propose a novel and effective video classification network for weakly-supervised CSG. Second, to ease the model training, we adopt an inflation strategy for the network, where pre-trained 2D convolution weights can be adapted into the 3D counterpart in our network for an effective warm start. Third, to enhance the feature discrimination of the video, we propose a novel attention-guided multi-dimension fusion (AMDF) transformer encoder to model and integrate global dependencies within and across spatial and temporal dimensions, where two lightweight cross-dimensional attention mechanisms are designed. Our approach is extensively validated on a large clinically collected carotid US video dataset, demonstrating state-of-the-art performance compared with strong competitors.",,,Attention models,Vascular,Data Efficient Learning,Transfer learning,Ultrasound,Video,,,,
Infusing physically inspired known operators in deep models of ultrasound elastography ,"The displacement estimation step of Ultrasound Elastography (USE) can be done by optical flow Convolutional Neural Networks (CNN). Even though displacement estimation in USE and computer vision share some challenges, USE displacement estimation has two distinct characteristics that set it apart from the computer vision counterpart: high-frequency nature of RF data, and the physical rules that govern the motion pattern. The high-frequency nature of RF data has been well addressed in recent works by modifying the architecture of the available optical flow CNNs. However, insufficient attention has been placed on the integration of physical laws of deformation into the displacement estimation. In USE, lateral displacement estimation, which is highly required for elasticity and Poisson’s ratio imaging, is a more challenging task compared to the axial one since the motion in the lateral direction is limited, and the sampling frequency is much lower than the axial one. Recently, Physically Inspired ConstrainT for Unsupervised Regularized Elastography (PICTURE) has been introduced which incorporates the physical laws of deformation by introducing a regularized loss function. PICTURE tries to limit the range of the lateral displacement by the feasible range of Poisson’s ratio and the estimated high-quality axial displacement. Despite the improvement, the regularization was only applied during the training phase. Furthermore, only a feasible range for Poisson’s ratio was enforced. We exploit the concept of known operators to incorporate iterative refinement optimization methods into the network architecture so that the network is forced to remain within the physically plausible displacement manifold. The refinement optimization methods are embedded into the different pyramid levels of the network architecture to improve the estimate. Our results on experimental phantom and in vivo data show that the proposed method substantially improves the estimated displacements.",http://code.sonography.ai/,http://data.sonography.ai/,Ultrasound,Abdomen,Breast,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images ,"In pathological image analysis, determination of gland morphology in histology images of the colon is essential to determine the grade of colon cancer. However, manual segmentation of glands is extremely challenging and there is a need to develop automatic methods for segmenting gland instances. Recently, due to the powerful noise-to-image denoising pipeline, the diffusion model has become one of the hot spots in computer vision research and has been explored in the field of image segmentation. In this paper, we propose an instance segmentation method based on the diffusion model that can perform automatic gland instance segmentation. Firstly, we model the instance segmentation process for colon histology images as a denoising process based on the diffusion model. Secondly, to recover details lost during denoising, we use Instance Aware Filters and multi-scale Mask Branch to construct global mask instead of predicting only local masks. Thirdly, to improve the distinction between the object and the background, we apply Conditional Encoding to enhance the intermediate features with the original image encoding. To objectively validate the proposed method, we compared state-of-the-art deep learning model on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and the Colorectal Adenocarcinoma Gland (CRAG) dataset. The experimental results show that our method improves the accuracy of segmentation and proves the efficacy of the method.",,,Histopathology,,,,,,,,,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation ,"Deep neural networks have been widely applied in dichotomous medical image segmentation (DMIS) of many anatomical structures in several modalities, achieving promising performance. However, existing networks tend to struggle with task-specific, heavy and complex designs to improve accuracy. They made little instructions to which feature channels would be more beneficial for segmentation, and that may be why the performance and universality of these segmentation models are hindered. In this study, we propose an instructive feature enhancement approach, namely IFE, to adaptively select feature channels with rich texture cues and strong discriminability to enhance raw features based on local curvature or global information entropy criteria. Being plug-and-play and applicable for diverse DMIS tasks, IFE encourages the model to focus on texture-rich features which are especially important for the ambiguous and challenging boundary identification, simultaneously achieving simplicity, universality, and certain interpretability. To evaluate the proposed IFE, we constructed the first large-scale DMIS dataset Cosmos55k, which contains 55,023 images from 7 modalities and 26 anatomical structures. Extensive experiments show that IFE can improve the performance of classic segmentation networks across different anatomies and modalities with only slight modifications. Code is available at https://github.com/yezi-66/IFE.",https://github.com/yezi-66/IFE,,Image Segmentation,Interpretability / Explainability,Model Generalizability / Federated Learning,CT,Histopathology,MRI,,,,
Intelligent Virtual B-scan Mirror (IVBM) ,"Swept-Source Optical Coherence Tomography (SS-OCT) allows surgeons to perform certain ophthalmic procedures under the exclusive guidance of real-time volumetric optical coherence tomography (4D OCT). In such scenarios, surgeons are no longer limited to rigid views through an operating microscope. Instead, direct volume rendering (DVR) of 4D OCT enables surgical maneuvers to be performed from arbitrary viewpoints. While 4D OCT maximizes the use of the depth-resolved OCT data by displaying it from an oblique perspective, performing complex instrument maneuvers from such views places a higher mental demand on the surgeon. In this work, we propose an Intelligent Virtual B-scan Mirror (IVBM), a novel concept for surgical 4D OCT visualization to provide additional guidance for targeted instrument interactions. The IVBM integrates a virtual mirror into a selected cross-section of the OCT volume. This mirror acts intelligently by only being sensitive to voxels associated with surgical instruments. Furthermore, volume structures aligned with the IVBM are highlighted, while structures behind the IVBM are preserved through an adaptive opacity transfer function. Unlike previous perceptual OCT visualization concepts, which primarily address depth perception in axial OCT direction, this novel approach aids surgical interactions from arbitrary views. This paper presents the definition and implementation of an IVBM in a 4D OCT integrated microscope. Our user study in a virtual simulation environment confirms the benefits and provides insights into the interaction with the concept.",,,Surgical Visualization and Mixed/Augmented/Virtual Reality,Ophthalmology,Guided Interventions and Surgery,Visualization in Biomedical Imaging,,,,,,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging ,"We developed an interpretable deep biomarker known as Siamese change biomarker generation network (SCBG-Net) to evaluate the effects of therapies on carotid atherosclerosis based on the vessel wall and plaque volume and texture features extracted from three-dimensional ultrasound (3DUS) images. To the best of our knowledge, SCBG-Net is the first deep network developed for serial monitoring of carotid plaque changes. SCBG-Net automatically integrates volume and textural features extracted from 3DUS to generate a change biomarker called AutoVT (standing for Automatic integration of Volume and Textural features) that is sensitive to dietary treatments. The proposed AutoVT improves the cost-effectiveness of clinical trials required to establish the benefit of novel treatments, thereby decreasing the period that new anti-atherosclerotic treatments are withheld from patients needing them. To facilitate the interpretation of AutoVT, we developed an algorithm to generate change biomarker activation maps (CBAM) localizing regions having an important effect on AutoVT. The ability to visualize locations with prominent plaque progression/regression afforded by CBAM improves the interpretability of the proposed deep biomarker. Improvement in interpretability would allow the deep biomarker to gain sufficient trust from clinicians for them to incorporate the model into clinical workflow.",,,Treatment Response and Outcome/Disease Prediction,Vascular,Computer Aided Diagnosis,Interpretability / Explainability,Ultrasound,,,,,
Interpretable Medical Image Classification using Prototype Learning and Privileged Information ,"Interpretability is often an essential requirement in medical imaging. Advanced deep learning methods are required to address this need for explainability and high performance. In this work, we investigate whether additional information available during the training process can be used to create an understandable and powerful model. We propose an innovative solution called Proto-Caps that leverages the benefits of capsule networks, prototype learning and the use of privileged information. Evaluating the proposed solution on the LIDC-IDRI dataset shows that it combines increased interpretability with above state-of-the-art prediction performance. Compared to the explainable baseline model, our method achieves more than 6 % higher accuracy in predicting both malignancy (93.0 %) and mean characteristic features of lung nodules. Simultaneously, the model provides case-based reasoning with prototype representations that allow visual validation of radiologist-defined attributes.",https://github.com/XRad-Ulm/Proto-Caps,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966254,Interpretability / Explainability,Lung,Computer Aided Diagnosis,CT,,,,,,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising using Boosted Contrastive Learning ,"The low-dose computed tomography (LDCT) denoising field is primarily dominated by supervised learning-based approaches, which necessitate the accurate pairing of LDCT and corresponding clean reference images (NDCT). However, obtaining real-world paired data is not feasible. Obtaining unpaired LDCT and NDCT data, on the other hand, is easy to do. Unsupervised methods have become increasingly popular for LDCT denoising. One commonly used method is CycleGAN,but it is both memory-intensive and challenging to train without the risk of collapse. To address these limitations, we propose a novel unsupervised method based on boosted contrastive learning (BCL), which requires only a single generator. Additionally, due to limitations in computing capability and memory size, most existing methods only focus on single slice, resulting in unstable results between slices. Our BCL-based method overcomes this problem. Our method astonishingly lows computing resources used and makes modifications to the original contrastive learning method, including weight optimization for positive-negative pairs and constraint of difference invariants. Our experiments demonstrate that our method outperforms existing state-of-the-art supervised and unsupervised methods in both qualitative and quantitative measures. Importantly, our framework does not require paired training data and is more adaptable for clinical application.",,https://www.aapm.org/grandchallenge/lowdosect/,CT,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Intraoperative CT augmentation for needle-based liver interventions ,"This paper addresses the need for improved CT-guidance during needle-based liver procedures (i.e., tumor ablation), while reduces the need for contrast agent injection during such interventions. To achieve this objective, we augment the intraoperative CT with the preoperative vascular network deformed to match the current acquisition. First, a neural network learns local image features in a non-contrasted CT image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching contrasted CT image. Then, the augmented CT is generated by fusing the labeled vascular tree and the non-contrasted intraoperative CT. Our method is trained and validated on porcine data, achieving an average dice score of 0.81 on the predicted vessel tree instead of 0.51 when a medical expert segments the non-contrasted CT. In addition, vascular labels can also be transferred to provide additional information. Code will be publicly released.",https://github.com/Sidaty1/Intraoperative_CT_augmentation,,Guided Interventions and Surgery,Abdomen,Vascular,Image Reconstruction,Interventional Imaging Systems,CT,Visualization in Biomedical Imaging,,,
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields ,"Minimally invasive spine surgery such as anterior vertebral tethering (AVT), enables the treatment of spinal deformities while seeking to preserve lower back mobility. However the intra-operative positioning and posture of the spine affects surgical outcomes. Forecasting the standing shape from adolescent patients with growing spines remains challenging with many factors influencing corrective spine surgery, but can allow spine surgeons to better prepare and position the spine prior to surgery. We propose a novel intra-operative framework anticipating the standing posture of the spine immediately after surgery from patients with idiopathic scoliosis. The method is based on implicit neural representations, which uses a backbone network to train kernels based on neural splines and estimate network parameters from intra-pose data, by regressing the standing shape on-the-fly using a simple positive definite linear system. To accommodate with the variance in spine appearance, we use a Signed Distance Function for articulated structures (A-SDF) to capture the articulation vectors in a disentangled latent space, using distinct encoding vectors to represent both shape and articulation parameters. The network’s loss function incorporates a term regularizing outputs from a pre-trained population growth trajectory to ensure transformations are smooth with respect to the variations seen on first-erect exams. The model was trained on 735 3D spine models and tested on a separate set of 81 patients using pre- and intra-operative models used as inputs. The neural kernel field framework forecasted standing shape outcomes with a mean average error of 1.6 +/- 0.6mm in vertebral points, and generated shapes with IoU scores of 94.0 compared to follow-up models.",,,Surgical Planning and Simulation,Musculoskeletal,Attention models,Other,CT,,,,,
Inverse Consistency by Construction for Multistep Deep Registration ,"Inverse consistency is a desirable property for image registration. We propose a simple technique to make a neural registration network inverse consistent by construction, as a consequence of its structure, as long as it parameterizes its output transform by a Lie group. We extend this technique to multi-step neural registration by composing many such networks in a way that preserves inverse consistency. This multi-step approach also allows for inverse-consistent coarse to fine registration. We evaluate our technique on synthetic 2-D data and four 3-D
medical image registration tasks and obtain excellent registration accuracy while assuring inverse consistency.",https://github.com/uncbiag/ByConstructionICON,,Image Registration,Lung,Musculoskeletal,Neuroimaging - Others,CT,MRI,,,,
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model ,"High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) from [21] trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder D of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder D, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction. Our source code is available online: https://github.com/BioMedAI-UCSC/InverseSR.",https://github.com/BioMedAI-UCSC/InverseSR,https://brain-development.org/ixi-dataset/,Image Reconstruction,MRI,,,,,,,,
Ischemic stroke segmentation from a cross-domain representation in multimodal diffusion studies ,"Localization and delineation of ischemic stroke are crucial for diagnosis and prognosis. Diffusion-weighted MRI studies allow to associate hypoperfused brain tissue with stroke findings, observed from ADC and DWI parameters. However, this process is expensive, time-consuming, and prone to expert observational bias. To address these challenges, currently, deep representations are based on deep autoencoder representations but are limited to learning from only ADC observations, biased also for one expert delineation. This work introduces a multimodal and multi-segmentation deep autoencoder that recovers ADC and DWI stroke segmentations. The proposed approach learns independent ADC and DWI convolutional branches, which are further fused into an embedding representation. Then, decoder branches are enriched with cross-attention mechanisms and adjusted from ADC and DWI findings. In this study, we validated the proposed approach from 82 ADC and DWI sequences, annotated by two interventional neuroradiologists. The proposed approach achieved higher mean dice scores of 55.7\% and 57.7\% for the ADC and DWI annotations by the training reference radiologist, outperforming models that only learn from one modality. Notably, it also demonstrated a proper generalization capability, obtaining mean dice scores of 60.5\% and 61.0\% for the ADC and DWI annotations of a second radiologist. This study highlights the effectiveness of modality-specific pattern learning in producing cross-domain embeddings that enhance ischemic stroke lesion estimations and generalize well over annotations by other radiologists.",https://gitlab.com/bivl2ab/research/2023-cross-domain-stroke-segmentation,,Image Segmentation,Attention models,MRI,,,,,,,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification ,"Whole Slide Image (WSI) classification remains a challenge due to their extremely high resolution and the absence of fine-grained labels. Presently, WSI classification is usually regarded as a Multiple Instance Learning (MIL) problem when only slide-level labels are available. MIL methods involve a patch embedding module and a bag-level classification module, but they are prohibitively expensive to be trained in an end-to-end manner. Therefore, existing methods usually train them separately, or directly skip the training of the embedder. Such schemes hinder the patch embedder’s access to slide-level semantic labels, resulting in inconsistency within the entire MIL pipeline. To overcome this issue, we propose a novel framework called Iteratively Coupled MIL (ICMIL), which bridges the loss back-propagation process from the bag-level classifier to the patch embedder. In ICMIL, we use category information in the bag-level classifier to guide the patch-level fine-tuning of the patch feature extractor. The refined embedder then generates better instance representations for achieving a more accurate bag-level classifier. By coupling the patch embedder and bag classifier at a low cost, our proposed framework enables information exchange between the two modules, benefiting the entire MIL classification model. We tested our framework on two datasets using three different backbones, and our experimental results demonstrate consistent performance improvements over state-of-the-art MIL methods.",https://github.com/Dootmaan/ICMIL,https://camelyon17.grand-challenge.org/Data/,Computational (Integrative) Pathology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,,,,
JCCS-PFGM: A Novel Circle-Supervision based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition ,"Multiphase contrast-enhanced computed tomography (CECT) scan is clinically significant to demonstrate the anatomy at different phases. But such multiphase scans inherently lead to the accumulation of huge radiation dose for patients, and directly reducing the scanning dose dramatically decrease the readability of the imaging. Therefore, guided with Joint Condition, a novel Circle-Supervision based Poisson Flow Generative Model (JCCS-PFGM) is proposed to promote the progressive low-dose reconstruction for multiphase CECT. JCCS-PFGM is constituted by three special designs: 1) a progressive low-dose reconstruction mechanism to leverages the imaging consistency and radiocontrast evolution along former-latter phases, so that enormously reduces the radiation dose needs and improve the reconstruction effect, even for the latter-phase scanning with extremely low dose; 2) a circle-supervision strategy embedded in PFGM to enhance the refactoring capabilities of normalized poisson field learned from the perturbed space to the specified CT image space, so that boosts the explicit reconstruction for noise reduction; 3) a joint condition to explore correlation between former phases and current phase, so that extracts the complementary information for current noisy CECT and guides the reverse process of diffusion jointly with multiphase condition for structure maintenance. The extensive experiments tested on the clinical dataset composed of 11436 images show that our JCCS-PFGM achieves promising PSNR up to 46.3dB, SSIM up to 98.5%, and MAE down to 9.67 HU averagely on phases I, II and III, in quantitative evaluations, as well as gains high-quality readable visualizations in qualitative assessments. All of these findings reveal our method a great potential in clinical multiphase CECT imaging.",,,CT,Image Reconstruction,,,,,,,,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation ,"We present a novel methodology that combines graph and
dense segmentation techniques by jointly learning both point and pixel
contour representations, thereby leveraging the benefits of each approach.
This addresses deficiencies in typical graph segmentation methods where
misaligned objectives restrict the network from learning discriminative
vertex and contour features. Our joint learning strategy allows for rich
and diverse semantic features to be encoded, while alleviating common
contour stability issues in dense-based approaches, where pixel-level ob-
jectives can lead to anatomically implausible topologies. In addition,
we identify scenarios where correct predictions that fall on the contour
boundary are penalised and address this with a novel hybrid contour
distance loss. Our approach is validated on several Chest X-ray datasets,
demonstrating clear improvements in segmentation stability and accu-
racy against a variety of dense- and point-based methods. Our source
code is freely available at: www.github.com/kitbransby/Joint_Graph_Segmentation",https://github.com/kitbransby/Joint_Graph_Segmentation,http://db.jsrt.or.jp/eng.php,Image Segmentation,Other,other,,,,,,,
Joint optimization of a β-VAE for ECG task-specific feature extraction ,"Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function.
In this work we study the use of β-variational autoencoders (VAEs) as
an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function
prediction. The extracted features are then used for cardiac function
prediction using logistic regression. The method is trained and tested
on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021.
The results show that our method significantly improved prediction and
explainability compared to a vanilla β-VAE, while still yielding similar
reconstruction performance.",https://github.com/ViktorvdValk/Task-Specific-VAE,,Interpretability / Explainability,Computer Aided Diagnosis,Image Reconstruction,EEG/ECG,Visualization in Biomedical Imaging,,,,,
"Joint prediction of response to therapy, molecular traits, and spatial organisation in colorectal cancer biopsies ","Existing methods for interpretability of model predictions are largely based on technical insights and are not linked to clinical context. We use the question of predicting response to radiotherapy in colorectal cancer patients as an exemplar for developing prediction models that do provide such contextual information and therefore can effectively support clinical decision making. There is a growing body of evidence that about 30% of colorectal cancer patients do not respond to radiotherapy and will need alternative treatment. The consensus molecular subtypes for colorectal cancer (CMS) provide one such approach to categorising patients based on their disease biology. Here we select the CMS4 subtype as a proxy for stromal infiltration. By jointly predicting a patient’s response to radiotherapy, the presence of CMS4, and the epithelial tissue map from morphological features extracted from standard H&E slides we provide a comprehensive clinically relevant assessment of a biopsy. A graph neural network is trained to achieve this joint prediction task, which subsequently provides novel interpretability maps to aid clinicians in their cancer treatment decision making process. Our model is trained and validated on two private rectal cancer datasets.",,,Treatment Response and Outcome/Disease Prediction,Visualization in Biomedical Imaging,,,,,,,,
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark ,"The 3-hinge gyral folding is the conjunction of gyrus crest lines from three different orientations, which has been proved to be unique anatomically, structurally, and functionally connective patterns. Compared with the normal gyri, the 3-hinge gyri has stronger structural connectivity and participates in more functional networks, which plays an important role in brain function and structure. However, for the large differences of brain across subjects, it is difficult to identify consistent 3-hinge regions across subjects and most previous studies on 3-hinge consistency merely focused on a single mode. In order to study the multi-modal consistency of 3-hinge regions, this paper proposes a joint representation of functional and structural profiles for identifying common and consistent 3-hinge. We use representation of 3-hinge participation in the functional network to obtain the functional consistency of 3-hinge cross subjects, and the distance between 3-hinge region and DICCCOL system to obtain the structural consistency. Combining these two sets of stability, 38 functionally and structurally consistent 3-hinge regions were successfully identified cross subjects. These consistent 3-hinge regions based on multi-modal data are more consistent than that merely based on structural data and experimental results elucidate those consistent 3-hinge regions are more correlated with visual function. This work deepens the understanding of the stability of 3-hinge region and provides basis for further inter-group analysis of 3-hinge gyral folding.",,,MRI,Neuroimaging - Brain Development,Neuroimaging - DWI and Tractography,Neuroimaging - Functional Brain Networks,,,,,,
Joint Segmentation and Sub-Pixel Localization in Structured Light Laryngoscopy ,"In recent years, phoniatric diagnostics has seen a surge of interest in structured light-based high-speed video endoscopy, as it enables the observation of oscillating human vocal folds in vertical direction. However, structured light laryngoscopy suffers from practical problems: specular reflections interfere with the projected pattern, mucosal tissue dilates the pattern, and lastly the algorithms need to deal with huge amounts of data generated by a high-speed video camera. To address these issues, we propose a neural approach for the joint semantic segmentation and keypoint detection in structured light high-speed video endoscopy that improves the robustness, accuracy, and performance of current human vocal fold reconstruction pipelines. Major contributions are the reformulation of one channel of a semantic segmentation approach as a single-channel heatmap regression problem, and the prediction of sub-pixel accurate 2D point locations through weighted least squares in a fully-differentiable manner with negligible computational cost. Lastly, we expand the publicly available Human Laser Endoscopic dataset to also include segmentations of the human vocal folds itself. The source code and dataset are available at: https://github.com/Henningson/SSSLsquared",https://github.com/Henningson/SSSLsquared,https://github.com/Henningson/HLEDataset,Computer Aided Diagnosis,Computational Anatomy and Physiology,Image Segmentation,Other,other,Video,,,,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training ,"The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowledge. Extensive experiments validate the effect of our framework on eight tasks including classification, segmentation, retrieval, and semantic relatedness, achieving comparable or better performance with the zero-shot or few-shot settings. Our code will be open-sourced. Our code is open on https://github.com/ChenXiaoFei-CS/KoBo.",https://github.com/ChenXiaoFei-CS/KoBo,stanfordmlgroup.github.io,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Text (clinical/radiology reports),,,,,,,
L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space ,"The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite kernel function to attain rich representation. Afterward, we optimize the L3 model by minimizing the discrepancies between the new sample representation and the subspace constructed using the old representation in RKHS. L3DMC is capable of adapting new knowledge better without forgetting old knowledge as it combines the representation power of multiple fixed-curvature spaces and is performed on higher-dimensional RKHS. Thorough experiments on three benchmarks demonstrate the effectiveness of our proposed distillation strategy for medical image classification in L3 settings.",,,Continual Learning,Other,,,,,,,,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity ,"In computational pathology, nuclei segmentation from histology images is a fundamental task. While deep learning based nuclei segmentation methods yield excellent results, they rely on a large amount of annotated images; however, annotating nuclei from histology images is tedious and time-consuming. To get rid of labeling burden completely, we propose a label-free approach for nuclei segmentation, motivated from one pronounced yet omitted property that characterizes histology images and nuclei: intra-image self similarity (IISS), that is, within an image, nuclei are similar in their shapes and appearances. First, we
leverage traditional machine learning and image processing techniques to generate a pseudo segmentation map, whose connected components form candidate nuclei, both positive or negative. In particular, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. Then, we filter the candidates based on a custom-designed index that roughly measures if a candidate contains multiple nuclei. The remaining candidates are used as pseudo labels, which we use to train a U-Net to discover the hierarchical features distinguish nuclei pixels from background. Finally, we apply the learned U-Net to produce final nuclei segmentation. We validate the proposed method on the public dataset MoNuSeg. Experimental results demonstrate the effectiveness of our design and, to the best of our knowledge, it achieves the state-of-the-art performances of label-free segmentation on the benchmark MoNuSeg dataset with a mean Dice score of 79.2%.",,https://monuseg.grand-challenge.org/Data/,Histopathology,Image Segmentation,,,,,,,,
Label-preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition ,"AI based methods have achieved considerable performance in screening for common retinal diseases using fundus images, particularly in the detection of Diabetic Retinopathy (DR). However, these methods rely heavily on large amounts of data, which is challenging to obtain due to limited access to medical data that complies with  medical data protection legislation. One of the crucial aspects to improve performance of the AI model is using data augmentation strategy on public datasets. However, standard data augmentation  cannot always guarantee the  clinical labels in diabetic retinopathy. This paper presents a label-preserving data augmentation method for DR detection using latent space manipulation. The proposed approach involves computing the contribution score of each latent code to the lesions in DR images, and  manipulating the lesion in DR images based on the latent code with the highest contribution score. This allows for a more targeted and effective label-preserving data augmentation approach for DR detection tasks, which is especially useful given the imbalanced classes and limited available data. The experiments in our study include two tasks, DR classification and DR severity levels grading, with 4K and 2K labeled images in training sets, respectively. The results of our experiments demonstrate that our data augmentation method was able to achieve a 6\% increase in accuracy for the DR classification task, and a 4\% increase in accuracy for the DR severity levels grading task without any further optimization of the model architectures.",https://github.com/AIEyeSystem/LpDA,https://www.kaggle.com/competitions/aptos2019-blindness-detection,Other,Ophthalmology,Computer Aided Diagnosis,Image Segmentation,Interpretability / Explainability,Transfer learning,,,,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms ,"Modern surgeries are performed in complex and dynamic settings, including ever-changing interactions between medical staff, patients, and equipment. The holistic modeling of the operating room (OR) is, therefore, a challenging but essential task, with the potential to optimize the performance of surgical teams and aid in developing new surgical technologies to improve patient outcomes. The holistic representation of surgical scenes as semantic scene graphs (SGG), where entities are represented as nodes and relations between them as edges, is a promising direction for fine-grained semantic OR understanding. We propose, for the first time, the use of temporal information for more accurate and consistent holistic OR modeling. Specifically, we introduce memory scene graphs, where the scene graphs of previous time steps act as the temporal representation guiding the current prediction. We design an end-to-end architecture that intelligently fuses the temporal information of our lightweight memory scene graphs with the visual information from point clouds and images. We evaluate our method on the 4D-OR dataset and demonstrate that integrating temporality leads to more accurate and consistent results achieving an +5% increase and a new SOTA of 0.88 in macro F1. This work opens the path for representing the entire surgery history with memory scene graphs and improves the holistic understanding in the OR. Introducing scene graphs as memory representations can offer a valuable tool for many temporal understanding tasks. We will publish our code upon acceptance.",https://github.com/egeozsoy/LABRAD-OR,https://github.com/egeozsoy/4D-OR,Surgical Scene Understanding,Surgical Data Science,Surgical Skill and Work Flow Analysis,,,,,,,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection ,"Vision Transformer (ViT) models have demonstrated a breakthrough in a wide range of computer vision tasks. However, compared to the Convolutional Neural Network (CNN) models, it has been observed that the ViT models struggle to capture high-frequency components of images, which can limit their ability to detect local textures and edge information. As abnormalities in human tissue, such as tumors and lesions, may greatly vary in structure, texture, and shape, high-frequency information such as texture is crucial for effective semantic segmentation tasks. To address this limitation in ViT models, we propose a new technique, Laplacian-Former, that enhances the self-attention map by adaptively re-calibrating the frequency information in a Laplacian pyramid. More specifically, our proposed method utilizes a dual attention mechanism via efficient attention and frequency attention while the efficient attention mechanism reduces the complexity of self-attention to linear while producing the same output, selectively intensifies the contribution of shape and texture features. Furthermore, we introduce a novel efficient enhancement multi-scale bridge that effectively transfers spatial information from the encoder to the decoder while preserving the fundamental features. We demonstrate the efficacy of Laplacian-former on multi-organ and skin lesion segmentation tasks with +1.87\% and +0.76\% dice scores compared to SOTA approaches, respectively.",https://github.com/mindflow-institue/Laplacian-Former,,Image Segmentation,Attention models,CT,,,,,,,
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality ,"The problem of missing modalities is both critical and non-trivial to be handled in multi-modal models. It is common for multi-modal tasks that certain modalities contribute more compared to other modalities, and if those important modalities are missing, the model performance drops significantly. Such fact remains unexplored by current multi-modal approaches that recover the representation from missing modalities by feature reconstruction or blind feature aggregation from other modalities, instead of extracting useful information from the best performing modalities. In this paper, we propose a Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively identify important modalities and distil knowledge from them to help other modalities from the cross-modal perspective for solving the missing modality issue. Our approach introduces a teacher election procedure to select the most “qualified” teachers based on their single modality performance on certain tasks. Then, cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks. Hence, even if the teacher modalities for certain tasks are missing during testing, the available student modalities can accomplish the task well enough based on the learned knowledge from their automatically elected teacher modalities. Experiments on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods by a considerable margin, improving the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score.",,,Image Segmentation,MRI,,,,,,,,
Learnable Query Initialization for Surgical Instrument Instance Segmentation ,"Surgical tool classification and instance segmentation are crucial for minimally invasive surgeries and related applications. Though most of the state-of-the-art for instance segmentation in natural images use transformer-based architectures, they have not been successful for medical instruments. In this paper, we investigate the reasons for the failure. Our analysis reveals that this is due to incorrect query initialization, which is unsuitable for fine-grained classification of highly occluded objects in a low data setting, typical for medical instruments. We propose a class-agnostic Query Proposal Network (QPN) to improve query initialization inputted to the decoder layers. Towards this, we propose a deformable-cross-attention-based learnable Query Proposal Decoder (QPD). The proposed QPN improves the recall rate of the query initialization by 44.89% at 0.9 IOU. This leads to an improvement in segmentation performance by 1.84% on Endovis17 and 2.09% on Endovis18 datasets, as measured by ISI-IOU. The source code can be accessed at https://aineurosurgery.github.io/learnableQPD.",https://github.com/AINeurosurgery/Learnable-QPD-for-maskDINO,,Image Segmentation,Surgical Data Science,,,,,,,,
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis ,"Different functional configurations of the brain, also named as “brain states”, reflect a continuous stream of brain cognitive activities. These distinct brain states can confer heterogeneous functions to brain networks. Recent studies have revealed that extracting information from functional brain networks is beneficial for neuroscience analysis and brain disorder diagnosis. Graph neural networks (GNNs) have been demonstrated to be superior in learning network representations. However, these GNN-based methods have few concerns about the heterogeneity of brain networks, especially the heterogeneous information of brain network functions induced by intrinsic brain states. To address this issue, we propose a learnable subdivision graph neural network (LSGNN) for brain network analysis. The core idea of LSGNN is to implement a learnable subdivision method to encode brain networks into multiple latent feature subspaces corresponding to functional configurations, and realize the feature extraction of brain networks in each subspace, respectively. Furthermore, considering the complex interactions among brain states, we also employ the self-attention mechanism to acquire a comprehensive brain network representation in a joint latent space. We conduct experiments on a publicly available dataset of cognitive disorders. The results affirm that our approach can achieve outstanding performance and also instill the interpretability of the brain network functions in the latent space.",https://github.com/haijunkenan/LSGNN,https://adni.loni.usc.edu/,Neuroimaging - Functional Brain Networks,Attention models,Interpretability / Explainability,,,,,,,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction ,"We propose a novel Learned Alternating Minimization Algorithm (LAMA) for dual-domain sparse-view CT image reconstruction. LAMA is naturally induced by a variational model for CT reconstruction with learnable nonsmooth nonconvex regularizers, which are parameterized as composite functions of deep networks in both image and sinogram domains. To minimize the objective of the model, we incorporate the smoothing technique and residual learning architecture into the design of LAMA. We show that LAMA substantially reduces network complexity, improves memory efficiency and reconstruction accuracy, and is provably convergent for reliable reconstructions. Extensive numerical experiments demonstrate that LAMA outperforms existing methods by a wide margin on multiple benchmark CT data sets.",https://github.com/chrisdcs/LAMA-Learned-Alternating-Minimization-Algorithm,,Image Reconstruction,Interpretability / Explainability,Model Generalizability / Federated Learning,CT,,,,,,
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis ,"Construction and analysis of functional brain network (FBN) with rs-fMRI is a promising method to diagnose functional brain diseases. Traditional methods usually construct FBNs at the individual level for feature extraction and classification. There are several issues with these approaches. Firstly, due to the unpredictable interferences of noises and artifacts in rs-fMRI, these individual-level FBNs have large variability, leading to instability and unsatisfactory diagnosis accuracy. Secondly, the construction and analysis of FBNs are conducted in two successive steps without negotiation with or joint alignment for the target task. In this case, the two steps may not cooperate well. To address these issues, we propose to learn common and individual FBNs adaptively within the Transformer framework. The common FBN is shared, and it would regularize the FBN construction as prior knowledge, alleviating the variability and enabling the network to focus on these disease-specific individual functional connectivities (FCs). Both the common and individual FBNs are built by specially designed modules, whose parameters are jointly optimized with the rest of the network for FBN analysis in an end-to-end manner, improving the flexibility and discriminability of the model. Another limitation of the current methods is that the FCs are only measured with synchronous rs-fMRI signals of brain regions and ignore their possible asynchronous functional interactions. To better capture the actual FCs, the rs-fMRI signals are divided into short segments to enable modeling cross-spatiotemporal interactions. The superior performance of the proposed method is consistently demonstrated in early AD diagnosis tasks on ADNI2 and ADNI3 data sets.",https://github.com/seuzjj/ACIFBN,https://adni.loni.usc.edu/,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Attention models,Interpretability / Explainability,Model Generalizability / Federated Learning,Treatment Response and Outcome/Disease Prediction,,,,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction ,"Sparse-view cone-beam CT (CBCT) reconstruction is an important direction to reduce radiation dose and benefit clinical applications. Previous voxel-based generation methods represent the CT as discrete voxels, resulting in high memory requirements and limited spatial resolution due to the use of 3D decoders. In this paper, we formulate the CT volume as a continuous intensity field and develop a novel DIF-Net to perform high-quality CBCT reconstruction from extremely sparse (≤10) projection views at an ultrafast speed. The intensity field of a CT can be regarded as a continuous function of 3D spatial points. Therefore, the reconstruction can be reformulated as regressing the intensity value of an arbitrary 3D point from given sparse projections. Specifically, for a point, DIF-Net extracts its view-specific features from different 2D projection views. These features are subsequently aggregated by a fusion module for intensity estimation. Notably, thousands of points can be processed in parallel to improve efficiency during training and testing. In practice, we collect a knee CBCT dataset to train and evaluate DIF-Net. Extensive experiments show that our approach can reconstruct CBCT with high image quality and high spatial resolution from extremely sparse projection views within 1.6 seconds, significantly outperforming state-of-the-art methods.",https://github.com/xmed-lab/DIF-Net,,Image Reconstruction,CT,,,,,,,,
Learning Expected Appearances for Intraoperative Registration during Neurosurgery ,"We present a novel method for intraoperative patient-to-image registration by learning Expected Appearances. Our method uses patient-specific preoperative imaging to synthesize expected views through a surgical microscope for a predicted range of transformations. Our method estimates the camera pose by minimizing the dissimilarity between the intraoperative 2D view through the optical microscope and the synthesized expected textures. In contrast to conventional methods, our approach transfers the processing tasks to the preoperative stage, reducing thereby the impact of low-resolution, distorted, and noisy intraoperative images, that often degrade the registration accuracy. We applied our method in the context of neuronavigation during brain surgery. We evaluated our approach on synthetic data and on retrospective data from 6 clinical cases. Our method outperformed state-of-the-art methods and achieved accuracies that met current clinical standards",https://github.com/rouge1616/ExApp/,https://drive.google.com/drive/u/2/folders/1T2NS_BftaxE6yYZj3I1LdspuqNKwSyCl,Guided Interventions and Surgery,Video,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,,,,
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis ,"Fueled by deep learning, computer-aided diagnosis achieves huge advances. However, out of controlled lab environments, algorithms could face multiple challenges. Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing. In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases. OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis. To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns. Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scaling factor to strengthen the generalization capacity. The latter, called Open-Space Suppression (OSS), opens the classifier by recognizing sparse embedding space as unknowns using proposed feature space descriptors. Besides, since medical OSR is still a nascent field, two publicly available benchmark datasets are proposed for comparison. Extensive ablation studies and feature visualization demonstrate the effectiveness of each proposed design. Compared with recent state-of-theart methods, MLAS achieves superior performances, measured by ACC, AUROC, and OSCR.",,,Computer Aided Diagnosis,Other,Microscopy,other,,,,,,
Learning normal asymmetry representations for homologous brain structures ,"Although normal homologous brain structures are approximately symmetrical by definition, they also have shape differences due to e.g. natural ageing. On the other hand, neurodegenerative conditions induce their own changes in this asymmetry, making them more pronounced or altering their location. Identifying when these alterations are due to a pathological deterioration is still challenging. Current clinical tools rely either on subjective evaluations, basic volume measurements or disease-specific deep learning models. This paper introduces a novel method to learn normal asymmetry patterns in homologous brain structures based on anomaly detection and representation learning. Our framework uses a Siamese architecture to map 3D segmentations of left and right hemispherical sides of a brain structure to a normal asymmetry embedding space, learned using a support vector data description objective. Being trained using healthy samples only, it can quantify deviations-from-normal-asymmetry patterns in unseen samples by measuring the distance of their embeddings to the center of the learned normal space. We demonstrate in public and in-house sets that our method can accurately characterize normal asymmetries and detect pathological alterations due to Alzheimer’s disease and hippocampal sclerosis, even though no diseased cases were accessed for training. Our source code is available at https://github.com/duiliod/DeepNORHA.",,,Neuroimaging - Others,Other,,,,,,,,
Learning Ontology-based Hierarchical Structural Relationship for Whole Brain Segmentation ,"Whole brain segmentation is vital for a variety of anatomical investigations in brain development, aging, and degradation. It is nevertheless challenging to accurately segment fine-grained brain structures due to the low soft-tissue contrast. In this work, we propose and validate a novel method for whole brain segmentation. By learning ontology-based hierarchical structural knowledge with a triplet loss enhanced by graph-based dynamic violate margin, our method can mimic experts’ hierarchical perception of the brain anatomy and capture the relationship across different structures. We evaluate the whole brain segmentation performance of our method on two publicly-accessible datasets, namely JHU Adult Atlas and CANDI, respectively possessing fine-grained (282) and coarse-grained (32) manual labels. Our method achieves mean Dice similarity coefficients of 83.67% and 88.23% on the two datasets. Quantitative and qualitative results identify the superiority of the proposed method over representative state-of-the-art whole brain segmentation approaches. The code is available at https://github.com/CRazorback/OHSR.",https://github.com/CRazorback/OHSR,,Image Segmentation,Other,MRI,,,,,,,
Learning Reliability of Multi-Modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models ,"Denoising diffusion probabilistic models (DDPM) for medical image segmentation are still a challenging task due to the lack of the ability to parse the reliability of multi-modality medical images. In this paper, we propose a novel evidence-identified DDPM (EI-DDPM) with contextual discounting for tumor segmentation by integrating multi-modality medical images. Advanced compared to previous work, the EI-DDPM deploys the DDPM-based framework for segmentation tasks under the condition of multi-modality medical images and parses the reliability of multi-modality medical images through contextual discounted evidence theory. We apply EI-DDPM on a BraTS 2021 dataset with 1251 subjects and a liver MRI dataset with 238 subjects. The extensive experiment proved the superiority of EI-DDPM, which outperforms the state-of-the-art methods.",,,Image Segmentation,MRI,,,,,,,,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk ,"In medical image analysis, imbalanced noisy dataset classification poses a long-standing and critical problem since clinical large-scale datasets often attain noisy labels and imbalanced distributions through annotation and collection. Current approaches addressing noisy labels and long-tailed distributions separately may negatively impact real-world practices. Additionally, the factor of class hardness hindering label noise removal remains undiscovered, causing a critical necessity for an approach to enhance the classification performance of noisy imbalanced medical datasets with various class hardness. To address this paradox, we propose a robust classifier that trains on a multi-stage noise removal framework, which jointly rectifies the adverse effects of label noise, imbalanced distribution, and class hardness. The proposed noise removal framework consists of multiple phases. Multi-Environment Risk Minimization (MER) strategy captures data-to-label causal features for noise identification, and the Rescaling Class-aware Gaussian Mixture Modeling (RCGM) learns class-invariant detection mappings for noise removal. Extensive experiments on two imbalanced noisy clinical datasets demonstrate the capability and potential of our method for boosting the performance of medical image classification.",,https://www.nature.com/articles/sdata2018161,Computer Aided Diagnosis,,,,,,,,,
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation ,"Obtaining labelled data in medical image segmentation is challenging due to the need for pixel-level annotations by experts. Recent works have shown that augmenting the object of interest with deformable transformations can help mitigate this challenge. However, these transformations have been learned globally for the image, limiting their transferability across datasets or applicability in problems where image alignment is difficult. While object-centric augmentations provide a great opportunity to overcome these issues, existing works are only focused on position and random transformations without considering shape variations of the objects. To this end, we propose a novel object-centric data augmentation model that is able to learn the shape variations for the objects of interest and augment the object in place without modifying the rest of the image. We demonstrated its effectiveness in improving kidney tumour segmentation when leveraging shape variations learned both from within the same dataset and transferred from external datasets.",https://github.com/nileshkumar0726/Learning_Transformations,https://competitions.codalab.org/competitions/17094,Data Efficient Learning,Image Registration,Image Segmentation,Model Generalizability / Federated Learning,Transfer learning,CT,,,,
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-Site Structural MRI ,"Construct a generalizable model for the diagnosis of Alzheimer’s disease (AD) is an important task in medical imaging. While deep neural networks have recently advanced classification performance for various diseases using structural magnetic resonance imaging (sMRI), existing methods often provide suboptimal and untrustworthy results because they do not incorporate domain-knowledge and global context information. Additionally, most state-of-the-art deep learning methods rely on multi-stage preprocessing pipelines, which are inefficient and prone to errors. In this paper, we propose a novel domain-knowledge-constrained neural network for automatic diagnosis of AD using multi-center sMRI. Specifically, we incorporate domain-knowledge into a ResNet-like architecture. We explicitly enforce the network to learn domain invariant and domain specific features by jointly training multiple weighted classifiers, so that pixel-wise predictive performance generalizes to unseen images. In addition, the network directly takes segmentation-free and patch-free images in original resolution as input, which offers accurate inference with global context information and accurate individualized abnormalities to further refines reproducible predictions. The framework was evaluated on a set of sMRI collected from 7 independent centers. The proposed approach identifies important discriminative brain abnormalities associated with AD. Experimental results demonstrate superior performance of our method compared to state-of-the-art methods.",https://github.com/Yanjie-Z/DomainKnowledge4AD,,Computer Aided Diagnosis,Neuroimaging - Others,Computational Anatomy and Physiology,Image Registration,Image Segmentation,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images ,"Deep neural networks have recently achieved impressive performance of automated tumor/lesion quantification with positron emission tomography (PET) imaging. However, deep learning usually requires a large amount of diverse training data, which is difficult for some applications such as neuroendocrine tumor (NET) image quantification, because of low incidence of the disease and expensive annotation of PET data. In addition, current deep lesion detection models often suffer from performance degradation when applied to PET images acquired with different scanners or protocols. In this paper, we propose a novel single-source domain generalization method, which learns with human annotation-free, list mode-synthesized PET images, for hepatic lesion identification in real-world clinical PET data. We first design a specific data augmentation module to generate out-of-domain images from the synthesized data, and incorporate it into a deep neural network for cross domain-consistent feature encoding. Then, we introduce a novel patch-based gradient reversal mechanism and explicitly encourage the network to learn domain-invariant features. We evaluate the proposed method on multiple cross-scanner 68Ga-DOTATATE PET liver NET image datasets. The experiments show that our method significantly improves lesion detection performance compared with the baseline and outperforms recent state-of-the-art domain generalization approaches.",https://github.com/xyang258/livdetSDG,,PET/SPECT,Computer Aided Diagnosis,,,,,,,,
Lesion-aware Contrastive Learning for Diabetic Retinopathy Diagnosis ,"Early diagnosis and screening of diabetic retinopathy are critical in reducing the risk of vision loss in patients. However, in a real clinical situation, manual annotation of lesion regions in fundus images is time-consuming. Contrastive learning(CL) has recently shown its strong ability for self-supervised representation learning due to its ability of learning the invariant representation without any extra labelled data.
In this study, we aim to investigate how CL can be applied to extract lesion features in medical images. However, can the direct introduction of CL into the deep learning framework enhance the representation ability of lesion characteristics? We show that the answer is no. Due to the lesion-specific regions being insignificant in medical images, directly introducing CL would inevitably lead to the effects of false negatives, limiting the ability of the discriminative representation learning. Essentially, two key issues should be considered: (1) How to construct positives and negatives to avoid the problem of false negatives? (2) How to exploit the hard negatives for promoting the representation quality of lesions? In this work, we present a lesion-aware CL framework for DR grading. Specifically, we design a new generating positives and negatives strategy to overcome the false negatives problem in fundus images. Furthermore, a dynamic hard negatives mining method based on knowledge distillation is proposed in order to improve the quality of the learned embeddings. Extensive experimental results show that our method significantly advances state-of-the-art DR grading methods to a considerable 88.0%ACC/86.8% Kappa on the EyePACS benchmark dataset. Our code is available at https://github.com/IntelliDAL/Image.",https://github.com/IntelliDAL/Image,https://kaggle.com/competitions/diabetic-retinopathy-detection,Ophthalmology,Computer Aided Diagnosis,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
LightNeuS: Neural Surface Reconstruction in Endoscopy using Illumination Decline ,"We propose a new approach to 3D reconstruction from sequences of images acquired by monocular endoscopes. It is based on two key insights. First, endoluminal cavities are watertight, a property naturally enforced by modeling them in terms of a signed distance function. Second, the scene illumination is variable. It comes from the endoscope’s light sources and decays with the inverse of the squared distance to the surface. To exploit these insights, we build on NeuS, a neural implicit surface reconstruction technique with an outstanding capability to learn appearance and a SDF surface model from multiple views, but currently limited to scenes with static illumination. To remove this limitation and exploit the relation between pixel brightness and depth, we modify the NeuS architecture to explicitly account for it and introduce a calibrated photometric model of the endoscope’s camera and light source.
Our method is the first one to produce watertight reconstructions of whole colon sections. We demonstrate excellent accuracy on phantom imagery. Remarkably, the watertight prior combined with illumination decline, allows to complete the reconstruction of unseen portions of the surface with acceptable accuracy, paving the way to automatic quality assessment of cancer screening explorations, measuring the global percentage of observed mucosa.",,https://doi.org/10.7303/syn26707219,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network ,"Liver tumor segmentation and classification are important tasks in computer aided diagnosis. We aim to address three problems: liver tumor screening and preliminary diagnosis in non-contrast computed tomography (CT), and differential diagnosis in dynamic contrast-enhanced CT. A novel framework named Pixel-Lesion-pAtient Network (PLAN) is proposed. It uses a mask transformer to jointly segment and classify each lesion with improved anchor queries and a foreground-enhanced sampling loss. A patient branch further aggregates information from the whole image and predicts image-level labels. A large-scale multi-phase dataset is collected containing 939 tumor patients and 810 normal subjects. 4010 tumor instances of eight types are extensively annotated. On the non-contrast tumor screening task, PLAN achieves 95% and 96% in patient-level sensitivity and specificity. On contrast-enhanced CT, our lesion-level detection precision, recall, and classification accuracy are 92%, 89%, and 86%, outperforming widely used CNN and transformers for lesion segmentation. We also conduct a reader study on a holdout set of 250 cases. PLAN is on par with a senior human radiologist, showing the clinical significance of our results.",,,Computer Aided Diagnosis,Abdomen,Oncology,Image Segmentation,CT,,,,,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion ,"Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostic tool for gastrointestinal (GI) diseases. However, due to GI anatomical constraints and hardware manufacturing limitations, WCE vision signals may suffer from insufficient illumination, leading to a complicated screening and examination procedure. Deep learning-based low-light image enhancement (LLIE) in the medical field gradually attracts researchers. Given the exuberant development of the denoising diffusion probabilistic model (DDPM) in computer vision, we introduce a WCE LLIE framework based on the multi-scale convolutional neural network (CNN) and reverse diffusion process. The multi-scale design allows models to preserve high-resolution representation and context information from low-resolution, while the curved wavelet attention (CWA) block is proposed for high-frequency and local feature learning. Moreover, we combine the reverse diffusion procedure to optimize the shallow output further and generate images highly approximate to real ones. The proposed method is compared with eleven state-of-the-art (SOTA) LLIE methods and significantly outperforms quantitatively and qualitatively. The superior performance on GI disease segmentation further demonstrates the clinical potential of our proposed model. Our code is publicly accessible at github.com/longbai1006/LLCaps.",https://github.com/longbai1006/LLCaps,https://mycuhk-my.sharepoint.com/:u:/g/personal/1155161502_link_cuhk_edu_hk/EYtX3vMBWE1KizB1scvGOkgBzG4JW5SjTMAnJuxZTUAwdg?e=KJk1k2,Image Reconstruction,Abdomen,Computer Aided Diagnosis,Other,other,,,,,
Localized Questions in Medical Visual Question Answering ,"Visual Question Answering (VQA) models aim to answer natural language questions about given images. Due to its ability to ask questions that differ from those used when training the model, medical VQA has received substantial attention in recent years. However, existing medical VQA models typically focus on answering questions that refer to an entire image rather than where the relevant content may be located in the image. Consequently, VQA models are limited in their interpretability power and the possibility to probe the model about specific image regions. This paper proposes a novel approach for medical VQA that addresses this limitation by developing a model that can answer questions about image regions while considering the context necessary to answer the questions. Our experimental results demonstrate the effectiveness of our proposed model, outperforming existing methods on three datasets. Our code and data are available at https://github.com/sergiotasconmorales/locvqa.",https://github.com/sergiotasconmorales/locvqa,https://github.com/sergiotasconmorales/locvqa,Attention models,Computer Aided Diagnosis,Interpretability / Explainability,Other,,,,,,
Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation ,"Recent advancements in self-supervised learning have demonstrated that effective visual representations can be learned from unlabeled images. This has led to increased interest in applying self-supervised learning to the medical domain, where unlabeled images are abundant and labeled images are difficult to obtain. However, most self-supervised learning approaches are modeled as image level discriminative or generative proxy tasks, which may not capture the finer level representations necessary for dense prediction tasks like multi-organ segmentation.",,,Model Generalizability / Federated Learning,Image Segmentation,,,,,,,,
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification ,"The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution. Evaluation on 227 subjects with challenging SPNs revealed a significant AUC improvement over a longitudinal multimodal baseline (0.824 vs 0.752 AUC), as well as improvements over a single cross-section multimodal scenario (0.809 AUC) and a longitudinal imaging-only scenario (0.741 AUC). This work demonstrates significant advantages with a novel approach for co-learning longitudinal imaging and non-imaging phenotypes with transformers. Code available at https://github.com/MASILab/lmsignatures.",https://github.com/MASILab/lmsignatures,https://cdas.cancer.gov/nlst/,Attention models,Lung,Interpretability / Explainability,CT,,,,,,
LOTUS: Learning to Optimize Task-based US representations ,"Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. 
Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and 
precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification.
In this paper, we propose a novel approach for learning to optimize task-based ultrasound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. 
Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.",https://github.com/danivelikova/lotus,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Abdomen,Image Segmentation,Transfer learning,Ultrasound,,,,,
Low-dose CT image super-resolution network with dual-guidance feature distillation and dual-path content communication ,"Low-dose computer tomography (LDCT) has been widely used in medical diagnosis yet suffered from spatial resolution loss and artifacts. Numerous methods have been proposed to deal with those issues, but there still exists drawbacks: (1) convolution without guidance causes essential information not highlighted; (2) features with fixed-resolution lose the attention to multi-scale information; (3) single super-resolution module fails to balance details reconstruction and noise removal. Therefore, we propose an LDCT image super-resolution network consisting of a dual-guidance feature distillation backbone for elaborate visual feature extraction, and a dual-path content communication head for artifacts-free and details-clear CT reconstruction. Specifically, the dual-guidance feature distillation backbone is composed of a dual-guidance fusion module (DGFM) and a sampling attention block (SAB). The DGFM guides the network to concentrate the feature representation of the 3D inter-slice information in the region of interest (ROI) by introducing the average CT image and segmentation mask as complements of the original LDCT input. Meanwhile, the elaborate SAB utilizes the essential multi-scale features to capture visual information more relative to edges. The dual-path reconstruction architecture introduces the denoising head before and after the super-resolution (SR) head in each path to suppress residual artifacts, respectively. Furthermore, the heads with the same function share the parameters so as to efficiently improve the reconstruction performance by reducing the amount of parameters. The experiments compared with 6 state-of-the-art methods on 2 public datasets prove the superiority of our method. The code is made available at \url{https://github.com/neu-szy/dual-guidance_LDCT_SR",https://github.com/neu-szy/dual-guidance_LDCT_SR,,Image Reconstruction,Attention models,,,,,,,,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning ,"Interpretability is a key issue when applying deep learning models to longitudinal brain MRIs. One way to address this issue is by visualizing the high-dimensional latent spaces generated by deep learning via self-organizing maps (SOM). SOM separates the latent space into clusters and then maps the cluster centers to a discrete (typically 2D) grid preserving the high-dimensional relationship between clusters. However, learning SOM in a high-dimensional latent space tends to be unstable, especially in a self-supervision setting. Furthermore, the learned SOM grid does not necessarily capture clinically interesting information, such as brain age. To resolve these issues, we propose the first self-supervised SOM approach that derives a high-dimensional, interpretable representation stratified by brain age solely based on longitudinal brain MRIs (i.e., without demographic or cognitive information). Called Longitudinally-consistent Self-Organized Representation learning (LSOR), the method is stable during training as it relies on soft clustering (vs. the hard cluster assignments used by existing SOM). Furthermore, our approach generates a latent space stratified according to brain age by aligning trajectories inferred from longitudinal MRIs to the reference vector associated with the corresponding SOM cluster. When applied to longitudinal MRIs of the Alzheimer’s Disease Neuroimaging Initiative (ADNI, N=632), LSOR generates an interpretable latent space and achieves comparable or higher accuracy than the state-of-the-art representations with respect to the downstream tasks of classification (static vs. progressive mild cognitive impairment) and regression (determining ADAS-Cog score of all subjects). The code is available at https://github.com/ouyangjiahong/longitudinal-som-single-modality.",https://github.com/ouyangjiahong/longitudinal-som-single-modality,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - Brain Development,Interpretability / Explainability,MRI,,,,,,
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network ,"The process of acquiring microscopic images in life sciences often results in image degradation and corruption, characterised by the presence of noise and blur, which poses significant challenges in accurately analysing and interpreting the obtained data. This paper proposes LUCYD, a novel method for the restoration of volumetric microscopy images that combines the Richardson-Lucy deconvolution formula and the fusion of deep features obtained by a fully convolutional network. By integrating the image formation process into a feature-driven restoration model, the proposed approach aims to enhance the quality of the restored images whilst reducing computational costs and maintaining a high degree of interpretability. Our results demonstrate that LUCYD outperforms the state-of-the-art methods in both synthetic and real microscopy images, achieving superior performance in terms of image quality and generalisability. We show that the model can handle various microscopy modalities and different imaging conditions by evaluating it on two different microscopy datasets, including volumetric widefield and light-sheet microscopy. Our experiments indicate that LUCYD  can significantly improve resolution, contrast, and overall quality of microscopy images. Therefore, it can be a valuable tool for microscopy image restoration and can facilitate further research in various microscopy applications. We made the source code for the model accessible under https://github.com/ctom2/lucyd-deconvolution/.",https://github.com/ctom2/lucyd-deconvolution/,,Image Reconstruction,Other,Microscopy,Visualization in Biomedical Imaging,,,,,,
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector ,"Deep-learning-based object detection methods show promise for improving screening mammography, but high rates of false positives can hinder their effectiveness in clinical practice. To reduce false positives, we identify three challenges: (1) unlike natural images, a malignant mammogram typically contains only one malignant finding; (2)~mammography exams contain two views of each breast, and both views ought to be considered to make a correct assessment; (3) most mammograms are negative and do not contain any findings. In this work, we tackle the three aforementioned challenges by: (1) leveraging Sparse R-CNN and showing that sparse detectors are more appropriate than dense detectors for mammography; (2) including a multi-view cross-attention module to synthesize information from different views; (3) incorporating multi-instance learning (MIL) to train with unannotated images and perform breast-level classification. The resulting model, M&M, is a Multi-view and Multi-instance learning system that can both localize malignant findings and provide breast-level predictions. We validate M&M’s detection and classification performance using five mammography datasets. In addition, we demonstrate the effectiveness of each proposed component through comprehensive ablation studies.",,,Computer Aided Diagnosis,Breast,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,
M3D-NCA: Robust 3D Segmentation with Built-in Quality Control ,"Medical image segmentation relies heavily on large-scale deep learning models, such as UNet-based architectures. However, the real-world utility of such models is limited by their high computational requirements, which makes them impractical for resource-constrained environments such as primary care facilities and conflict zones. Furthermore, shifts in the imaging domain can render these models ineffective and even compromise patient safety if such errors go undetected. To address these challenges, we propose M3D-NCA, a novel methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D medical images using n-level patchification. Moreover, we exploit the variance in M3D-NCA to develop a novel quality metric which can automatically detect errors in the segmentation process of NCAs. M3D-NCA outperforms the two magnitudes larger UNet models in hippocampus and prostate segmentation by 2% Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the potential of M3D-NCA as an effective and efficient alternative for medical image segmentation in resource-constrained environments.",github.com/MECLabTUDA/M3D-NCA,medicaldecathlon.com,Other,Image Segmentation,MRI,,,,,,,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging ,"Mitral regurgitation (MR) is a heart valve disease with potentially fatal consequences that can only be forestalled through timely diagnosis and treatment. Traditional diagnosis methods are expensive, labor-intensive and require clinical expertise, posing a barrier to screening for MR. To overcome this impediment, we propose a new semi-supervised model for MR classification called CUSSP. CUSSP operates on cardiac magnetic resonance (CMR) imaging slices of the 4-chamber view of the heart. It uses standard computer vision techniques and contrastive models to learn from large amounts of unlabeled data, in conjunction with specialized classifiers to establish the first ever automated MR classification system using CMR imaging sequences. Evaluated on a test set of 179 labeled – 154 non-MR and 25 MR – sequences, CUSSP attains an F1 score of 0.69 and a ROC-AUC score of 0.88, setting the first benchmark result for detecting MR from CMR imaging sequences.",https://github.com/Information-Fusion-Lab-Umass/CUSSP_UKB_MR,,Cardiac,Computer Aided Diagnosis,Other,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis ,"Cross-modality medical image synthesis is a critical topic and has the potential to facilitate numerous applications in the medical imaging field. Despite recent successes in deep-learning-based generative models, most current medical image synthesis methods rely on generative adversarial networks and suffer from notorious mode collapse and unstable training. Moreover, the 2D backbone-driven approaches would easily result in volumetric inconsistency, while 3D backbones are challenging and impractical due to the tremendous memory cost and training difficulty. In this paper, we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones and present a diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image synthesis. To learn the cross-modality slice-wise mapping, we employ a latent diffusion model and learn a low-dimensional latent space, resulting in high computational efficiency. To enable the 3D image synthesis and mitigate volumetric inconsistency, we further insert a series of volumetric layers in the 2D slice-mapping model and fine-tune them with paired 3D data. This paradigm extends the 2D image diffusion model to a volumetric version with a slightly increasing number of parameters and computation, offering a principled solution for generic cross-modality 3D medical image synthesis. We showcase the effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate that our framework achieves superior synthesis results with volumetric consistency.",,https://rire.insight-journal.org/index.html,MRI,Other,,,,,,,,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification ,"Breast cancer diagnosis is a challenging task. Recently, the application of deep learning techniques to breast cancer diagnosis has become a popular trend. However, the effectiveness of deep neural networks is often limited by the lack of interpretability and the need for significant amount of manual annotations. To address these issues, we present a novel approach by leveraging both gaze data and multi-view data for mammogram classification. The gaze data of the radiologist serves as a low-cost and simple form of coarse annotation, which can provide rough localizations of lesions. We also develop a pyramid loss better fitting to the gaze-supervised process. Moreover, considering many studies overlooking interactive information relevant to diagnosis, we accordingly utilize transformer-based attention in our network to mutualize multi-view pathological information, and further employ a bidirectional fusion learning (BFL) to more effectively fuse multi-view information. Experimental results demonstrate that our proposed model significantly improves both mammogram classification performance and interpretability through incorporation of gaze data and cross-view interactive information.",,,Breast,Computer Aided Diagnosis,Attention models,Interpretability / Explainability,CT,Visualization in Biomedical Imaging,,,,
Many tasks make light work: Learning to localise medical anomalies from multiple synthetic tasks ,"There is a growing interest in single-class modelling and out-of-distribution detection as fully supervised machine learning models cannot reliably identify classes not included in their training. The long tail of infinitely many out-of-distribution classes in real-world scenarios, e.g., for screening, triage, and quality control, means that it is often necessary to train single-class models that represent an expected feature distribution, e.g., from only strictly healthy volunteer data. Conventional supervised machine learning would require the collection of datasets that contain enough samples of all possible diseases in every imaging modality, which is not realistic. Self-supervised learning methods with synthetic anomalies are currently amongst the most promising approaches, alongside generative auto-encoders that analyse the residual reconstruction error. However, all methods suffer from a lack of structured validation, which makes calibration for deployment difficult and dataset-dependant.
Our method alleviates this by making use of multiple visually-distinct synthetic anomaly learning tasks for both training and validation. This enables more robust training and generalisation. With our approach we can readily outperform state-of-the-art methods, which we demonstrate on exemplars in brain MRI and chest X-rays. Code is available at https://github.com/matt-baugh/many-tasks-make-light-work .",https://github.com/matt-baugh/many-tasks-make-light-work,https://www.humanconnectome.org/study/hcp-young-adult,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,MRI,other,,,,,,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images ,"Semantic segmentation is an important issue of intraoperative guidance in laparoscopic surgery. However, the acquisition and annotation of laparoscopic datasets requires a large amount of workload, which limits the research on semantic segmentation of laparoscopic images. In this paper, we address the Domain-Adaptive Semantic Segmentation (DASS) task, which requires only computer-generated simulated images and unlabelled real images to train a laparoscopic image segmentation network. In order to bridge the large domain gap between generated and real images, we propose a Masked Frequency Consistency (MFC) module to encourage network learning frequency related information of the target domain as additional clues for robust visual recognition. Specifically, MFC randomly masks some of the high-frequency information of the image to enhance the consistency of the network’s predictions for low-frequency images and real images. Extensive experimental results show that the proposed MFC module can be flexibly inserted into existing DASS frameworks and improve performance. Our approach can perform comparable to fully supervised learning method on the CholecSeg8K dataset without using any manual annotation.",https://github.com/MoriLabNU/MFC,http://opencas.dkfz.de/image2image,Transfer learning,Image Segmentation,,,,,,,,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering ,"Medical visual question answering (VQA) is a challenging task that requires an-swering clinical questions of a given medical image, by taking consider of both visual and language information. However, due to the small scale of training data for medical VQA, pre-training fine-tuning paradigms have been a commonly used solution to improve model generalization performance. In this paper, we present a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption da-tasets, by leveraging both unimodal and multimodal contrastive losses, along with masked language modeling and image text matching as pre-training objectives. The pre-trained model is then transferred to downstream medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA) performance on three pub-licly available medical VQA datasets with significant accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conduct a comprehensive anal-ysis to validate the effectiveness of different components of the approach and study different pre-training settings. Our codes and models are available at https://github.com/pengfeiliHEU/MUMC.",https://github.com/pengfeiliHEU/MUMC,https://osf.io/bd96f,Imaging Biomarkers,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,,,,,,
Maximum Entropy on Erroneous Predictions: Improving model calibration for medical image segmentation ,"Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. 
We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also in segmentation quality.",https://github.com/agosl/Maximum-Entropy-on-Erroneous-Predictions/,https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/AECRSD,Uncertainty,Cardiac,Neuroimaging - Others,Image Segmentation,MRI,,,,,
Maximum-entropy estimation of joint relaxation-diffusion distribution using multi-TE diffusion MRI ,Combinedrelaxation-diffusionMRI(rdMRI)techniqueprobes tissue microstructure using imaging data with multiple b-values and echo-time. Joint analysis of rdMRI data and provide the relaxation diffusion distribution (RDD) function to characterize heterogeneous tissue microstructure without using multi-component models. This paper shows that the problem of estimating RDD functions is equivalent to the multivariate Hausdorff moment problem by changing variables. Then three maximum entropy (ME) estimation problems are proposed to estimate the ME-RDD functions in different parameter spaces. All three problems can be solved by using convex optimization algorithms. The performance of the proposed algorithms is compared with the standard methods using basis functions based on simulation and in vivo rdMRI data. The proposed ME-RDD functions can provide more accurate estimation results.,https://github.com/LipengNing/ME-RDD,https://github.com/LipengNing/ME-RDD/tree/main/MICCAI_example,Neuroimaging - DWI and Tractography,MRI,,,,,,,,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images ,"Super-resolution (SR) of wireless capsule endoscopy (WCE) images is challenging because paired high-resolution (HR) images are not available. An intuitive solution is to simulate paired low-resolution (LR) WCE images from HR electronic endoscopy images for supervised learning. However, the SR model obtained by this method cannot be well adapted to real WCE images due to the large domain gap between electronic endoscopy images and WCE images.  To address this issue, we propose a Multi-level Domain Adaptation SR model (MDA-SR) in an unsupervised manner using arbitrary set of WCE images and HR electronic endoscopy images. Our approach implements domain adaptation at the image level and latent level during the degradation and SR processes, respectively. To the best of our knowledge, this is the first work to explore an unsupervised SR approach for WCE images. Furthermore, we design an Endoscopy Image Quality Evaluator (EIQE) based on the reference-free image evaluation metric NIQE, which is more suitable for evaluating WCE image quality. Extensive experiments demonstrate that our MDA-SR method outperforms state-of-the-art SR methods both quantitatively and qualitatively.",https://github.com/SMU-MedicalVision/MDA-SR,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Abdomen,other,,,,,,,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets ,"Despite its clinical utility, medical image segmentation (MIS) remains a daunting task due to images’ inherent complexity and variability. Vision transformers (ViTs) have recently emerged as a promising solution to improve MIS; however, they require larger training datasets than convolutional neural networks. To overcome this obstacle, data-efficient ViTs were proposed, but they are typically trained using a single source of data, which overlooks the valuable knowledge that could be leveraged from other available datasets. Naïvly combining datasets from different domains can result in negative knowledge transfer (NKT), i.e., a decrease in model performance on some domains with non-negligible inter-domain heterogeneity. In this paper, we propose MDViT, the first multi-domain ViT that includes domain adapters to mitigate data-hunger and combat NKT by adaptively exploiting knowledge in multiple small data resources (domains).
Further, to enhance representation learning across domains, we integrate a mutual knowledge distillation paradigm that transfers knowledge between a universal network (spanning all the domains) and auxiliary domain-specific network branches. Experiments on 4 skin lesion segmentation datasets show that MDViT outperforms state-of-the-art algorithms, with superior segmentation performance and a fixed model size, at inference time, even as more domains are added. Our code is available at https://github.com/siyi-wind/MDViT.",https://github.com/siyi-wind/MDViT,https://challenge.isic-archive.com/data/#2018,Image Segmentation,Dermatology,Attention models,Data Efficient Learning,,,,,,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation ,"Acquiring and annotating sufficient labeled data is crucial in developing accurate and robust learning-based models, but obtaining such data can be challenging in many medical image segmentation tasks. One promising solution is to synthesize realistic data with ground-truth mask annotations. However, no prior studies have explored generating complete 3D volumetric images with masks. In this paper, we present MedGen3D, a deep generative framework that can generate paired 3D medical images and masks. First, we represent the 3D medical data as 2D sequences and propose the Multi-Condition Diffusion Probabilistic Model (MC-DPM) to generate multi-label mask sequences adhering to anatomical geometry. Then, we use an image sequence generator and semantic diffusion refiner conditioned on the generated mask sequences to produce realistic 3D medical images that align with the generated masks. Our proposed framework guarantees accurate alignment between synthetic images and segmentation maps. Experiments on 3D thoracic CT and brain MRI datasets show that our synthetic data is both diverse and faithful to the original data, and demonstrate the benefits for downstream segmentation tasks. We anticipate that MedGen3D’s ability to synthesize paired 3D medical images and masks will prove valuable in training deep learning models for medical imaging tasks.",,,Transfer learning,Image Segmentation,,,,,,,,
Medical Boundary Diffusion Model for Skin Lesion Segmentation ,"Skin lesion segmentation in dermoscopy images has seen recent success due to advancements in multi-scale boundary attention and feature-enhanced modules. However, existing methods that rely on end-to-end learning paradigms, which directly input images and output segmentation maps, often struggle with extremely hard boundaries, such as those found in lesions of particularly small or large sizes. This limitation arises because the receptive field and local context extraction capabilities of any finite model are inevitably limited, and the acquisition of additional expert-labeled data required for larger models is costly. Motivated by the impressive advances of diffusion models that regard image synthesis as a parameterized chain process, we introduce a novel approach that formulates skin lesion segmentation as a boundary evolution process to thoroughly investigate the boundary knowledge. Specifically, we propose the Medical Boundary Diffusion Model (MB-Diff), which starts with a randomly sampled Gaussian noise, and the boundary evolves within finite times to obtain a clear segmentation map. First, we propose an efficient multi-scale image guidance module to constrain the boundary evolution, which makes the evolution direction suit our desired lesions. Second, we propose an evolution uncertainty-based fusion strategy to refine the evolution results and yield more precise lesion boundaries. We evaluate the performance of our model on two popular skin lesion segmentation datasets and compare our model to the latest CNN and transformer models. Our results demonstrate that our model outperforms existing methods in all metrics and achieves superior performance on extremely challenging skin lesions. The proposed approach has the potential to significantly enhance the accuracy and reliability of skin lesion segmentation, providing critical information for diagnosis and treatment. All resources will be publicly at https://github.com/jcwang123/MBDiff.",https://github.com/jcwang123/MBDiff,,Image Segmentation,Dermatology,,,,,,,,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment ,"Medical phrase grounding (MPG) aims to locate the most relevant region in a medical image, given a phrase query describing certain medical findings, which is an important task for medical image analysis and radiological diagnosis. Existing visual grounding methods rely on general visual features for identifying objects in natural images and fail to take subtle and specialized features of medical findings into account, leading to sub-optimal MPG performance. In this paper, we propose MedRPG, an end-to-end approach for MPG. MedRPG is built on a lightweight vision-language transformer encoder and directly predicts the box coordinates of mentioned medical findings, which can be trained with limited medical data. To enable MedRPG to locate nuanced medical findings with better region-phrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to pull both the features and attention outputs of relevant region-phrase pairs close together while pushing those of irrelevant regions far away, such that the final box prediction depends more on its finding-specific regions and phrases. Experimental results on three MPG datasets demonstrate that our MedRPG outperforms state-of-the-art visual grounding approaches by a large margin, and the proposed TaCo strategy is effective in enhancing finding localization ability and reducing spurious region-phrase correlations.",,,Lung,Text (clinical/radiology reports),,,,,,,,
MedIM: Boost Medical Image Representation via Radiology Report-guided Masking ,"Masked image modelling (MIM)-based pre-training shows promise in improving image representations with limited annotated data by randomly masking image patches and reconstructing them. However, random masking may not be suitable for medical images due to their unique pathology characteristics. This paper proposes \textbf{M}asked m\textbf{ed}ical \textbf{I}mage \textbf{M}odelling (MedIM), a novel approach, to our knowledge, the first research that masks and reconstructs discriminative areas guided by radiological reports, encouraging the network to explore the stronger semantic representations from medical images. We introduce two mutual comprehensive masking strategies, knowledge word-driven masking (KWM) and sentence-driven masking (SDM). KWM uses Medical Subject Headings (MeSH) words unique to radiology reports to identify discriminative cues mapped to MeSH words and guide the mask generation. 
SDM considers that reports usually have multiple sentences, each of which describes different findings, and therefore integrates sentence-level information to identify discriminative regions for mask generation. MedIM integrates both strategies by simultaneously restoring the images masked by KWM and SDM for a more robust and representative medical visual representation. Our extensive experiments on various downstream tasks covering multi-label/class image classification, medical image segmentation, and medical image-text analysis, demonstrate that MedIM with report-guided masking achieves competitive performance. Our method substantially outperforms ImageNet pre-training, MIM-based pre-training, and medical image-report pre-training counterparts. 
Codes are available at \url{https://github.com/YtongXie/MedIM}.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Image Segmentation,Transfer learning,Text (clinical/radiology reports),,,,,
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation ,"There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is available here: https://github.com/MIC-DKFZ/MedNeXt",https://github.com/MIC-DKFZ/MedNeXt,https://www.synapse.org/#!Synapse:syn3193805/wiki/89480,Image Segmentation,Attention models,CT,MRI,,,,,,
Memory Replay for Continual Medical Image Segmentation through Atypical Sample Selection ,"Medical image segmentation is critical for accurate diagnosis, treatment planning and disease monitoring. Existing deep learning based segmentation models can suffer from catastrophic forgetting, especially when faced with varying patient populations and imaging protocols. Continual learning(CL) addresses this challenge by enabling the model to learn continuously from a stream of incoming data without the need to retrain from scratch. In this work, we propose a continual learning based approach for medical image segmentation using a novel memory replay-based learning scheme. The approach uses a simple and effective algorithm for image selection to create the memory bank by ranking and selecting images based on their contribution to the learning process.  We evaluate our proposed algorithm on three different problems and compare it with several baselines, showing significant improvements in performance. Our study highlights the potential of continual learning-based algorithms for medical image segmentation and underscores the importance of efficient sample selection in creating memory banks.",,,Image Segmentation,Continual Learning,MRI,,,,,,,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images ,"Sparse-view computed tomography (CT) has been adopted as an important technique for speeding up data acquisition and decreasing radiation dose. However, due to the lack of sufficient projection data, the reconstructed CT images often present severe artifacts, which will be further amplified when patients carry metallic implants. For this joint sparse-view reconstruction and metal artifact reduction task, most of the existing methods are generally confronted with two main limitations: 1) They are almost built based on common network modules without fully embedding the physical imaging geometry constraint of this specific task into the dual-domain learning; 2) Some important prior knowledge is not deeply explored and sufficiently utilized. Against these issues, we specifically construct a dual-domain reconstruction model and propose a model-driven equivariant proximal network, called MEPNet. The main characteristics of MEPNet are: 1) It is optimization-inspired and has a clear working mechanism; 2) The involved proximal operator is modeled via a rotation equivariant convolutional neural network, which finely represents the inherent rotational prior underlying the CT scanning that the same organ can be imaged at different angles. Extensive experiments conducted on several datasets comprehensively substantiate that compared with the conventional convolution-based proximal network, such a rotation equivariance mechanism enables our proposed method to achieve better reconstruction performance with fewer network parameters. We will release the code once the paper is accepted.",https://github.com/hongwang01/MEPNet,,Image Reconstruction,CT,,,,,,,,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer ,"Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting region-specific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a merging-diverging learning framework for survival prediction from multi-modality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region-specific Attention Gate (RAG) block to screen out the features related to lesion regions. Our framework is demonstrated on survival prediction from PET-CT images in Head and Neck (H&N) cancer, by designing an X-shape merging-diverging hybrid transformer network (named XSurv). Our XSurv combines the complementary information in PET and CT images and extracts the region-specific prognostic information in PT and MLN regions. Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.",https://github.com/MungoMeng/Survival-XSurv,https://hecktor.grand- challenge.org/,Treatment Response and Outcome/Disease Prediction,Oncology,PET/SPECT,,,,,,,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy ,"Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reducing any bias due to template selection. The proposed method operates directly on meshes and is computationally efficient, making it an attractive alternative to traditional and deep learning-based SSM approaches.",https://github.com/iyerkrithika21/mesh2SSM_2023/tree/main,http://medicaldecathlon.com/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,,,,,,,,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging ,"In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common finetuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based learning rate (LR) tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns LRs for different layers in an online fashion, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various medical applications show that MetaLR outperforms previous state-of-the-art (SOTA) fine-tuning strategies. Codes are released.",https://github.com/Schuture/MetaLR,https://github.com/jannisborn/covid19 ultrasound,Transfer learning,Computer Aided Diagnosis,Data Efficient Learning,Meta-learning,CT,Ultrasound,,,,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization ,"This paper presents a novel approach to addressing the problem of chaotic latent space in self-supervised medical vision-language processing (VLP). Our proposed method is called Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), which leverages a frozen language model for training stability and efficiency and employs two losses (a novel vision uniformity loss and a vision-language alignment loss) to harmonize the latent space geometry during VLP.
Our extensive experimental results on three diverse downstream tasks: supervised image classification, semantic segmentation, and object detection, across five public datasets, demonstrate that M-FLAG significantly outperforms existing medical VLP approaches with much lower model complexity (reducing the number of parameters by 78%). Notably, M-FLAG achieves outstanding performance on the segmentation task while using only 1% of the RSNA dataset, even outperforming ImageNet pre-trained models that have been fine-tuned using 100% of the data.",https://github.com/cheliu-computation/m-flag-miccai2023,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Data Efficient Learning,other,Text (clinical/radiology reports),,,,,,
M-GenSeg: Domain Adaptation For Target Modality Tumor Segmentation With Annotation-Efficient Supervision ,"Automated medical image segmentation using deep neural networks typically requires substantial supervised training. However, these models fail to generalize well across different imaging modalities. This shortcoming, amplified by the limited availability of expert annotated data, has been hampering the deployment of such methods at a larger scale across modalities. To address these issues, we propose M-GenSeg, a new semi-supervised generative training strategy for cross-modality tumor segmentation on unpaired bi-modal datasets. With the addition of known healthy images, an unsupervised objective encourages the model to disentangling tumors from the background, which parallels the segmentation task. Then, by teaching the model to convert images across modalities, we leverage available pixel-level annotations from the source modality to enable segmentation in the unannotated target modality. We evaluated the performance on a brain tumor segmentation dataset composed of four different contrast sequences from the public BraTS 2020 challenge data. We report consistent improvement in Dice scores over state-of-the-art domain-adaptive baselines on the unannotated target modality. Unlike the prior art, M-GenSeg also introduces the ability to train with a partially annotated source modality.",https://github.com/MaloADBA/MGenSeg_2D,https://www.med.upenn.edu/cbica/brats2020/registration.html,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments ,"Most diffusion biophysical models capture basic properties of tissue microstructure, such as diffusivity and anisotropy. More realistic models that reate the diffusion-weighted signal to cell size and membrane permeability often require simplifying assumptions such as short gradient pulse and Gaussian phase distribution, leading to statistical features relating to tissue properties instead of real quantitative measurements. Here, we propose a method to quantify tissue microstructure without jeopardizing accuracy owing to unrealistic assumptions. Our method utilize realistic signals simulated from the geometries of cellular microenvironments as fingerprints, which are then employed in a spherical mean estimation framework to disentangle the effects of orientation dispersion from the microscopic tissue properties. We demonstrate the efficacy of microstructure fingerprinting in estimating intra-cellular, extra-cellular, and intra-soma volume fractions as well as axon radius, soma radius, and membrane permeability.",,,Neuroimaging - DWI and Tractography,MRI,,,,,,,,
Minimal-supervised Medical Image Segmentation via Vector Quantization Memory ,"Medical image segmentation is a critical key task for computer-assisted diagnosis and disease monitoring. However, collecting a large-scale medical dataset with well-annotation is time-consuming and requires domain knowledge. Reducing the number of annotations poses two challenges: obtaining sufficient supervision and generating high-quality pseudo labels. To address these, we propose a universal framework for annotation-efficient medical segmentation, which is capable of handling both scribble-supervised and point-supervised segmentation. Our approach includes an auxiliary reconstruction branch that provides more supervision and backward sufficient gradients for learning visual representations. Besides, a novel pseudo label generation branch utilizes the VQ bank to store texture-oriented and global features for generating pseudo labels. To boost the model training, we generate high-quality pseudo labels by mixing the segmentation prediction and pseudo labels from the VQ bank. The experiments on the ACDC MRI segmentation dataset demonstrate the effectiveness of our proposed method. We obtain a comparable performance (0.86 vs. 0.87 DSC score).",,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection ,"During ultrasonic scanning processes, real-time lesion detection can assist radiologists in accurate cancer diagnosis. However, this essential task remains challenging and underexplored. General-purpose real-time object detection models can mistakenly report obvious false positives (FPs) when applied to ultrasound videos, potentially misleading junior radiologists. One key issue is their failure to utilize negative symptoms in previous frames, denoted as negative temporal contexts (NTC). To address this issue, we propose to extract contexts from previous frames, including NTC, with the guidance of inverse optical flow. By aggregating extracted contexts, we endow the model with the ability to suppress FPs by leveraging NTC. We call the resulting model UltraDet. The proposed UltraDet demonstrates significant improvement over previous state-of-the-arts and achieves real-time inference speed. We release the code, checkpoints, and high-quality labels of the CVA-BUS dataset used in our experiments in https://github.com/HaojunYu1998/UltraDet.",https://github.com/HaojunYu1998/UltraDet,https://github.com/HaojunYu1998/UltraDet,Ultrasound,Video,,,,,,,,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization ,"Generalization capabilities of learning-based medical image segmentation across domains are currently limited by the performance degradation caused by the domain shift, particularly for ultrasound (US) imaging. The quality of US images heavily relies on carefully tuned acoustic parameters, which vary across sonographers, machines, and settings. To improve the generalizability on US images across domains, we propose MI-SegNet, a novel mutual information (MI) based framework to explicitly disentangle the anatomical and domain feature representations; therefore, robust domain-independent segmentation can be expected. Two encoders are employed to extract the relevant features for the disentanglement. The segmentation only uses the anatomical feature map for its prediction. In order to force the encoders to learn meaningful feature representations a cross-reconstruction method is used during training. Transformations, specific to either domain or anatomy are applied to guide the encoders in their respective feature extraction task. Additionally, any MI present in both feature maps is punished to further promote separate feature spaces. We validate the generalizability of the proposed domain-independent segmentation approach on several datasets with varying parameters and machines. Furthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving as a pre-trained model by comparing it with state-of-the-art networks.",https://github.com/yuan-12138/MI-SegNet,,Ultrasound,Vascular,Image Segmentation,,,,,,,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis ,"Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis.  Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.",,,Uncertainty,Dermatology,Neuroimaging - Others,Treatment Response and Outcome/Disease Prediction,,,,,,
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping ,"Detection of mitosis events plays an important role in biomedical research. Deep-learning-based mitosis detection methods achieve outstanding performance with a certain amount of labeled data. However, these methods require annotation for each imaging condition. Collecting a certain amount of labeled data is time-consuming and human labor. In this paper, we propose a mitosis detection method that can be trained with partially annotated sequences. The base idea is to generate a fully labeled dataset from the partially annotated sequences. We first generate an image pair which not contain mitosis events by frame-order-flipping. Then, we paste mitosis events to the image pair by alpha-blending-pasting and generate a fully labeled dataset. We demonstrate the performance of our method on four datasets, and we confirm that our method outperforms other comparisons which use partially labeled sequences.",https://github.com/naivete5656/MDPAFOF,https://github.com/naivete5656/MDPAFOF,Microscopy,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning ,"Mitral regurgitation (MR) is the most common heart valve disease. Prolonged regurgitation can cause changes in the heart size, lead to impaired systolic and diastolic capacity, and even threaten life. In clinical practice, MR is evaluated by the proximal isovelocity surface area (PISA) method, where manual measurements of the regurgitation velocity and the value of PISA radius from multiple ultrasound images are required to obtain the mitral regurgitant stroke volume (MRSV) and effective regurgitant orifice area (EROA). In this paper, we propose a fully automatic method for MR quantification, which follows the pipeline of ECG-based cycle detection, Doppler spectrum segmentation, PISA radius segmentation, and MR quantification. Specifically, for the Doppler spectrum segmentation, we proposed a novel adaptive-weighting multi-channel segmentation network, PISA-net, to accurately identify the upper and lower contours of the PISA radius from a pair of coupled M-mode PISA image and corresponding M-mode decolored image. Using the complementary information of the two coupled images and combing with the spatial attention module, the proposed PISA-net can well identify the contours of the PISA radius and therefore lead to accurate quantification of MR parameters. To the best of our knowledge, this is the first study of automatic MR quantification. Experimental results demonstrated the effectiveness of the whole pipeline, especially the PISA-net for PISA radius segmentation. The full method achieves a high Pearson correlation of 0.994 for both MRSV and EROA, implying its great potential in the clinical application of MR diagnosis.",,,Computer Aided Diagnosis,Cardiac,Image Segmentation,Other,Ultrasound,,,,,
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis ,"Analyses of longitudinal brain networks, i.e., graphs, are of significant interest to understand the dynamics of brain changes with respect to aging and neurodegenerative diseases. However, each subject has a graph of heterogeneous structure and time-points as the data are obtained over several years. Moreover, most existing datasets suffer from lack of samples as the images are expensive to acquire, which leads to overfitting in complex deep neural networks. To address these issues for characterizing progressively altered changes of brain connectome and region-wise measures as early as possible, we develop Spatio-Temporal Graph Multi-Layer Perceptron (STGMLP) that mixes features over both graph and time spaces to classify sets of longitudinal human brain connectomes. The proposed model is made efficient and interpretable such that it can be easily adopted to medical imaging datasets and identify personalized features responsible for a specific diagnostic label. Extensive experiments show that our method achieves successful results in both performance and computational efficiency on Alzheimer’s Disease Neuroimaging Initiative (ADNI) and Adolescence Brain Cognitive Development (ABCD) datasets independently.",,,Data Efficient Learning,Neuroimaging - Brain Development,Neuroimaging - DWI and Tractography,MRI,PET/SPECT,,,,,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis ,"Multiple instance learning exhibits a powerful approach for whole slide image-based diagnosis in the absence of pixel- or patch-level annotations. In spite of the huge size of whole slide images, the number of individual slides is often rather small, leading to a small number of labeled samples. To improve training, we propose and investigate novel data augmentation strategies for multiple instance learning based on the idea of linear and multilinear interpolation of feature vectors within and between individual whole slide images. Based on state-of-the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study was conducted considering a range of common data augmentation strategies. Whereas a strategy based on to the original MixUp approach showed decreases in accuracy, a novel multilinear intra-slide interpolation method led to consistent increases in accuracy.",https://gitlab.com/mgadermayr/mixupmil,https://gitlab.com/mgadermayr/mixupmil,Histopathology,Computer Aided Diagnosis,Data Efficient Learning,,,,,,,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI ,"Abdominal MRI is critical for diagnosing a wide variety of diseases. However, due to respiratory motion and other organ motions, it is challenging to obtain motion-free and isotropic MRI for clinical diagnosis. Imaging patients with inflammatory bowel disease (IBD) can be especially problematic, owing to involuntary bowel movements and difficulties with long breath-holds during acquisition. Therefore, this paper proposes a deep adversarial super-resolution (SR) reconstruction approach to address the problem of multi-task degradation by utilizing cycle consistency in a staged reconstruction model. We leverage a low-resolution (LR) latent space for motion correction, followed by super-resolution reconstruction, compensating for imaging artefacts caused by respiratory motion and spontaneous bowel movements. This alleviates the need for semantic knowledge about the intestines and paired data. Both are examined through variations of our proposed approach and we compare them to conventional, model-based, and learning-based MC and SR methods. Learned image reconstruction approaches are believed to occasionally hide disease signs. We investigate this hypothesis by evaluating a downstream task, automatically scoring IBD in the area of the terminal ileum on the reconstructed images and show evidence that our method does not suffer a synthetic domain bias.",https://github.com/vito1820/MoCoSR,https://portal.gdc.cancer.gov/projects/TCGA-LIHC,Image Reconstruction,Model Generalizability / Federated Learning,MRI,,,,,,,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks ,"Alzheimer’s disease (AD) is a common irreversible neurodegenerative disease among elderlies. Establishing relationships between brain networks and cognitive scores plays a vital role in identifying the progression of AD. However, most of the previous works focus on a single time point, without modeling the disease progression with longitudinal brain networks data. Besides, the longitudinal data is insufficient for sufficiently modeling the predictive models. To address these issues, we propose a Self-supervised Multi-Task learning Progression model SMP-Net for modeling the relationship between longitudinal brain networks and cognitive scores. Specifically, the proposed model is trained in a self-supervised way by designing a masked graph auto-encoder and a  temporal contrastive learning that simultaneously learn the structural and evolutional features from the longitudinal brain networks. Furthermore, we propose a temporal multi-task learning paradigm to model the relationship among multiple cognitive scores prediction tasks. Experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset show the effectiveness of our method and achieve consistent improvements over state-of-the-art methods in terms of Mean Absolute Error (MAE), Pearson Correlation Coefficient (PCC) and Concordance Correlation Coefficient (CCC). Our code is available at https
://github.com/IntelliDAL/Graph/tree/main/SMP-Net.",https://github.com/IntelliDAL/Graph/tree/main/SMP-Net,https://adni.loni.usc.edu/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,,,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer ,"The Transformer structures have been widely used in computer vision and have recently made an impact in the area of medical image registration. However, the use of Transformer in most registration networks is straightforward. These networks often merely use the attention mechanism to boost the feature learning as the segmentation networks do, but do not sufficiently design to be adapted for the registration task. In this paper, we propose a novel motion decomposition Transformer (ModeT) to explicitly model multiple motion modalities by fully exploiting the intrinsic capability of the Transformer structure for deformation estimation. The proposed ModeT naturally transforms the multi-head neighborhood attention relationship into the multi-coordinate relationship to model multiple motion modes. Then the competitive weighting module (CWM) fuses multiple deformation sub-fields to generate the resulting deformation field. Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets show that our method outperforms current state-of-the-art registration networks and Transformers, demonstrating the potential of our ModeT for the challenging non-rigid deformation estimation problem. The benchmarks and our code are publicly available at https://github.com/ZAX130/SmileCode.",https://github.com/ZAX130/SmileCode,,Image Registration,Attention models,MRI,,,,,,,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI ,"Resting-state functional MRI (rs-fMRI) is increasingly used to detect altered functional connectivity patterns caused by brain disorders, thereby facilitating objective quantification of brain pathology. Existing studies typically extract fMRI features using various machine/deep learning methods, but the generated imaging biomarkers are often challenging to interpret. Besides, the brain operates as a modular system with many cognitive/topological modules, where each module contains subsets of densely inter-connected regions-of-interest (ROIs) that are sparsely connected to ROIs in other modules. However, current methods
cannot effectively characterize brain modularity. This paper proposes a
modularity-constrained dynamic representation learning (MDRL) framework for interpretable brain disorder analysis with rs-fMRI. The MDRL consists of 3 parts: (1) dynamic graph construction, (2) modularityconstrained spatiotemporal graph neural network (MSGNN) for dynamic feature learning, and (3) prediction and biomarker detection. In particular, the MSGNN is designed to learn spatiotemporal dynamic representations of fMRI, constrained by 3 functional modules (i.e., central executive network, salience network, and default mode network). To enhance discriminative ability of learned features, we encourage the MSGNN to reconstruct network topology of input graphs. Experimental results on two public and one private datasets with a total of 1, 155 subjects validate that our MDRL outperforms several state-of-the-art methods in fMRI-based brain disorder analysis. The detected fMRI biomarkers have good explainability and can be potentially used to improve clinical diagnosis.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Interpretability / Explainability,,,,,,,,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from cine CMR with Improved Accuracy and Efficiency ,"Anatomical heart mesh models created from cine cardiac images are useful for the evaluation and monitoring of cardiovascular diseases, but require challenging and time-consuming reconstruction processes. Errors due to reduced spatial resolution and motion artefacts limit the accuracy of 3D models. We proposed ModusGraph to produce a higher quality 3D and 4D (3D+time) heart models automatically, employing i) a voxel processing module with Modality Handles and a super-resolution decoder to define low-resolution and high-resolution segmentations and correct motion artefacts with multi-modal unpaired data, ii) a Residual Spatial-temporal Graph Convolution Network to generate mesh models by controlled and progressive spatial-temporal deformation to better capture the cardiac motion, and iii) a Signed Distance Sampling process to bridge those two parts for end-to-end training. ModusGraph was trained and evaluated on CT angiograms and cardiovascular MRI cines, showing superior performance compared to other mesh reconstruction methods. It creates well-defined meshes from sparse MRI cines, enabling vertex tracking across cardiac cycle frames. This process aids in analyzing myocardium function and conducting biomechanical analyses from imaging data. https://github.com/MalikTeng/ModusGraph",https://github.com/MalikTeng/ModusGraph,,Computational Anatomy and Physiology,Cardiac,Image Segmentation,Transfer learning,CT,MRI,Visualization in Biomedical Imaging,,,
Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping ,"Designing deep learning algorithms for gland segmentation is crucial for automatic cancer diagnosis and prognosis, yet the expensive annotation cost hinders the development and application of this technology. In this paper, we make a first attempt to explore a deep learning method for unsupervised gland segmentation, where no manual annotations are required. Existing unsupervised semantic segmentation methods encounter a huge challenge on gland images: They either over-segment a gland into many fractions or under-segment the gland regions by confusing many of them with the background. To overcome this challenge, our key insight is to introduce an empirical cue about gland morphology as extra knowledge to guide the segmentation process. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping. We first leverage the empirical cue to selectively mine out proposals for gland sub-regions with variant appearances. Then, a Morphology-aware Semantic Grouping module is employed to summarize the overall information about the gland by explicitly grouping the semantics of its sub-region proposals. In this way, the final segmentation network could learn comprehensive knowledge about glands and produce well-delineated, complete predictions. We conduct experiments on GlaS dataset and CRAG dataset. Our method exceeds the second-best counterpart over 10.56% at mIOU.",https://github.com/xmed-lab/MSSG,https://warwick.ac.uk/fac/cross_fac/tia/data/mildnet/,Image Segmentation,Histopathology,,,,,,,,
Motion Compensated Unsupervised Deep Learning for 5D MRI ,"We propose an unsupervised deep learning algorithm for the
motion-compensated reconstruction of 5D cardiac MRI data from 3D
radial acquisitions. Ungated free-breathing 5D MRI simplifies the scan
planning, improves patient comfort, and offers several clinical benefits
over breath-held 2D exams, including isotropic spatial resolution and the
ability to reslice the data to arbitrary views. However, the current reconstruction
algorithms for 5D MRI take very long computational time,
and their outcome is greatly dependent on the uniformity of the binning
of the acquired data into different physiological phases. The proposed
algorithm is a more data-efficient alternative to current motion-resolved
reconstructions. This motion-compensated approach models the data in
each cardiac/respiratory bin as Fourier samples of the deformed version
of a 3D image template. The deformation maps are modeled by a convolutional
neural network driven by the physiological phase information.
The deformation maps and the template are then jointly estimated from
the measured data. The cardiac and respiratory phases are estimated
from 1D navigators using an auto-encoder. The proposed algorithm is
validated on 5D bSSFP datasets acquired from two subjects.",https://github.com/joseph-kettelkamp/5D_MRI,https://drive.google.com/drive/folders/1E1FvKVhR1ye_xVW-z9_6E65G_R3kFckq?usp=share_link,MRI,Image Reconstruction,,,,,,,,
MPBD-LSTM: A Predictive Model For Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans ,"Colorectal cancer is a prevalent form of cancer, and many patients develop colorectal cancer liver metastasis (CRLM) as a result. Early detection of CRLM is critical for improving survival rates. Radiologists usually rely on a series of multi-phase contrast-enhanced computed tomography (CECT) scans done during follow-up visits to perform early detection of the potential CRLM. These scans form unique five-dimensional data (time, phase, and axial, sagittal, and coronal planes in 3D CT). Most of the existing deep learning models can readily handle four-dimensional data (e.g., time-series 3D CT images) and it is not clear how well they can be extended to handle the additional dimension of phase. In this paper, we build a dataset of time-series CECT scans to aid in the early diagnosis of CRLM, and build upon state-of-the-art deep learning techniques to evaluate how to best predict CRLM. Our experimental results show that a multi-plane architecture based on 3D bi-directioal LSTM, which we call MPBD-LSTM, works best, achieving an area under curve (AUC) of 0.79. On the other hand, analysis of the results shows that there is still great room for further improvement.",https://github.com/XueyangLiOSU/MPBD-LSTM,https://github.com/XueyangLiOSU/MPBD-LSTM,Other,CT,,,,,,,,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities ,"Multiple imaging modalities are often used for disease diagnosis, prediction, or population-based analyses. However, not all modalities might be available due to cost, different study designs, or changes in imaging technology. If the differences between the types of imaging are small, data harmonization approaches can be used; for larger changes, direct image synthesis approaches have been explored.  In this paper, we develop an approach, MRIS, based on multi-modal metric learning to synthesize images of diverse modalities. We use metric learning via multi-modal image retrieval, resulting in embeddings that can relate images of different modalities. Given a large image database, the learned image embeddings allow us to use k-nearest neighbor regression for image synthesis. Our driving medical problem is knee osteoarthritis, but our developed method is general after proper image alignment. We test our approach by synthesizing cartilage thickness maps obtained from 3D magnetic resonance images using 2D radiographs. Our experiments show that the proposed method outperforms direct image synthesis and that the synthesized thickness maps retain information relevant to downstream tasks such as progression prediction and Kellgren-Lawrence grading. Our results suggest that retrieval approaches can be used to obtain high-quality and meaningful image synthesis results given large image databases.",https://github.com/uncbiag/MRIS,https://nda.nih.gov/oai/,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,other,,,,,,
MSKdeX: Musculoskeletal (MSK) decomposition from an X-ray image for fine-grained estimation of lean muscle mass and muscle volume ,"Musculoskeletal diseases such as sarcopenia and osteoporosis are major obstacles to health during aging. Although dual-energy X-ray absorptiometry (DXA) and computed tomography (CT) can be used to evaluate musculoskeletal conditions, frequent monitoring is difficult due to the cost and accessibility (as well as high radiation exposure in the case of CT). We propose a method (named MSKdeX) to estimate fine-grained muscle properties from a plain X-ray image, a low-cost, low-radiation, and highly accessible imaging modality, through musculoskeletal decomposition leveraging fine-grained segmentation in CT. We train a multi-channel quantitative image translation model to decompose an X-ray image into projections of CT of individual muscles to infer the lean muscle mass and muscle volume. We propose the object-wise intensity-sum loss, a simple yet surprisingly effective metric invariant to muscle deformation and projection direction, utilizing information in CT and X-ray images collected from the same patient. While our method is basically an unpaired image translation, we also exploit the nature of the bone’s rigidity, which provides the paired data through 2D-3D rigid registration, adding strong pixel-wise supervision in unpaired training. Through the evaluation using a 539-patient dataset, we showed that the proposed method significantly outperformed conventional methods. The average Pearson correlation coefficient between the predicted and CT-derived ground truth metrics was increased from 0.424 to 0.857. We believe our method opened up a new musculoskeletal diagnosis method and has the potential to be extended to broader applications in multi-channel quantitative image translation tasks.",,,Musculoskeletal,Computer Aided Diagnosis,Image Reconstruction,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,other,,,,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation ,"The conventional histopathology paradigm can provide the gold standard for clinical diagnosis, which, however, suffers from lengthy processing time and requires costly laboratory equipment. Recent advancements made in deep learning for computational histopathology have sparked lots of efforts in achieving a rapid chemical-free staining technique. Yet, existing approaches are limited to well-prepared thin sections, and invalid in handling more than one stain. In this paper, we present a multiple histological staining model for thick tissues (MulHiST), without any laborious sample preparation, sectioning, and staining process. We use the grey-scale light-sheet microscopy image of thick tissues as model input and transfer it into different histologically stained versions, including hematoxylin and eosin (H&E), Masson’s trichrome (MT), and periodic acid-Schiff (PAS). This is the first work that enables the automatic and simultaneous generation of multiple histological staining for thick biological samples. Moreover, we empirically demonstrate that the AdaIN-based generator offers an advantage over other configurations to achieve higher-quality multi-style image generation. Extensive experiments also indicated that multi-domain data fusion is conducive to the model capturing shared pathological features. We believe that the proposed MulHiST can potentially be applied in clinical rapid pathology and will significantly improve the current histological workflow.",https://github.com/TABLAB-HKUST/MulHiST,,Computational (Integrative) Pathology,Image Reconstruction,Histopathology,,,,,,,
Multi-Head Multi-Loss Model Calibration ,"Delivering meaningful uncertainty estimates is essential for a successful deployment of machine learning models in the clinical practice. A central aspect of uncertainty quantification is the ability of a model to return predictions that are well-aligned with the actual probability of the model being correct, also known as model calibration. Although many methods have been proposed to improve calibration, no technique can match the simple, but expensive approach of training an ensemble of deep neural networks. In this paper we introduce a form of simplified ensembling that bypasses the costly training and inference of deep ensembles, yet it keeps its calibration capabilities. The idea is to replace the common linear classifier at the end of a network by a set of heads that are supervised with different loss functions to enforce diversity on their predictions. Specifically, each head is trained to minimize a weighted Cross-Entropy loss, but the weights are different among the different branches. We show that the resulting averaged predictions can achieve excellent calibration without sacrificing accuracy in two challenging datasets for histopathological and endoscopic image classification. Our experiments indicate that Multi-Head Multi-Loss classifiers are inherently well-calibrated, outperforming other recent calibration techniques and even challenging Deep Ensembles’ performance. Code to reproduce our experiments can be found at \url{https://github.com/witheld}",https://github.com/agaldran/mhml_calibration,https://bupt-ai-cz.github.io/HSA-NRL/,Uncertainty,Interpretability / Explainability,,,,,,,,
Multi-IMU with Online Self-Consistency for Freehand 3D Ultrasound Reconstruction ,"Ultrasound (US) imaging is a popular tool in clinical diagnosis, offering safety, repeatability, and real-time capabilities. Freehand 3D US is a technique that provides a deeper understanding of scanned regions without increasing complexity. However, estimating elevation displacement and accumulation error remains challenging, making it difficult to infer the relative position using images alone. The addition of external lightweight sensors has been proposed to enhance reconstruction performance without adding complexity, which has been shown to be beneficial. We propose a novel online self-consistency network (OSCNet) using multiple inertial measurement units (IMUs) to improve reconstruction performance. OSCNet utilizes a modal-level self-supervised strategy to fuse multiple IMU information and reduce differences between reconstruction results obtained from each IMU data. Additionally, a sequence-level self-consistency strategy is proposed to improve the hierarchical consistency of prediction results among the scanning sequence and its sub-sequences. Experiments on large-scale arm and carotid datasets with multiple scanning tactics demonstrate that our OSCNet outperforms previous methods, achieving state-of-the-art reconstruction performance.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Musculoskeletal,Image Reconstruction,Ultrasound,,,,,,
Multimodal brain age estimation using interpretable adaptive population-graph learning ,"Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer’s. Population graphs, which include multimodal imaging information of the subjects, along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeline can be trained end-to-end. Additionally, by visualizing the attention weights that were the most important for the graph construction, we increase the interpretability of the graph. We use the UK Biobank, which provides a big variety of neuroimaging and non-imaging phenotypes, to evaluate our method on brain age regression and classification. The proposed method outperforms competing static graph approaches and other state-of-the-art adaptive methods. We further show that the assigned attention scores indicate that there are both imaging and non-imaging phenotypes that are informative for brain age estimation and are in agreement with the relevant literature.",https://github.com/bintsi/adaptive-graph-learning,,Imaging Biomarkers,Neuroimaging - Others,Interpretability / Explainability,Other,MRI,,,,,
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk ,"Radiotherapy (RT) is a standard treatment modality for head and neck (HaN) cancer that requires accurate segmentation of target volumes and nearby healthy organs-at-risk (OARs) to optimize radiation dose distribution. However, computed tomography (CT) imaging has low image contrast for soft tissues, making accurate segmentation of soft tissue OARs challenging. Therefore, magnetic resonance (MR) imaging has been recommended to enhance the segmentation of soft tissue OARs in the HaN region. Based on our two empirical observations that deformable registration of CT and MR images of the same patient is inherently imperfect and that concatenating such images at the input layer of a deep learning network cannot optimally exploit the information provided by the MR modality, we propose a novel modality fusion module (MFM) that learns to spatially align MR-based feature maps before fusing them with CT-based feature maps. The proposed MFM can be easily implemented into any existing multimodal backbone network. Our implementation within the nnU-Net framework shows promising results on a dataset of CT and MR image pairs from the same patients. Furthermore, the evaluation on a clinically realistic scenario with the missing MR modality shows that MFM outperforms other state-of-the-art multimodal approaches.",,https://doi.org/10.5281/zenodo.7442914,Image Segmentation,Oncology,Image Registration,Data Efficient Learning,CT,MRI,,,,
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study ,"Multimodal fusion of different types of neural image data offers an invaluable opportunity to leverage complementary cross-modal information and has greatly advanced our understanding of mild cognitive impairment (MCI), a precursor to Alzheimer’s disease (AD). Current multi-modal fusion methods  assume that both brain’s natural geometry and the related feature embeddings are in Euclidean space. However, recent studies have suggested that non-Euclidean hyperbolic space may provide a more accurate interpretation of brain connectomes than Euclidean space. In light of these findings, we propose a novel graph-based hyperbolic deep model with a learnable topology to integrate the individual structural network with functional information in hyperbolic space for the MCI/NC (normal control) classification task. We comprehensively compared the classification performance of the proposed model with state-of-the-art methods and analyzed the feature representation in hyperbolic space and its Euclidean counterparts. The results demonstrate the superiority of the proposed model in both feature representation and classification performance, highlighting the advantages of using hyperbolic space for multimodal fusion in the study of brain diseases. (Code is available here3.)",https://github.com/nasyxx/MDF-HS,,Computer Aided Diagnosis,Neuroimaging - Others,Data Efficient Learning,MRI,,,,,,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis ,"Breast cancer (BC) is one of the most common cancers identified globally among women, which has become the leading cause of death. Multi-modal pathological images contain different information for BC diagnosis. Hematoxylin and eosin (H&E) staining images could reveal a considerable amount of microscopic anatomy. Immunohistochemical (IHC) staining images provide the evaluation of the expression of various biomarkers, such as the human epidermal growth factor receptor 2 (HER2) hybridization. In this paper, we propose a novel multi-modal pre-training model via pathological images for BC diagnosis. The proposed pre-training model contains three modules: (1) the modal-fusion encoder, (2) the mixed attention, and (3) the modal-specific decoders. The pre-trained model could be performed on multiple relevant tasks (IHC Reconstruction and IHC classification). The experiments on two datasets (HEROHE Challenge and BCI Challenge) show state-of-the-art results.",,,Computational (Integrative) Pathology,Histopathology,,,,,,,,
Multi-Modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification ,"Alzheimer’s disease (AD) is an irreversible neurodegenerative disease, so early identification of Alzheimer’s disease and its early stage disorder, mild cognitive impairment (MCI), is of great significance. However, currently available labeled datasets are still small, so the development of semi-supervised classification algorithms will be beneficial for clinical applications. We propose a novel uncertainty-aware semi-supervised learning framework based on the improved evidential regression (ER). Our framework uses the aleatoric uncertainty (AU) from the data itself and the epistemic uncertainty (EU) from the model to optimize the evidential classifier and feature extractor step by step to achieve the best performance close to supervised learning with small labeled data counts. We conducted various experiments on the ADNI-2 dataset, demonstrating the effectiveness and advancement of our method.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Data Efficient Learning,Uncertainty,,,,,,,
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities ,"One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the ‘normal’ behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than a uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect diseased individuals, capture disease severity, and correlate with patient cognition than baseline approaches. We also propose a multivariate latent deviation metric, measuring deviations from the joint latent space, which outperformed feature-based metrics.",https://github.com/alawryaguila/multimodal-normative-models,https://adni.loni.usc.edu/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - DWI and Tractography,Neuroimaging - Others,Interpretability / Explainability,MRI,Treatment Response and Outcome/Disease Prediction,,,,
Multi-modality contrastive learning for sarcopenia screening from hip X-rays and clinical information ,"Sarcopenia is a condition of age-associated muscle degeneration that shortens the life expectancy in those it affects, compared to individuals with normal muscle strength. Accurate screening for sarcopenia is a key process of clinical diagnosis and therapy. In this work, we propose a novel multi-modality contrastive learning (MM-CL) based method that combines hip X-ray images and clinical parameters for sarcopenia screening. Our method captures the long-range information with Non-local CAM Enhancement, explores the correlations in visual-text features via Visual-text Feature Fusion, and improves the model’s feature representation ability through Auxiliary contrastive representation. Furthermore, we establish a large in-house dataset with 1,176 patients to validate the effectiveness of multi-modality based methods. Significant performances with an AUC of 84.64%, ACC of 79.93%, F1 of 74.88%, SEN of 72.06%, SPC of 86.06%, and PRE of 78.44%, show that our method outperforms other single-modality and multi-modality based methods.",https://github.com/qgking/MM-CL.git,,Computer Aided Diagnosis,other,Text (clinical/radiology reports),,,,,,,
Multi-objective point cloud autoencoders for explainable myocardial infarction prediction ,"Myocardial infarction (MI) is one of the most common causes of death in the world. Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart’s 3D anatomy and thus limit diagnostic accuracy. In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function. Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space. Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds. In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images’ pixel resolution. Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve. In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.",,,Interpretability / Explainability,Cardiac,Computational (Integrative) Pathology,Computational Anatomy and Physiology,MRI,Treatment Response and Outcome/Disease Prediction,Visualization in Biomedical Imaging,,,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction ,"Metal artifact reduction (MAR) is important to alleviate the impacts of metal implants on clinical diagnosis with CT images. However, enhancing the quality of metal-corrupted image remains a challenge. Although the deep learning-based MAR methods have achieved impressive success, their interpretability and generalizability need further improvement. It is found that metal artifacts mainly concentrate in high frequency, and their distributions in the wavelet domain are significantly different from those in the image domain. Decomposing metal artifacts into different frequency bands is conducive for us to characterize them. Based on these observations, a model is constructed with dual-domain constraints to encode artifacts by utilizing wavelet transform. To facilitate the optimization of the model and improve its interpretability, a novel multi-perspective adaptive iteration network (MAIN) is proposed. Our MAIN is constructed under the guidance of the proximal gradient technique. Moreover, with the usage of the adaptive wavelet module, the network gains better generalization performance. Compared with the representative state-of-the-art deep learning-based MAR methods, the results show that our MAIN significantly outperforms other methods on both of a synthetic and a clinical datasets.",,,CT,Image Reconstruction,,,,,,,,
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models ,"The success of large-scale pre-trained vision-language models (VLM) has provided a promising direction of transferring natural image representations to the medical domain by providing a well-designed prompt with medical expert-level knowledge. However, one prompt has difficulty in describing the medical lesions thoroughly enough and containing all the attributes. Besides, the models pre-trained with natural images fail to detect lesions precisely. To solve this problem, fusing multiple prompts is vital to assist the VLM in learning a more comprehensive alignment between textual and visual modalities. In this paper, we propose an ensemble guided fusion approach to leverage multiple statements when tackling the phrase grounding task for zero-shot lesion detection. Extensive experiments are conducted on three public medical image datasets across different modalities and the detection accuracy improvement demonstrates the superiority of our method.",,,Transfer learning,Computer Aided Diagnosis,Data Efficient Learning,Other,,,,,,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection ,"Electrocardiogram (ECG) is a widely used diagnostic tool for detecting heart conditions. Rare cardiac diseases may be underdiagnosed using traditional ECG analysis, considering that no training dataset can exhaust all possible cardiac disorders. This paper proposes using anomaly detection to identify any unhealthy status, with normal ECGs solely for training. However, detecting anomalies in ECG can be challenging due to significant inter-individual differences and anomalies present in both global rhythm and local morphology. To address this challenge, this paper introduces a novel multi-scale cross-restoration framework for ECG anomaly detection and localization that considers both local and global ECG characteristics. The proposed framework employs a two-branch autoencoder to facilitate multi-scale feature learning through a masking and restoration process, with one branch focusing on global features from the entire ECG and the other on local features from heartbeat-level details, mimicking the diagnostic process of cardiologists. Anomalies are identified by their high restoration errors. To evaluate the performance on a large number of individuals, this paper introduces a new challenging benchmark with signal point-level ground truths annotated by experienced cardiologists. The proposed method demonstrates state-of-the-art performance on this benchmark and two other well-known ECG datasets. The benchmark dataset and source code are available at: https://github.com/MediaBrain-SJTU/ECGAD",https://github.com/MediaBrain-SJTU/ECGAD,https://physionet.org/content/ptb-xl/1.0.3/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,EEG/ECG,,,,,,,,
Multi-Scale Prototypical Transformer for Whole Slide Image Classification ,"Whole slide image (WSI) classification is an essential task in computational pathology. Despite the recent advances in multiple instance learning (MIL) for WSI classification, accurate classification of WSIs remains challenging due to the extreme imbalance between the positive and negative instances in bags, and the complicated pre-processing to fuse multi-scale information of WSI. To this end, we propose a novel multi-scale prototypical Transformer (MSPT) for WSI classification, which includes a prototypical Transformer (PT) module and a multi-scale feature fusion module (MFFM). The PT is developed to reduce redundant instances in bags by integrating prototypical learning into the Transformer architecture. It substitutes all instances with cluster prototypes, which are then re-calibrated through the self-attention mechanism of the Trans-former. Thereafter, an MFFM is proposed to fuse the clustered prototypes of different scales, which employs MLP-Mixer to enhance the information communication between prototypes. The experimental results on two public WSI datasets demonstrate that the proposed MSPT outperforms all the compared algorithms, suggesting its potential applications.",,TCGA https://portal.gdc.cancer.gov,Histopathology,Computer Aided Diagnosis,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Multi-Scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision ,"Longitudinal lesion or tumor tracking is an essential task in
different clinical workflows, including treatment monitoring with followup
imaging or planning of re-treatments for radiation therapy. Accurately
establishing correspondence between lesions at different timepoints, recognizing
new lesions or lesions that have disappeared is a tedious task
that only grows in complexity as the number of lesions or timepoints
increase. To address this task, we propose a generic approach based on
multi-scale self-supervised learning. The multi-scale approach allows the
efficient and robust learning of a similarity map between multi-timepoint
image acquisitions to derive correspondence, while the self-supervised
learning formulation enables the generic application to different types
of lesions and image modalities. In addition, we impose optional supervision
during training by leveraging tens of anatomical landmarks that
can be extracted automatically. We train our approach at large scale
with more than 50,000 computed tomography (CT) scans and validate
it on two different applications: 1) Tracking of generic lesions based on
the DeepLesion dataset, including liver tumors, lung nodules, enlarged
lymph-nodes, for which we report highest matching accuracy of 92%,
with localization accuracy that is nearly 10% higher than the state-ofthe-
art; and 2) Tracking of lung nodules based on the NLST dataset
for which we achieve similarly high performance. In addition, we include
an error analysis based on expert radiologist feedback, and discuss next
steps as we plan to scale our system across more applications.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Oncology,CT,,,,,,,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image based Cancer Survival Prediction ,"Cancer survival prediction requires considering not only the biological morphology but also the contextual interactions of tumor and surrounding tissues. The major limitation of previous learning frameworks for whole slide image (WSI) based survival prediction is that the contextual interactions of pathological components (e.g., tumor, stroma, lymphocyte, etc.) lack sufficient representation and quantification. In this paper, we proposed a multi-scope analysis driven Hierarchical Graph Transformer (HGT) to overcome this limitation. Specifically, we first utilize a multi-scope analysis strategy, which leverages an in-slide superpixel and a cross-slide clustering, to mine the spatial and semantic priors of WSIs. Furthermore, based on the extracted spatial prior, a hierarchical graph convolutional network is proposed to progressively learn the topological features of the variant microenvironments ranging from patch-level to tissue-level. In addition, guided by the identified semantic prior, tissue-level features are further aggregated to represent the meaningful pathological components, whose contextual interactions are established and quantified by the designed Transformer-based prediction head. We evaluated the proposed framework on our collected Colorectal Cancer (CRC) cohort and two public cancer cohorts from the TCGA project, i.e., Liver Hepatocellular Carcinoma (LIHC) and Kidney Clear Cell Carcinoma (KIRC). Experimental results demonstrate that our proposed method yields superior performance and richer interpretability compared to the state-of-the-art approaches.",https://github.com/Baeksweety/superpixel_transformer,,Computational (Integrative) Pathology,Treatment Response and Outcome/Disease Prediction,,,,,,,,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation ,"Despite the remarkable achievements made by deep convolutional neural networks in medical image segmentation, the limitation that they rely heavily on high-precision and intensively annotated samples makes it difficult to adapt to novel classes that have not been seen before. Few-shot learning is introduced to solve these challenges by learning the generalized representation of a semantic class from very few annotated support samples that can be used as a reference for unannotated query samples. In this paper, instead of averaging multiple support prototypes, we propose a multi-shot prototype contrastive learning and semantic reasoning network (MPSNet) for medical image segmentation. The multi-shot learning network exists independently within the support set, obtains effective semantic features for support images and gives priority to training the core segmentation model of prototype contrastive learning. We also propose a semantic reasoning network that takes the prior semantic features and prior segmentation model learned from the support set as the immediate and necessary conditions for the query image to deduce its segmentation mask. The proposed method is verified to be superior to the state-of-the-art methods on three public datasets, revealing its powerful segmentation and generalization abilities. Code: https://github.com/H51705/FSS_MPSNet.",https://github.com/H51705/FSS_MPSNet,,Image Segmentation,Abdomen,Cardiac,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI,,,,
MultiTalent: A Multi-Dataset Approach to Medical Image Segmentation ,"The medical imaging community generates a wealth of data-
sets, many of which are openly accessible and annotated for specific
diseases and tasks such as multi-organ or lesion segmentation. Current
practices continue to limit model training and supervised pre-training
to one or a few similar datasets, neglecting the synergistic potential of
other available annotated data. We propose MultiTalent, a method that
leverages multiple CT datasets with diverse and conflicting class defini-
tions to train a single model for a comprehensive structure segmenta-
tion. Our results demonstrate improved segmentation performance com-
pared to previous related approaches, systematically, also compared to
single-dataset training using state-of-the-art methods, especially for le-
sion segmentation and other challenging structures. We show that Mul-
tiTalent also represents a powerful foundation model that offers a su-
perior pre-training for various segmentation tasks compared to com-
monly used supervised or unsupervised pre-training baselines. Our find-
ings offer a new direction for the medical imaging community to ef-
fectively utilize the wealth of available data for improved segmenta-
tion performance. The code and model weights will be published here:
https://github.com/MIC-DKFZ/MultiTalent",https://github.com/MIC-DKFZ/MultiTalent,http://medicaldecathlon.com/,Image Segmentation,Abdomen,Lung,Oncology,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,CT,,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation ,"Domain shift is a big challenge when deploying deep learning models in real-world applications due to various data distributions. The recent advances of domain adaptation mainly come from explicitly learning domain invariant features (e.g., by adversarial learning, metric learning and self-training). While they cannot be easily extended to multi-domains due to the diverse domain knowledge. In this paper, we present a novel multi-target domain adaptation (MTDA) algorithm, i.e., prompt-DA, through implicit feature adaptation for medical image segmentation. In particular, we build a feature transfer module by simply obtaining the domain-specific prompts and utilizing them to generate the domain-aware image features via a specially designed simple feature fusion module. Moreover, the proposed prompt-DA is compatible with the previous DA methods (e.g., adversarial learning based) and the performance can be continuously improved. The proposed method is evaluated on two challenging domain-shift datasets, i.e., the Iseg2019 (domain shift in infant MRI of different ages), and the BraTS2018 dataset (domain shift between high-grade and low-grade gliomas). Experimental results indicate our proposed method achieves state-of-the-art performance in both cases, and also demonstrate the effectiveness of the proposed prompt-DA. The experiments with adversarial learning DA show our proposed prompt-DA can go well with other DA methods. Our code is available at https://github.com/MurasakiLin/prompt-DA.",https://github.com/MurasakiLin/prompt-DA,,Transfer learning,Image Segmentation,MRI,,,,,,,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development ,"During the early postnatal period, the human brain undergoes rapid and dynamic development. Over the past decades, there has been increased attention in studying the cognitive and cortical development of infants. However, accurate prediction of the infant cognitive and cortical development at an individual-level is a significant challenge, due to the huge complexities in highly irregular and incomplete longitudinal data that is commonly seen in current studies. Besides, joint prediction of cognitive scores and cortical morphology is barely investigated, despite some studies revealing the tight relationship between cognitive ability and cortical morphology and suggesting their potential mutual benefits. To tackle this challenge, we develop a flexible multi-task framework for joint prediction of cognitive scores and cortical morphological maps, namely, disentangled intensive triplet spherical adversarial autoencoder (DITSAA). First, we extract the mixed representative latent vector through a triplet spherical ResNet and further disentangles latent vector into identity-related and age-related features with an attention-based module. The identity recognition and age estimation tasks are introduced as supervision for a reliable disentanglement of the two components. Then we formulate the individualized cortical profile at a specific age by combining disentangled identity-related information and corresponding age-related information. Finally, an adversarial learning strategy is integrated to achieve a vivid and realistic prediction of cortical morphology, while a cognitive module is employed to predict cognitive scores. Extensive experiments are conducted on a public dataset, and the results affirm our method’s ability to predict cognitive scores and cortical morphology jointly and flexibly using incomplete longitudinal data.",,,MRI,Guided Interventions and Surgery,,,,,,,,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma ,"Most recently, the histopathology diagnosis of cancer is shifting to integrating genomic makers with histology. It is a urgent need for digital pathology methods to effectively integrate genomics with histology, which could lead to more accurate diagnosis in the real world scenarios. This paper is a first attempt to integrate genomics with histoimics and model their interactions for classifying diffuse glioma bases on whole slide images. Specifically, we propose a hierarchical multi-task multi-instance learning framework based on histopathological data to jointly predict histology and genomics. Moreover, we propose a co-occurrence probability-based label correction graph network to the co-occurrence of genomic markers. Lastly, we design an inter-omic interaction strategy with the dynamical confidence constraint loss to model the interactions of histomics and genomics. Our experiments show that our method outperforms other state-of-the-art methods in classifying diffuse glioma, 	as well as related genomics and histology on a multi-institutional dataset.",,https://portal.gdc.cancer.gov/,Oncology,Computational (Integrative) Pathology,Computer Aided Diagnosis,Imaging Biomarkers,,,,,,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-temporal Correspondence ,"This paper proposes a novel self-supervised monocular depth estimation approach for laparoscopic scenes. Previous methods independently predicted depth maps ignoring spatial coherence in local regions and temporal correlation between adjacent images. The proposed approach leverages spatio-temporal coherence to address the challenges of textureless areas and homogeneous colors in such scenes. This approach utilizes a multi-view depth estimation model to guide monocular depth estimation when predicting depth maps. Moreover, the minimum reprojection error is extended to construct a cost volume for the multi-view model using adjacent images. A cycled prediction learning for view synthesis and relative poses is also designed to exploit the temporal correlation between adjacent images fully. To benefit from spatial coherence, deformable patch-matching is introduced to the monocular and multi-view models to smooth depth maps in local regions. Additionally, the 3D consistency of the point cloud back-projected from predicted depth maps is optimized for the monocular depth estimation model. Experimental results show that the proposed method outperforms existing methods in both qualitative and quantitative evaluations.",https://github.com/MoriLabNU/MGMDepthL,,Guided Interventions and Surgery,Image Reconstruction,Interventional Imaging Systems,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Surgical Scene Understanding,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,
Multi-View Vertebra Localization and Identification from CT Images ,"Accurately localizing and identifying vertebra from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information.In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views.Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally.Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone.Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae.Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently.",https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images,https://s3.bonescreen.de/public/VerSe-complete/dataset-verse19training.zip,Computer Aided Diagnosis,CT,,,,,,,,
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis ,"Early diagnosis of renal cancer can greatly improve the survival rate of patients. Contrast-enhanced ultrasound (CEUS) is a cost-effective and non-invasive imaging technique and has become more and more frequently used for renal tumor diagnosis. However, the classification of benign and malignant renal tumors can still be very challenging due to the highly heterogeneous appearance of cancer and imaging artifacts. Our aim is to detect and classify renal tumors by integrating B-mode and CEUS-mode ultrasound videos. To this end, we propose a novel multi-modal ultrasound video fusion network that can effectively perform multi-modal feature fusion and video classification for renal tumor diagnosis. The attention-based multi-modal fusion module uses cross-attention and self-attention to extract modality-invariant features and modality-specific features in parallel. In addition, we design an object-level temporal aggregation (OTA) module that can automatically filter low-quality features and efficiently integrate temporal information from multiple frames to improve the accuracy of tumor diagnosis. Experimental results on a multicenter dataset show that the proposed framework outperforms the single-modal models and the competing methods. Furthermore, our OTA module achieves higher classification accuracy than the frame-level predictions.",,,Computer Aided Diagnosis,Oncology,Attention models,Other,Ultrasound,Video,,,,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models ,"In recent years, computational pathology has seen tremendous progress driven by deep learning methods in segmentation and classification tasks aiding prognostic and diagnostic settings. Nuclei segmentation, for instance, is an important task for diagnosing different cancers. However, training deep learning models for nuclei segmentation requires large amounts of annotated data, which is expensive to collect and label. This necessitates explorations into generative modeling of histopathological images. In this work, we use recent advances in conditional diffusion modeling to formulate a first-of-its-kind nuclei-aware semantic tissue generation framework (NASDM) which can synthesize realistic tissue samples given a semantic instance mask of up to six different nuclei types, enabling pixel-perfect nuclei localization in generated samples. These synthetic images are useful in applications in pathology pedagogy, validation of models, and supplementation of existing nuclei segmentation datasets. We demonstrate that NASDM is able to synthesize high-quality histopathology images of the colon with superior quality and semantic controllability over existing generative methods.",https://github.com/4m4n5/NASDM,https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/,Computational (Integrative) Pathology,Image Reconstruction,Other,Histopathology,,,,,,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues ,"Reconstructing deformable tissues from endoscopic stereo videos in robotic surgery is crucial for various clinical applications. However, existing methods relying only on implicit representations are computationally expensive and require dozens of hours, which limits further practical applications.
To address this challenge, we introduce LerPlane, a novel method for fast and accurate reconstruction of surgical scenes under a single-viewpoint setting. 
LerPlane treats surgical procedures as 4D volumes and factorizes them into explicit 2D planes of static and dynamic fields, resulting in a compact memory footprint and significantly accelerated optimization. 
The efficient factorization is accomplished by fusing features obtained through linear interpolation of each plane and allows us to use lightweight neural networks to model surgical procedures. Besides, LerPlane shares static fields, significantly reducing the workload of dynamic tissue modeling.
We also propose a novel sample strategy to boost optimization and improve rendering quality in regions with tool occlusion and large motions. Our experiments on DaVinci robotic surgery videos demonstrate that LerPlane accelerates surgical scene optimization by over 100$\times$ while maintaining high quality across various non-rigid deformations, showing significant promise for future intraoperative surgery applications.",https://github.com/Loping151/LerPlane,,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,,,,,,
Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI Pre-processing ,"Head MRI pre-processing involves converting raw images to an intensity-normalized, skull-stripped brain in a standard coordinate space. In this paper, we propose an end-to-end weakly supervised learning approach, called Neural Pre-processing (NPP), for solving all three sub-tasks simultaneously via a neural network, trained on a large dataset without individual sub-task supervision. Because the overall objective is highly under-constrained, we explicitly disentangle geometric-preserving intensity mapping (skull-stripping and intensity normalization) and spatial transformation (spatial normalization). Quantitative results show that our model outperforms state-of-the-art methods which tackle only a single sub-task. Our ablation experiments demonstrate the importance of the architecture design we chose for NPP. Furthermore, NPP affords the user the flexibility to control each of these tasks at inference time. 
The code and model are freely-available at https://github.com/Novestars/Neural-Pre-processing.",https://github.com/Novestars/Neural_Pre_Processing,,Neuroimaging - Others,MRI,,,,,,,,
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants ,"In addition to model accuracy, current neuroimaging studies require more explainable model outputs to relate brain development, degeneration, or disorders to uncover atypical local alterations. For this purpose, existing approaches typically explicate network outputs in a post-hoc fashion. However, for neuroimaging data with high dimensional and redundant information, end-to-end learning of explanation factors can inversely assure fine-grained explainability while boosting model accuracy. Meanwhile, most methods only deal with gridded data and do not support brain cortical surface-based analysis. In this paper, we propose an explainable geometric deep network, the NeuroExplainer, with applications to uncover altered infant cortical development patterns associated with preterm birth. Given fundamental cortical attributes as network input, our NeuroExplainer adopts a hierarchical attention-decoding framework to learn fine-grained attention and respective discriminative representations in a spherical space to accurately recognize preterm infants from term-born infants at term-equivalent age. NeuroExplainer learns the hierarchical attention-decoding modules under subject-level weak supervision coupled with targeted regularizers deduced from domain knowledge regarding brain development. These prior-guided constraints implicitly maximize the explainability metrics (i.e., fidelity, sparsity, and stability) in network training, driving the learned network to output detailed explanations and accurate classifications. Experimental results on the public dHCP benchmark suggest that NeuroExplainer led to quantitatively reliable explanation results that are qualitatively consistent with representative neuroimaging studies.",https://github.com/ladderlab-xjtu/NeuroExplainer,http://www.developingconnectome.org/,Interpretability / Explainability,Neuroimaging - Brain Development,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,
NISF: Neural Implicit Segmentation Functions ,"Segmentation of anatomical shapes from medical images has taken an important role in the automation of clinical measurements. While typical deep-learning segmentation approaches are performed on discrete voxels, the underlying objects being analysed exist in a real-valued continuous space. Approaches that rely on convolutional neural networks (CNNs) are limited to grid-like inputs and not easily applicable to sparse or partial measurements. We propose a novel family of image segmentation models that tackle many of CNNs’ shortcomings: Neural Implicit Segmentation Functions (NISF). Our framework takes inspiration from the field of neural implicit functions where a network learns a mapping from a real-valued coordinate-space to a shape representation. NISFs have the ability to segment anatomical shapes in high-dimensional continuous spaces. Training is not limited to voxelized grids, and covers applications with sparse and partial data. Interpolation between observations is learnt naturally in the training procedure and requires no post-processing. Furthermore, NISFs allow the leveraging of learnt shape priors to make predictions for regions outside of the original image plane. We go on to show the framework achieves dice scores of 0.87 ± 0.045 on a (3D+t) short-axis cardiac segmentation task using the UK Biobank dataset. We also provide a qualitative analysis on our frameworks ability to perform segmentation and image interpolation on unseen regions of an image volume at arbitrary resolutions.",https://github.com/NILOIDE/Implicit_segmentation,https://www.ukbiobank.ac.uk/,Image Segmentation,Cardiac,Image Reconstruction,Other,MRI,,,,,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising ,"Deep neural networks have been extensively studied for denoising low-dose computed tomography (LDCT) images, but some challenges related to robustness and generalization still need to be addressed. It is known that CNN-based denoising methods perform optimally when all the training and testing images have the same noise variance, but this assumption does not hold in the case of LDCT denoising. As the variance of the CT noise varies depending on the tissue density of the scanned organ, CNNs fails to perform at their full capacity. To overcome this limitation, we propose a novel noise-conditioned feature modulation layer that scales the weight matrix values of a particular convolutional layer based on the noise level present in the input signal. This technique creates a neural network that is conditioned on the input image and can adapt to varying noise levels. Our experiments on two public benchmark datasets show that the proposed dynamic convolutional layer significantly improves the denoising performance of the baseline network, as well as its robustness and generalization to previously unseen noise levels.",,,Image Reconstruction,Model Generalizability / Federated Learning,CT,,,,,,,
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT ,"Respiratory Correlated Cone Beam Computed Tomography (4DCBCT) is a technique used to address respiratory motion artifacts that affect reconstruction quality, especially for the thorax and upper-abdomen. 4DCBCT sorts the acquired projection images in multiple respiratory correlated bins. This technique results in the emergence of aliasing artifacts caused by the low number of projection images per bin, which severely impacts the image quality and limits downstream use. Previous attempts to address this problem relied on traditional algorithms, while only recently deep learning techniques are being employed. 
In this work, we propose Noise2Aliasing, which reduces both view-aliasing and statistical noise present in 4DCBCT scans. Using a fundamental property of the FDK reconstruction algorithm, and prior results from the literature, we prove mathematically the ability of the method to work and specify the underlying assumptions. 
We apply the method to a public dataset and to an in-house dataset and show that it matches the performance of a supervised approach and outperforms it when measurement noise is present in the data.",,,Image Reconstruction,Oncology,Other,,,,,,,
Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration ,"Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address this limitation for their capabilities to capture long-range dependency, but the benefits of using transformers for NICE registration have not been explored. In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE- Trans) for image registration. Our NICE-Trans is the first deep registration method that (i) performs joint affine and deformable coarse-to-fine registration within a single network, and (ii) embeds transformers into a NICE registration framework to model long-range relevance between images. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.",https://github.com/MungoMeng/Registration-NICE-Trans,https://adni.loni.usc.edu/,Image Registration,,,,,,,,,
Nonuniformly Spaced Control Points based on Variational Cardiac Image Registration ,"Non-uniformly spaced control points located on the interface of different objects are beneficial for constructing an accurate displacement field for image registration. However, extracting features of non-uniformly spaced control points in images is challenging for convolutional neural networks (CNNs). We extend a probabilistic image registration model using uniformed-spaced control points by employing non-uniformly-spaced control points. We construct a network to extract the image and spatial features of non-uniformly-spaced control points. Moreover, a variational Bayesian (VB) model using a factorized prior is employed to estimate the distribution of latent variables. In theory, we analyze the KL divergence between the posterior and the two separated priors. We found that the factorized prior has the advantage of decreasing the KL divergence, but too more factorized priors, such as the standard normal, might deteriorate registration accuracy. Moreover, we analyze the relationship between the uncertainty of the displacement field and the spatial distribution of control points. Experimental results on four public datasets show that our network outperforms the state-of-arts registration networks and can provide registration uncertainty.",,,Image Registration,Cardiac,Uncertainty,MRI,,,,,,
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition ,"Many medical ultrasound video recognition tasks involve identifying key anatomical features regardless of when they appear in the video suggesting that modeling such tasks may not benefit from temporal features. Correspondingly, model architectures that exclude temporal features may have better sample efficiency. We propose a novel multi-head attention architecture that incorporates these hypotheses as inductive priors to achieve better sample efficiency on common ultrasound tasks. We compare the performance of our architecture to an efficient 3D CNN video recognition model in two settings: one where we expect not to require temporal features and one where we do. In the former setting, our model outperforms the 3D CNN – especially when we artificially limit the training data. In the latter, the outcome reverses. These results suggest that expressive time-independent models may be more effective than state-of-the-art video recognition models for some common ultrasound tasks in the low-data regime. Code is available at https://github.com/MedAI-Clemson/pda_detection.",https://github.com/MedAI-Clemson/pda_detection,,Data Efficient Learning,Cardiac,Computer Aided Diagnosis,Attention models,Interpretability / Explainability,Ultrasound,Video,,,
One-shot Federated Learning on Medical Data using Knowledge Distillation with Image Synthesis and Client Model Adaptation ,"One-shot federated learning (FL) has emerged as a promising solution in scenarios where multiple communication rounds are not practical. Notably, as feature distributions in medical data are less discriminative than those of natural images, robust global model training with FL is non-trivial and can lead to overfitting. To address this issue, we propose a novel one-shot FL framework leveraging Image Synthesis and Client model Adaptation (FedISCA) with knowledge distillation (KD). To prevent overfitting, we generate diverse synthetic images ranging from random noise to realistic images. This approach (i) alleviates data privacy concerns and (ii) facilitates robust global model training using KD with decentralized client models. To mitigate domain disparity in the early stages of synthesis, we design noise-adapted client models where batch normalization statistics on random noise (synthetic images) are updated to enhance KD. Lastly, the global model is trained with both the original and noise-adapted client models via KD and synthetic images. This process is repeated till global model convergence. Extensive evaluation of this design on five small- and three large-scale medical image classification datasets reveals superior accuracy over prior methods. Code is available at https://github.com/myeongkyunkang/FedISCA.",https://github.com/myeongkyunkang/FedISCA,,Model Generalizability / Federated Learning,Computer Aided Diagnosis,,,,,,,,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification ,"Brain segmentation of patients with severe traumatic brain injuries (sTBI) is essential for clinical treatment, but fully-supervised segmentation is limited by the lack of annotated data. One-shot segmentation based on learned transformations (OSSLT) has emerged as a powerful tool to overcome the limitations of insufficient training samples, which involves learning spatial and appearance transformations to perform data augmentation, and learning segmentation with augmented images. However, current practices face challenges in the limited diversity of augmented samples and the potential label error introduced by learned transformations. In this paper, we propose a novel one-shot traumatic brain segmentation method that surpasses these limitations by adversarial training and uncertainty rectification. The proposed method challenges the segmentation by adversarial disturbance of augmented samples to improve both the diversity of augmented data and the robustness of segmentation. Furthermore, potential label error introduced by learned transformations is rectified according to the uncertainty in segmentation. We validate the proposed method by the one-shot segmentation of consciousness-related brain regions in traumatic brain MR scans. Experimental results demonstrate that our proposed method has surpassed state-of-the-art alternatives.",https://github.com/hsiangyuzhao/TBIOneShot,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,MRI,,,,,,
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification ,"Active learning (AL) is an effective approach to select the most informative samples to label so as to reduce the annotation cost. Existing AL methods typically work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model. However, in some practical clinical tasks, the unlabeled pool may contain not only the target classes that need to be fine-grainedly classified, but also non-target classes that are irrelevant to the clinical tasks. Existing AL methods cannot work well in this scenario because they tend to select a large number of non-target samples. In this paper, we formulate this scenario as an open-set AL problem and propose an efficient framework, OpenAL, to address the challenge of querying samples from an unlabeled pool with both target class and non-target class samples. Experiments on fine-grained classification of pathology images show that OpenAL can significantly improve the query quality of target class samples and achieve higher performance than current state-of-the-art AL methods. Codes will be available.",,,Active Learning,Computational (Integrative) Pathology,Histopathology,,,,,,,
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models ,"Medical Visual Question Answering (VQA) is an important challenge, as it would lead to faster and more accurate diagnoses and treatment decisions. Most existing methods approach it as a multi-class classification problem, which restricts the outcome to a predefined closed-set of curated answers. We focus on open-ended VQA and motivated by the recent advances in language models consider it as a generative task. Leveraging pre-trained language models, we introduce a novel method  particularly suited for small, domain-specific, medical datasets. To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens. Then, alongside the question, these learnable tokens directly prompt the language model. We explore recent parameter-efficient fine-tuning strategies for language models, which allow for resource- and data-efficient fine-tuning.  We evaluate our approach on the prime medical VQA benchmarks, namely, Slake, OVQA and PathVQA. The results demonstrate that our approach outperforms existing methods across various training settings while also being computationally efficient.",https://github.com/tjvsonsbeek/open-ended-medical-vqa,,Text (clinical/radiology reports),Other,Transfer learning,,,,,,,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue ,"Compression-based optical coherence elastography (OCE) enables characterization of soft tissue by estimating elastic properties. However, previous probe designs have been limited to surface applications. We propose a bevel tip OCE needle probe for percutaneous insertions, where biomechanical characterization of deep tissue could enable precise needle placement, e.g., in prostate biopsy. We consider a dual-fiber OCE needle probe that provides estimates of local strain and load at the tip. Using a novel setup, we simulate deep tissue indentations where frictional forces and bulk sample displacement can affect biomechanical characterization. Performing surface and deep tissue indentation experiments, we compare our approach with external force and needle position measurements at the needle shaft. We consider two tissue mimicking materials simulating healthy and cancerous tissue and demonstrate that our probe can be inserted into deep tissue layers. Compared to surface indentations, external force-position measurements are strongly affected by frictional forces and bulk displacement and show a relative error of 49.2% and 42.4% for soft and stiff phantoms, respectively. In contrast, quantitative OCE measurements show a reduced relative error of 26.4% and 4.9% for deep indentations of soft and stiff phantoms, respectively. Finally, we demonstrate that the OCE measurements can be used to effectively discriminate the tissue mimicking phantoms.",,,Guided Interventions and Surgery,Oncology,Computer Aided Diagnosis,Interventional Imaging Systems,Medical Robotics and Haptics,Biophotonics,,,,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study ,"An abdominal aortic aneurysm (AAA) is a persistent localized dilatation of the aorta to more than 1.5 times the expected diameter, which may lead to rupture with resultant high mortality. Endovascular repair (EVAR) of AAAs is a mini-mally invasive procedure that involves the peripheral delivery of one or more covered endografts to the aneurysmal segment, via a catheter-based system. A particularly challenging group of patients to treat are those in which the aneurys-mal sac extends proximally to include the origin of the renal arteries (15% of all AAAs). To maintain the patency of renal side branches in these “complex” cases, in situ fenestration (ISF) of endografts during AAA procedures has been pro-posed. The challenges addressed in this study were a) to develop an endovascular imaging system for visualizing side branches beyond deployed endografts and thereby to determine the locations for ISF; b) to obtain an initial assessment of the clinical utility of this system. Here, all-optical ultrasound (OpUS) imaging with a fiber optic transducer was used for real-time guidance, wherein ultrasonic pulses are generated in nanocomposite coatings via the photoacoustic effect and received optically using a Fabry-Perot cavity. These custom OpUS transducer components were integrated into a steerable sheath (6 Fr) that also included a separate optical fiber for delivering laser pulses for fenestrating the endograft. In an ex-vivo mod-el, it was shown that OpUS imaging extended through the endograft and underly-ing aortic tissue, and permitted aortic side-branch visualization. During an EVAR procedure in a porcine model in vivo, an aortic side branch was visualized with OpUS imaging after the endograft was deployed and optical fenestration of the stent graft was successfully performed. This study showed that OpUS is a prom-ising modality for guiding EVAR and could find particularly utility with identify-ing aortic side branches for ISF during treatment of complex AAAs.",,,Guided Interventions and Surgery,Vascular,Interventional Imaging Systems,Ultrasound,,,,,,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures ,"To treat bone fractures, implant manufacturers produce 2D anatomically contoured plates. Unfortunately, existing plates only fit a limited segment of the population and/or require manual bending during surgery. Patient-specific implants would provide major benefits such as reducing surgery time and improving treatment outcomes but they are still rare in clinical practice. In this work, we propose a patient-specific design for the long helical 2D PHILOS (Proximal Humeral Internal Locking System) plate, used to treat humerus shaft fractures. Our method automatically creates a custom plate from a CT scan of a patient’s bone. We start by designing an optimal plate on a template bone and, with an anatomy-aware registration method, we transfer this optimal design to any bone. In addition, for an arbitrary bone, our method assesses if a given plate is fit for surgery by automatically positioning it on the bone. We use this process to generate a compact set of plate shapes capable of fitting the bones within a given population. This plate set can be pre-printed in advance and readily available, removing the fabrication time between the fracture occurrence and the surgery. Extensive experiments on ex-vivo arms and 3D-printed bones show that the generated plate shapes (personalized and plate-set) faithfully match the individual bone anatomy and are suitable for clinical practice.",https://humerusplate.is.tue.mpg.de/,,Musculoskeletal,Computational Anatomy and Physiology,Image Registration,CT,Surgical Planning and Simulation,,,,,
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping ,"Glioblastoma (GBM) is the most aggressive malignant brain tumor. Its poor survival rate highlights the pressing need to adopt easily accessible, non-invasive neuroimaging techniques to preoperatively predict GBM survival, which can benefit treatment planning and patient care. MRI and MRI-based radiomics, although effective for survival prediction, do not consider brain’s functional alternations caused by tumors, which are clinically significant for guiding therapeutic strategies aimed at inhibiting tumor-brain communication. In this paper, we propose an augmented lesion network mapping (A-LNM) based survival prediction framework, where a novel neuroimaging feature family, called functional lesion network (FLN) maps generated by the A-LNM, is achieved from patients’ structural MRI, and thus are more readily available than functional connections measured with functional MRI of patients. Specifically, for each patient, the A-LNM first estimates functional disconnection (FDC) maps by embedding the lesion (the whole tumor) into an atlas of functional connections in a large cohort of healthy subjects, and many FLN maps are then obtained by averaging subsets of the FDC maps such that we can artificially boost data volume (i.e., FLN maps), which helps to mitigate over-fitting and improve survival prediction performance when learning a deep neural network from a small sized dataset. The augmented FLN maps are finally fed to a 3D ResNet-based backbone followed by the average pooling operation and fully-connected layers for GBM survival prediction. Experimental results on the BraTS $2020$ training dataset demonstrate the effectiveness of our proposed framework with the A-LNM derived FLN maps for GBM survival classification. Moreover, we identify the survival-relevant brain regions that can be traced back with biological interpretability.",,,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,Treatment Response and Outcome/Disease Prediction,,,,,
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists ,"Lung cancer is a leading cause of death worldwide and early screening is critical for improving survival outcomes. In clinical practice, the contextual structure of nodules and the accumulated experience of radiologists are the two core elements related to the accuracy of identification of benign and malignant nodules. Contextual information provides comprehensive information about nodules such as location, shape, and peripheral vessels, and experienced radiologists can search for clues from previous cases as a reference to enrich the basis of decision-making. In this paper, we propose a radiologist-inspired method to simulate the diagnostic process of radiologists, which is composed of context parsing and prototype recalling modules. The context parsing module first segments the context structure of nodules and then aggregates contextual information for a more comprehensive understanding of the nodule. The prototype recalling module utilizes prototype-based learning to condense previously learned cases as prototypes for comparative analysis, which is updated online in a momentum way during training. Building on the two modules, our method leverages both the intrinsic characteristics of the nodules and the external knowledge accumulated from other nodules to achieve a sound diagnosis. To meet the needs of both low-dose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.",,,Computer Aided Diagnosis,Lung,CT,,,,,,,
Partial Vessels Annotation-based Coronary Artery Segmentation with Self-training and Prototype Learning ,"Coronary artery segmentation on coronary-computed tomography angiography (CCTA) images is crucial for clinical use. Due to the expertise-required and labor-intensive annotation process, there is a growing demand for the relevant label-efficient learning algorithms. To this end, we propose partial vessels annotation (PVA) based on the challenges of coronary artery segmentation and clinical diagnostic characteristics. Further, we propose a progressive weakly supervised learning framework to achieve accurate segmentation under PVA. First, our proposed framework learns the local features of vessels to propagate the knowledge to unlabeled regions. Subsequently, it learns the global structure by utilizing the propagated knowledge, and corrects the errors introduced in the propagation process. Finally, it leverages the similarity between feature embeddings and the feature prototype to enhance testing outputs. Experiments on clinical data reveals that our proposed framework outperforms the competing methods under PVA (24.29% vessels), and achieves comparable performance in trunk continuity with the baseline model using full annotation (100% vessels).",https://github.com/ZhangZ7112/PVA-CAS,,Data Efficient Learning,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
Partially Supervised Multi-Organ Segmentation via Affinity-aware Consistency Learning and Cross Site Feature Alignment ,"Partially Supervised Multi-Organ Segmentation (PSMOS) has attracted increasing attention. However, facing with challenges from lacking sufficiently labeled data and cross-site data discrepancy, PSMOS remains largely an unsolved problem. In this paper, to fully take advantage of the unlabeled data, we propose to incorporate voxel-to-organ affinity in embedding space into a consistency learning framework, ensuring consistency in both label space and latent feature space. Furthermore, to mitigate the cross-site data discrepancy, we propose to propagate the organ-specific feature centers and inter-organ affinity relationships across different sites, calibrating the multi-site feature distribution from a statistical perspective. Extensive experiments manifest that our method generates favorable results compared with other state-of-the-art methods, especially on hard organs with relatively smaller sizes.",,,Data Efficient Learning,Abdomen,Image Segmentation,CT,,,,,,
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-branch Network ,"In recent years, the emergence and rapid spread of multi-drug resistant bacteria has become a serious threat to global public health. Antibiotic susceptibility testing (AST) is used clinically to determine the susceptibility of bacteria to antibiotics, thereby guiding physicians in the rational use of drugs as well as slowing down the process of bacterial resistance. However, traditional phenotypic AST methods based on bacterial culture are time-consuming and laborious (usually 24-72 hours). Because delayed identification of drug-resistant bacteria increases patient morbidity and mortality, there is an urgent clinical need for a rapid AST method that allows physicians to prescribe ap-propriate antibiotics promptly. In this paper, we present a parallel dual-branch network (i.e, PAS-Net) to predict bacterial antibiotic susceptibility from fluorescent images. Specifically, we use the feature interaction unit (FIU) as a connecting bridge to align and fuse the local features from the convolutional neural network (CNN) branch (C-branch) and the global repre-sentations from the Transformer branch (T-branch) interactively and effectively. Moreover, we propose a new hierarchical multi-head self-attention (HMSA) module that reduces the computational overhead while maintaining the global relationship modeling capability of the T-branch. PAS-Net is experimented on a fluorescent image dataset of clinically isolated Pseudomonas aeruginosa (PA) with promising prediciton performance. Also, we verify the generalization performance of our algorithm in fluorescence image classification on two HEp-2 cell public datasets.",,,Computer Aided Diagnosis,Microscopy,,,,,,,,
Path-based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis ,"Brain functional connectivity analysis is important for understanding brain development, aging, sexual distinction and brain disorders. Existing methods typically adopt the resting-state functional connectivity (rs-FC) measured by functional MRI as an effective tool, while they either neglect the importance of information exchange between different brain regions or the heterogeneity of brain activities. To address these issues, we propose a Path-based Heterogeneous Brain Transformer Network (PH-BTN) for analyzing rs-FC. Specifically, to integrate the path importance and heterogeneity of rs-FC for a comprehensive description of the brain, we first construct the brain functional network as a path-based heterogeneous graph using prior knowledge and gain initial edge features from rs-FC. Then, considering the constraints of graph convolution in aggregating long-distance and global information, we design a Heterogeneous Path Graph Transformer Convolution (HP-GTC) module to extract edge features by aggregating different paths’ information. Furthermore, we adopt Squeeze-and-Excitation (SE) with HP-GTC modules, which can alleviate the over-smoothing problem and enhance influential features. Finally, we apply a readout layer to generate the final graph embedding to estimate brain age and gender, and thoroughly evaluate the PH-BTN on the Baby Connectome Project (BCP) dataset. Experimental results demonstrate the superiority of PH-BTN over other state-of-the-art methods. The proposed PH-BTN offers a powerful tool to investigate and explore brain functional connectivity.",,,Neuroimaging - Functional Brain Networks,Neuroimaging - Brain Development,Attention models,Other,MRI,,,,,
Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction ,"Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at https://github.com/Cassie07/PathOmics.",https://github.com/Cassie07/PathOmics,,Imaging Biomarkers,Computational (Integrative) Pathology,Computer Aided Diagnosis,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis ,"In current pathology image classification, methods mostly rely on patch-based multi-instance learning (MIL), which only considers the relationship between patches and slides. However, in clinical medicine, doctors use slide-level labels to summarize patient-level labels as a diagnostic result, indicating the involvement of three levels of patch, slide, and patient in actual pathology image analysis, which we refer to as the multi-level multi-instance learning (ML-MIL) problem. To address this issue, we propose a novel and general framework called Patients and Slides are Equal (P&SrE), inspired by the doctor’s diagnostic process of repeatedly confirming labels at the patient and slide level. In this framework, we treat patients and slides as instances at the same level and use transformers and attention mechanisms to build connections between them. This allows for interaction between patient-level and slide-level information and the correction of their respective features to achieve better classification performance. We evaluate our method on two datasets using two state-of-the-art MIL methods as baselines. The results show that our method improves the performance of the baselines on both slide and patient levels. Our method provides a simple and effective solution to the common problem of ML-MIL in medical clinical scenarios and has broad potential applications.",,,Computer Aided Diagnosis,Computational (Integrative) Pathology,Treatment Response and Outcome/Disease Prediction,,,,,,,
PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction ,"T1 mapping is a quantitative magnetic resonance imaging
(qMRI) technique that has emerged as a valuable tool in the diagnosis of diffuse myocardial diseases. However, prevailing approaches have
relied heavily on breath-hold sequences to eliminate respiratory motion
artifacts. This limitation hinders accessibility and effectiveness for patients who cannot tolerate breath-holding. Image registration can be
used to enable free-breathing T1 mapping. Yet, inherent intensity differences between the different time points make the registration task challenging. We introduce PCMC-T1, a physically-constrained deep-learning
model for motion correction in free-breathing T1 mapping. We incorporate the signal decay model into the network architecture to encourage
physically-plausible deformations along the longitudinal relaxation axis.
We compared PCMC-T1 to baseline deep-learning-based image registration approaches using a 5-fold experimental setup on a publicly available
dataset of 210 patients. PCMC-T1 demonstrated superior model fitting
quality (R2: 0.955) and achieved the highest clinical impact (clinical
score: 3.93) compared to baseline methods (0.941, 0.946 and 3.34, 3.62
respectively). Anatomical alignment results were comparable (Dice score:
0.9835 vs. 0.984, 0.988).",https://github.com/eyalhana/PCMC-T1,https://cardiacmr.hms.harvard.edu/downloads-0,Cardiac,Image Registration,MRI,,,,,,,
Pelphix: Surgical Phase Recognition from X-ray Images in Percutaneous Pelvic Fixation ,"Surgical phase recognition (SPR) is a crucial element in the digital transformation of the modern operating theater. While SPR based on video sources is well-established, incorporation of interventional X-ray sequences has not yet been explored. This paper presents Pelphix, a first approach to SPR for X-ray-guided percutaneous pelvic fracture fixation, which models the procedure at four levels of granularity – corridor, activity, view, and frame value – simulating the pelvic fracture fixation workflow as a Markov process to provide fully annotated training data. 
Using added supervision from detection of bony corridors, tools, and anatomy, we learn image representations that are fed into a transformer model to regress surgical phases at the four granularity levels. Our approach demonstrates the feasibility of X-ray-based SPR, achieving an average accuracy of 99.2% on simulated sequences and 71.7% in cadaver across all granularity levels, with up to 84% accuracy for the target corridor in real data. This work constitutes the first step toward SPR for the X-ray domain, establishing an approach to categorizing phases in X-ray-guided surgery, simulating realistic image sequences to enable machine learning model development, and demonstrating that this approach is feasible for the analysis of real procedures. As X-ray-based SPR continues to mature, it will benefit procedures in orthopedic surgery, angiography, and interventional radiology by equipping intelligent surgical systems with situational awareness in the operating room.",https://github.com/benjamindkilleen/pelphix,https://github.com/benjamindkilleen/pelphix,Guided Interventions and Surgery,Other,other,Surgical Data Science,Surgical Skill and Work Flow Analysis,,,,,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints ,"As one of the most challenging orthopedic injuries, pelvic fractures typically involve iliac and sacral fractures as well as joint dislocations. Structural repair is the most crucial phase in pelvic fracture surgery. Due to the absence of data for the intact pelvis before fracture, reduction planning heavily relies on surgeon’s experience. We present a two-stage method for automatic reduction planning to restore the healthy morphology for complex pelvic trauma. First, multiple bone fragments are registered to morphable templates using a novel SSM-based symmetrical complementary (SSC) registration. Then the optimal target reduction pose of dislocated bone is computed using a novel articular surface (AS) detection and matching method. A leave-one-out experiment was conducted on 240 simulated samples with six types of pelvic fractures on a pelvic atlas with 40 members. In addition, our method was tested in four typical clinical cases corresponding to different categories. The proposed method outperformed traditional SSM, mean shape reference, and contralateral mirroring methods in the simulation experiment, achieving a root-mean-square error of 3.4 ± 1.6 mm, with statistically significant improvement. In the clinical feasibility experiment, the results on various fracture types satisfied clinical requirements on distance measurements and were considered acceptable by senior experts. We have demonstrated the benefit of combining morphable models and structural constraints, which simultaneously utilizes cohort statistics and patient-specific features.",,https://github.com/I-STAR/PelvisAtlas,Surgical Planning and Simulation,Musculoskeletal,Image Registration,Guided Interventions and Surgery,CT,other,,,,
Pelvic Fracture Segmentation Using a Multi-scale Distance-weighted Neural Network ,"Pelvic fracture is a severe type of high-energy injury. Segmentation of pelvic fractures from 3D CT images is fundamental for trauma diagnosis, evaluation, and treatment planning. Manual delineation of the fracture surface can be done in a slice-by-slice fashion but is slow and error-prone. Automatic fracture segmentation is challenged by the complex structure of pelvic bones and the large variations in fracture types and shapes. This study proposes a deep-learning method for automatic pelvic fracture segmentation. Our approach consists of two consecutive networks. The anatomical segmentation network extracts left and right ilia and sacrum from CT scans. Then, the fracture segmentation network further isolates the fragments in each masked bone region. We design and integrate a distance-weighted loss into a 3D U-net to improve accuracy near the fracture site. In addition, multi-scale deep supervision and a smooth transition strategy are used to facilitate training. We built a dataset containing 100 CT scans with fractured pelvis and manually annotated the fractures. A five-fold cross-validation experiment shows that our method outperformed max-flow segmentation and network without distance weighting, achieving a global Dice of 99.38%, a local Dice of 93.79%, and an Hausdorff distance of 17.12 mm. We have made our dataset and source code publicly available and expect them to facilitate further pelvic research, especially reduction planning.",https://github.com/YzzLiu/FracSegNet,https://github.com/YzzLiu/FracSegNet,Image Segmentation,Musculoskeletal,Guided Interventions and Surgery,Attention models,CT,,,,,
Performance Metrics for Probabilistic Ordinal Classifiers ,"Ordinal classification models assign higher penalties to predictions further away from the true class. As a result, they are appropriate for relevant diagnostic tasks like disease progression prediction or medical image grading. The consensus for assessing their categorical predictions dictates the use of distance-sensitive metrics like the Quadratic-Weighted Kappa score or the Expected Cost. However, there has been little discussion regarding how to measure performance of probabilistic predictions for ordinal classifiers. In conventional classification, common measures for probabilistic predictions are Proper Scoring Rules (PSR) like the Brier score, or Calibration Errors like the ECE, yet these are not optimal choices for ordinal classification. A PSR named Ranked Probability Score (RPS), widely popular in the forecasting field, is more suitable for this task, but it has received no attention in the image analysis community. This paper advocates the use of the RPS for image grading tasks. In addition, we demonstrate a failure mode of this score resulting in a counter-intuitive behavior, and propose a simple fix for it. Comprehensive experiments on four large-scale biomedical image grading problems over three different datasets show that the RPS is a more suitable performance metric for probabilistic ordinal predictions. Code to reproduce our experiments can be found at \url{github.com/witheld}.",github.com/agaldran/prob_ord_metrics,https://tmed.cs.tufts.edu/tmed_v2.htm,Uncertainty,Computer Aided Diagnosis,Interpretability / Explainability,,,,,,,
Personalized Patch-based Normality Assessment of Brain Atrophy in Alzheimer’s Disease ,"Cortical thickness is an important biomarker associated with
gray matter atrophy in neurodegenerative diseases. In order to conduct
meaningful comparisons of cortical thickness between different subjects,
it is imperative to establish correspondence among surface meshes. Conventional
methods achieve this by projecting surface onto canonical domains
such as the unit sphere or averaging feature values in anatomical
regions of interest (ROIs). However, due to the natural variability in cortical
topography, perfect anatomically meaningful one-to-one mapping
can be hardly achieved and the practice of averaging leads to the loss
of detailed information. For example, two subjects may have different
number of gyral structures in the same region, and thus mapping can
result in gyral/sulcal mismatch which introduces noise and averaging in
detailed local information loss. Therefore, it is necessary to develop new
method that can overcome these intrinsic problems to construct more
meaningful comparison for atrophy detection. To address these limitations,
we propose a novel personalized patch-based method to improve
cortical thickness comparison across subjects. Our model segments the
brain surface into patches based on gyral and sulcal structures to reduce
mismatches in mapping method while still preserving detailed topological
information which is potentially discarded in averaging. Moreover,the
personalized templates for each patch account for the variability of folding
patterns, as not all subjects are comparable. Finally, through normality
assessment experiments, we demonstrate that our model performs
better than standard spherical registration in detecting atrophy in patients
with mild cognitive impairment (MCI) and Alzheimer’s disease
(AD).",,https://adni.loni.usc.edu/,Computational Anatomy and Physiology,Image Registration,MRI,,,,,,,
PET Image Denoising with Score-Based Diffusion Probabilistic Models ,"Low-count positron emission tomography (PET) imaging is an effective way to reduce the radiation risk of PET at the cost of a low signal-to-noise ratio. Our study aims to denoise low-count PET images in an unsupervised mode since the mainstream methods rely on paired data, which is not always feasible in clinical practice. We adopt the diffusion probabilistic model in consideration of its strong generation ability. Our model consists of two stages. In the training stage, we learn a score function network via evidence lower bound (ELBO) optimization. In the sampling stage, the trained score function and low-count image are employed to generate the corresponding high-count image under two handcrafted conditions. One is based on restoration in latent space, and the other is based on noise insertion in latent space. Thus, our model is named the bidirectional condition diffusion probabilistic model (BC-DPM). Real patient whole-body data are utilized to evaluate our model. The experiments show that our model achieves better performance in both qualitative and quantitative respects compared to several traditional and recently proposed learning-based methods.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Reconstruction,PET/SPECT,,,,,,,
PET-diffusion: Unsupervised PET Enhancement based on the Latent Diffusion Model ,"Positron emission tomography (PET) is an advanced nuclear imaging technique with an irreplaceable role in neurology and oncology studies, but its accessibility is often limited by the radiation hazards inherent in imaging. To address this dilemma, PET enhancement methods have been developed by improving the quality of low-dose PET (LPET) images to standard-dose PET (SPET) images. However, previous PET enhancement methods rely heavily on the paired LPET and SPET data which are rare in clinic. Thus, in this paper, we propose an unsupervised PET enhancement (uPETe) framework based on the latent diffusion model, which can be trained only on SPET data. Specifically, our SPET-only uPETe consists of an encoder to compress the input SPET/LPET images into latent representations, a latent diffusion model to learn/estimate the distribution of SPET latent representations, and a decoder to recover the latent representations into SPET images. Moreover, from the theory of actual PET imaging, we improve the latent diffusion model of uPETe by 1) adopting PET image compression for reducing the computational cost of diffusion model, 2) using Poisson diffusion to replace Gaussian diffusion for making the perturbed samples closer to the actual noisy PET, and 3) designing CT-guided cross-attention for incorporating additional CT images into the inverse process to aid the recovery of structural details in PET. With extensive experimental validation, our uPETe can achieve superior performance over state-of-the-art methods, and shows stronger generalizability to the dose changes of PET imaging.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Abdomen,Image Reconstruction,PET/SPECT,,,,,,
Physics-based Decoding Improves Magnetic Resonance Fingerprinting ,"Magnetic Resonance Fingerprinting (MRF) is a promising
approach for fast Quantitative Magnetic Resonance Imaging (QMRI).
However, existing MRF methods suffer from slow imaging speeds and
poor generalization performance on radio frequency pulse sequences gen-
erated in various scenarios. To address these issues, we propose a novel
MRI physics-informed regularization for MRF. The proposed approach
adopts a supervised encoder-decoder framework, where the encoder per-
forms the main task, i.e. predicting the target tissue properties from
input magnetic responses, and the decoder servers as a regularization
via reconstructing the inputs from the estimated tissue properties us-
ing a Bloch-equation based MRF physics model. The physics-based de-
coder improves the generalization performance and uniform stability by
a considerable margin in practical out-of-distribution settings. Exten-
sive experiments verified the effectiveness of the proposed approach and
achieved state-of-the-art performance on tissue property estimation.",https://github.com/rmrisforbidden/CauMedical.git,https://github.com/rmrisforbidden/CauMedical.git,Model Generalizability / Federated Learning,MRI,,,,,,,,
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T ,"Chemical exchange saturation transfer (CEST) is an MRI
method that provides insights on the metabolic level. Several metabolite
effects appear in the CEST spectrum. These effects are isolated by
Lorentzian curve fitting. The separation of CEST effects suffers from the
inhomogeneity of the saturation field B1. This leads to inhomogeneities
in the associated metabolic maps. Current B1 correction methods require
at least two sets of CEST-spectra. This at least doubles the acquisition
time. In this study, we investigated the use of an unsupervised
physics-informed conditional autoencoder (PICAE) to efficiently correct
B1 inhomogeneity and isolate metabolic maps while using a single CEST
scan. The proposed approach integrates conventional Lorentzian model
into the conditional autoencoder and performs voxel-wise B1 correction
and Lorentzian line fitting. The method provides clear interpretation of
each step and is inherently generative. Thus, CEST-spectra and fitted
metabolic maps can be created at arbitrary B1 levels. This is important
because the B1 dispersion contains information about the exchange
rates and concentration of metabolite protons, paving the way for their
quantification. The isolated maps for tumor data showed a robust B1 correction
and more than 25% increase in structural similarity index (SSIM)
with gadolinium reference image compared to the standard interpolationbased
method and subsequent Lorentzian curve fitting. This efficient correction
method directly results in at least 50% reduction in scan time.",https://git5.cs.fau.de/rajput/picae,,MRI,Neuroimaging - Others,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Visualization in Biomedical Imaging,,,,,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography ,"Magnetic resonance elastography (MRE) is a medical imaging modality that non-invasively quantifies tissue stiffness (elasticity) and is commonly used for diagnosing liver fibrosis. Constructing an elasticity map of tissue requires solving an inverse problem involving a partial differential equation (PDE). Current numerical techniques to solve the inverse problem are noise-sensitive and require explicit specification of physical relationships. In this work, we apply physics-informed neural networks to solve the inverse problem of tissue elasticity reconstruction. Our method does not rely on numerical differentiation and can be extended to learn relevant correlations from anatomical images while respecting physical constraints. We evaluate our approach on simulated data and in vivo data from a cohort of patients with non-alcoholic fatty liver disease (NAFLD). Compared to numerical baselines, our method is more robust to noise and more accurate on realistic data, and its performance is further enhanced by incorporating anatomical information.",https://github.com/batmanlab/MRE-PINN,https://bioqic-apps.charite.de/downloads,Image Reconstruction,Abdomen,Other,MRI,other,,,,,
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network ,"Filamentous objects are ubiquitous in biomedical images, and segmenting individual filaments is fundamental for biomedical research. Unlike common objects with well-defined boundaries and centers, filaments are thin, non-rigid, varying in shape, and often densely overlapping. These properties make it extremely challenging to extract individual filaments. This paper proposes a novel approach to extract filamentous objects by transforming an instance segmentation problem into a sequence modeling problem. Our approach first identifies filaments’ tip points, and we segment each instance by tracing them from each tip with a sequential encoder-decoder framework. The proposed method simulates the process of humans extracting filaments: pick a tip and trace the filament. As few datasets contain instance labels of filaments, we first generate synthetic filament datasets for training and evaluation. Then, we collected a dataset of 15 microscopic images of microtubules with instance labels for evaluation. Our proposed method can alleviate the data shortage problem since our proposed model can be trained with synthetic data and achieve state-of-art results when directly evaluated on the microtubule dataset and P. rubescens dataset. We also demonstrate our approaches’ capabilities in extracting short and thick elongated objects by evaluating on the C. elegans dataset. Our method achieves a comparable result compared to the state-of-art method with faster processing time. Our code is available at https://github.com/VimsLab/DRIFT.",https://github.com/VimsLab/DRIFT,https://github.com/VimsLab/DRIFT,Image Segmentation,Microscopy,,,,,,,,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation ,"Transfer learning is a critical technique in training deep neural networks for the challenging medical image segmentation task that requires enormous resources. With the abundance of medical image data, many research institutions release models trained on various datasets that can form a huge pool of candidate source models to choose from. Hence, it’s vital to estimate the source models’ transferability (i.e., the ability to generalize across different downstream tasks) for proper and efficient model reuse. To make up for its deficiency when applying transfer learning to medical image segmentation, in this paper, we therefore propose a new  \textbf{Transferability Estimation} (TE) method. We first analyze the drawbacks of using the existing TE algorithms for medical image segmentation and then design a source-free TE framework that considers both class consistency and feature variety for better estimation. Extensive experiments show that our method surpasses all current algorithms for transferability estimation in medical image segmentation. Code is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV.",https://github.com/EndoluminalSurgicalVision-IMR/CCFV,,Transfer learning,Image Segmentation,,,,,,,,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer ,"Large deformation image registration is a challenging task in medical image registration. Iterative registration and pyramid registration are two common CNN-based methods for the task. However, these methods usually consume more parameters and time. Additionally, the existing CNN-based registration methods mainly focus on local feature extraction, limiting their ability to capture the long-distance correlation between image pairs. In this paper, we propose a fast and accurate learning-based algorithm, Pyramid-Iterative Vision Transformer (PIViT), for 3D large deformation medical image registration. Our method constructs a novel pyramid iterative composite structure to solve large deformation problem by using low-scale iterative registration with a Swin Transformer-based long-distance correlation decoder. Furthermore, we exploit pyramid structure to supplement the detailed information of the deformation field by using high-scale feature maps. Comprehensive experimental results implemented on brain MRI and liver CT datasets show that the proposed method is superior to the existing registration methods in terms of registration accuracy, training time and parameters, especially of a significant advantage in running time. Our code is available at https://github.com/Torbjorn1997/PIViT.",https://github.com/Torbjorn1997/PIViT,https://drive.google.com/file/d/1rJtP9M1N3lSjNzJ5kIzRrrwPe1bWCfXB/view,Image Registration,Attention models,,,,,,,,
PLD-AL: Pseudo-Label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images ,"Segmentation of the carotid intima-media (CIM) offers more precise morphological evidence for obesity and atherosclerotic disease compared to the method that measures its thickness and roughness during routine ultrasound scans. Although advanced deep learning technology has shown promise in enabling automatic and accurate medical image segmentation, the lack of a large quantity of high-quality CIM labels may hinder the model training process. Active learning (AL) tackles this issue by iteratively annotating the subset whose labels contribute the most to the training performance at each iteration. However, this approach substantially relies on the expert’s experience, particularly when addressing ambiguous CIM boundaries that may be present in real-world ultrasound images. Our proposed approach, called pseudo-label divergence-based active learning (PLD-AL), aims to train segmentation models using a gradually enlarged and refined labeled pool. The approach has an outer and an inner loops: The outer loop calculates the Kullback–Leibler (KL) divergence of predictive pseudo-labels related to two consecutive AL iterations. It determines which portion of the unlabeled pool should be annotated by an expert. The inner loop trains two networks: The student network is fully trained on the current labeled pool, while the teacher network is weighted upon itself and the student one, ultimately refining the labeled pool. We evaluated our approach using both the Carotid Ultrasound Boundary Study dataset and an in-house dataset from Children’s Hospital, Zhejiang University School of Medicine. Our results demonstrate that our approach outperforms state-of-the-art AL approaches. Furthermore, the visualization results show that our approach less over-estimates the CIM area than the rest methods, especially for severely ambiguous ultrasound images at the thickness direction.",https://github.com/CrystalWei626/PLD_AL,https://data.mendeley.com/datasets/fpv535fss7/1,Active Learning,Image Segmentation,,,,,,,,
PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents ,"Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral’s OpenAccess subset,  which is 8 times larger than before, PMC-CLIP covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, {\em i.e.}, subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP achieve state-of-the-art results on various downstream tasks, including image-text retrieval on ROCO, MedMNIST image classification, Medical VQA,  i.e., +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification.",https://github.com/WeixiongLin/PMC-CLIP,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Biophotonics,CT,EEG/ECG,Histopathology,Microscopy,MRI,other,PET/SPECT,Text (clinical/radiology reports),Ultrasound
Point Cloud Diffusion Models for Automatic Implant Generation ,"Advances in 3D printing of biocompatible materials make patient-specific implants increasingly popular. The design of these implants is, however, still a tedious and largely manual process. Existing approaches to automate implant generation are mainly based on 3D U-Net architectures on downsampled or patch-wise data, which can result in a loss of detail or contextual information. Following the recent success of Diffusion Probabilistic Models, we propose a novel approach for implant generation based on a combination of 3D point cloud diffusion models and voxelization networks. Due to the stochastic sampling process in our diffusion model, we can propose an ensemble of different implants per defect, from which the physicians can choose the most suitable one. We evaluate our method on the SkullBreak and SkullFix datasets, generating high-quality implants and achieving competitive evaluation scores. The project page can be found at https://pfriedri.github.io/pcdiff-implant-io.",https://github.com/pfriedri/pcdiff-implant,https://www.fit.vutbr.cz/~ikodym/skullbreak_training.zip,Other,Guided Interventions and Surgery,CT,Surgical Planning and Simulation,,,,,,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation ,"The shape of the posterior eyeball is a crucial factor in many
clinical applications, such as myopia prevention, surgical planning, and
disease screening. However, current shape representations are limited by
their low resolution or small field of view, providing insufficient infor-
mation for surgeons to make accurate decisions. This paper proposes
a novel task of reconstructing complete 3D posterior shapes based on
small-FOV OCT images and introduces a novel Posterior Eyeball Shape
Network (PESNet) to accomplish this task. The proposed PESNet is de-
signed with dual branches that incorporate anatomical information of
the eyeball as guidance. To capture more detailed information, we intro-
duce a Polar Voxelization Block (PVB) that transfers sparse input point
clouds to a dense representation. Furthermore, we propose a Radius-wise
Fusion Block (RFB) that fuses correlative hierarchical features from the
two branches. Our qualitative results indicate that PESNet provides a
well-represented complete posterior eyeball shape with a chamfer dis-
tance of 9.52, SSIM of 0.78, and Density of 0.013 on the self-made pos-
terior ocular shape dataset. We also demonstrate the effectiveness of our
model by testing it on patients’ data. Overall, our proposed PESNet
offers a significant improvement over existing methods in accurately re-
constructing the complete 3D posterior eyeball shape. This achievement
has important implications for clinical applications.",,,Visualization in Biomedical Imaging,Ophthalmology,Image Reconstruction,other,,,,,,
Polar-Net: A Clinical-Friendly Model for Alzheimer’s Disease Detection in OCTA Images ,"Optical Coherence Tomography Angiography (OCTA) is a promising tool for detecting Alzheimer’s disease (AD) by imaging the retinal microvasculature. Ophthalmologists commonly use region-based analysis, such as the ETDRS grid, to study OCTA image biomarkers and understand the correlation with AD. However, existing studies have used general deep computer vision methods, which present challenges in providing interpretable results and leveraging clinical prior knowledge. To address these challenges, we propose a novel deep-learning framework called Polar-Net. Our approach involves mapping OCTA images from Cartesian coordinates to polar coordinates, which allows for the use of approximate sector convolution and enables the implementation of the ETDRS grid-based regional analysis method commonly used in clinical practice. Furthermore, Polar-Net incorporates clinical prior information of each sector region into the training process, which further enhances its performance. Additionally, our framework adapts to acquire the importance of the corresponding retinal region, which helps researchers and clinicians understand the model’s decision-making process in detecting AD and assess its conformity to clinical observations. Through evaluations on private and public datasets, we have demonstrated that Polar-Net outperforms existing state-of-the-art methods and provides more valuable pathological evidence for the association between retinal vascular changes and AD. In addition, we also show that the two innovative modules introduced in our framework have a significant impact on improving overall performance.",https://github.com/iMED-Lab/Polar-Net-Pytorch.git,https://ieee-dataport.org/open-access/octa-500,Ophthalmology,Vascular,Computer Aided Diagnosis,other,Visualization in Biomedical Imaging,,,,,
Position-aware masked autoencoder for histopathology WSI representation learning ,"Transformer-based multiple instance learning (MIL) framework has been proved advanced for whole slide image (WSI) analysis. However, existing spatial embedding strategies in Transformer can only represent fixed structural information, which are hard to tackle the scale-varying and isotropic characteristics of WSIs. Moreover, the current MIL cannot take the advantage of a large number of unlabeled WSIs for training. In this paper, we propose a novel self-supervised whole slide image representation learning framework named position-aware masked autoencoder (PAMA), which can make full use of abundant unlabeled WSIs to improve the discrimination of slide features. Moreover, we propose a position-aware cross-attention (PACA) module with a kernel reorientation (KRO) strategy, which makes PAMA able to maintain spatial integrity and semantic enrichment during the training. We evaluated the proposed method on a public TCGA-Lung dataset with 3,064 WSIs and an in-house Endometrial dataset with 3,654 WSIs and compared it with 4 state-of-the-art methods. The results of experiments show our PAMA is superior to SOTA MIL methods and SSL methods.",https://github.com/WkEEn/PAMA,https://portal.gdc.cancer.gov/projects/TCGA-LUAD,Computational (Integrative) Pathology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis ,"In brain functional networks, nodes represent brain regions while edges symbolize the functional connections that enable the transfer of information between brain regions. However, measuring the transportation cost of information transfer between brain regions is a challenge for most existing methods in brain network analysis. To address this problem, we propose a graph sliced Wasserstein distance to measure the cost of transporting information between brain regions in a brain functional network. Building upon the graph sliced Wasserstein distance, we propose a new graph kernel called sliced Wasserstein graph kernel to measure the similarity of brain functional networks. Compared to existing graph methods, including graph kernels and graph neural networks, our proposed sliced Wasserstein graph kernel is positive definite and a faster method for comparing brain functional networks. To evaluate the effectiveness of our proposed method, we conducted classification experiments on functional magnetic resonance imaging data of brain diseases. Our experimental results demonstrate that our method can significantly improve classification accuracy and computational speed compared to state-of-the-art graph methods for classifying brain diseases.",,,Computer Aided Diagnosis,Neuroimaging - Functional Brain Networks,MRI,,,,,,,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities ,"The surgical usage of Mixed Reality (MR) has received growing attention in areas such as surgical navigation systems, skill assessment, and robot-assisted surgeries. For such applications, pose estimation for hand and surgical instruments from an egocentric perspective is a fundamental task and has been studied extensively in the computer vision field in recent years. However, the development of this field has been impeded by a lack of datasets, especially in the surgical field, where bloody gloves and reflective metallic tools make it hard to obtain 3D pose annotations for hands and objects using conventional methods. To address this issue, we propose POV-Surgery, a large-scale, synthetic, egocentric dataset focusing on pose estimation for hands with different surgical gloves and three orthopedic surgical instruments, namely scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329 frames, featuring high-resolution RGB-D video streams with activity annotations, accurate 3D and 2D annotations for hand-object pose, and 2D hand-object segmentation masks. We fine-tune the current SOTA methods on POV-Surgery and further show the generalizability when applying to real-life cases with surgical gloves and tools by extensive evaluations. The code and the dataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.",https://batfacewayne.github.io/POV_Surgery_io/,https://batfacewayne.github.io/POV_Surgery_io/,Surgical Data Science,Attention models,Other,Surgical Scene Understanding,Surgical Skill and Work Flow Analysis,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-Contexts Discriminator GAN ,"Revealing structural-functional relationship is crucial issue in neuroscience study since it helps to understand brain activities. Structural Connectivity (SC) represents the fibers connection between the brain regions, which is relatively static. Functional Connectivity (FC) represents the active signal correlations between the brain regions, which is relatively dynamic and diverse. Many works predict FC from SC and achieve unique FC prediction. However, FC is diverse since it represents brain activities. In this work, we propose the MCGAN, a multi-contexts discriminator based generative adversarial network for predicting diverse FC from SC. The proposed multi-contexts discriminator provides three kinds of supervisions to strengthen the generator, i.e. edge-level, node-level and graph-level. Since FC represents the connection of the brain regions, which can be regarded as edge-based graph. We adopt edge-based graph convolution method to model the context encoding. Moreover, to introduce the diversity of generated FC, we utilize monte-carlo mean samples to bring in more FC data for training. We validate our MCGAN on Human Connextome Project (HCP) dataset and Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. The results show that our method can generate diverse and meaningful FC from SC, revealing the one-to-many relationship between the individual SC and the multiple FC. The glamorous significance of this work is that once we have anatomical structure of brain represented by SC, we can predict diverse developments of brain activity represented by FC, which helps to reveal individual brain’s static-dynamic structural-functional mode.",,,MRI,Neuroimaging - Functional Brain Networks,Image Reconstruction,,,,,,,
Prediction of Cognitive Scores by Joint Use of Movie-watching fMRI Connectivity and Eye Tracking via Attention-CensNet ,"Brain functional connectivity under the naturalistic paradigm has been demon-strated to be better at predicting individual behaviors than other brain states, such as rest and task. Nevertheless, the state-of-the-art methods are difficult to achieve desirable results from movie-watching paradigm fMRI(mfMRI) induced brain functional connectivity, especially when the datasets are small, because it is diffi-cult to quantify how much useful dynamic information can be extracted from a single mfMRI modality to describe the state of the brain. Eye tracking, becoming popular due to its portability and less expense, can provide abundant behavioral features related to the output of human’s cognition, and thus might supplement the mfMRI in observing subjects’ subconscious behaviors. However, there are very few works on how to effectively integrate the multimodal information to strengthen the performance by unified framework. To this end, an effective fu-sion approach with mfMRI and eye tracking, based on Convolution with Edge-Node Switching in Graph Neural Networks (CensNet), is proposed in this arti-cle, with subjects taken as nodes, mfMRI derived functional connectivity as node feature, different eye tracking features used to compute similarity between sub-jects to construct heterogeneous graph edges. By taking multiple graphs as differ-ent channels, we introduce squeeze-and-excitation attention module to CensNet (A-CensNet) to integrate graph embeddings from multiple channels into one. The experiments demonstrate the proposed model outperforms the one using single modality, single channel and state-of-the-art methods. The results suggest that brain functional activities and eye behaviors might complement each other in in-terpreting trait-like phenotypes.",,,Attention models,Neuroimaging - Functional Brain Networks,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Prediction of Infant Cognitive Development with Cortical Surface-based Multimodal Learning ,"Exploring the relationship between the cognitive ability and infant cortical structural and functional development is critically important to advance our understanding of early brain development, which, however, is very challenging due to the complex and dynamic brain development in early postnatal stages. Conventional approaches typically use either the structural MRI or resting-state functional MRI and rely on the region-level features or inter-region connectivity features after cortical parcellation for predicting cognitive scores. However, these methods have two major issues: 1) spatial information loss, which discards the critical fine-grained spatial patterns containing rich information related to cognitive development; 2) modality information loss, which ignores the complementary information and the interaction between the structural and functional images. To address these issues, we unprecedentedly invent a novel framework, namely cortical surface-based multimodal learning framework (CSML), to leverage fine-grained multimodal features for cognition development prediction. First, we introduce the fine-grained surface-based data representation to capture spatially detailed structural and functional information. Then, a dual-branch network is proposed to extract the discriminative features for each modality respectively and further captures the modality-shared and complementary information with a disentanglement strategy. Finally, an age-guided cognition prediction module is developed based on the prior that the cognition develops along with age. We validate our method on an infant multimodal MRI dataset with 318 scans. Compared to state-of-the-art methods, our method consistently achieves superior performances, and for the first time suggests crucial regions and features for cognition development hidden in the fine-grained spatial details of cortical structure and function.",,,Attention models,Neuroimaging - Brain Development,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Guided Interventions and Surgery,MRI,,,,
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping ,"Pre-operative survival prediction for diffuse glioma patients is desired for personalized treatment. Clinical findings show that tumor types are highly correlated with the prognosis of diffuse glioma. However, the tumor types are unavailable before craniotomy and cannot be used in pre-operative survival prediction. In this paper, we propose a new deep learning based pre-operative survival prediction method. Besides the common survival prediction backbone, a tumor subtyping network is integrated to provide tumor-type-related features. Moreover, a novel ordinal manifold mixup is presented to enhance the training of the tumor subtyping network. Unlike the original manifold mixup, which neglects the feature distribution, the proposed method forces the feature distribution of different tumor types in the order of risk grade, by which consistency between the augmented features and labels can be strengthened. We evaluate our method on both in-house and public datasets comprising 1936 patients and demonstrate up to a 10% improvement in concordance-index compared with the state-of-the-art methods. Ablation study further confirms the effectiveness of the proposed tumor subtyping network and the ordinal manifold mixup.",,,Image Segmentation,Neuroimaging - Others,,,,,,,,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement ,"Deep learning-based medical image enhancement methods (e.g., denoising and super-resolution) mainly rely on paired data and correspondingly the well-trained models can only handle one type of task. 
In this paper, we address the limitation with a diffusion model-based framework that mitigates the requirement of paired data and can simultaneously handle multiple enhancement tasks by one pre-trained diffusion model without fine-tuning.
Experiments on low-dose CT and heart MR datasets demonstrate that the proposed method is versatile and robust for image denoising and super-resolution. We believe our work constitutes a practical and versatile solution to scalable and generalizable image enhancement.",https://github.com/bowang-lab/DPM-MedImgEnhance,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758026,Model Generalizability / Federated Learning,CT,,,,,,,,
Prior-driven Dynamic Brain Networks for Multi-Modal Emotion Recognition ,"Emotions are closely related to many mental and cognitive diseases, such as depression, mania, Parkinson’s Disease, etc, and the recognition of emotion plays an important role in diagnosis of these diseases, which is mostly limited to the patient’s self-description. Because emotion is always unstable, the objective quantitative methods are urgently needed for more accurate recognition of emotion, which can help improve the diagnosis performance for emotion related brain disease. Existing studies have shown that EEG and facial expressions are highly correlated, and combining EEG with facial expressions can better depict emotion-related information. However, most of the existing multi-modal emotion recognition studies cannot combine multiple modalities properly, and ignore the temporal variability of channel connectivity in EEG. In this paper, we propose a spatial-temporal feature extraction framework for multi-modal emotion recognition by constructing prior-driven Dynamic Functional Connectivity Networks (DFCNs). First, we consider each electrode as a node to construct the original dynamic brain networks. Second, we calculate the correlation between EEG and facial expression through cross attention, as a prior knowledge of dynamic brain networks, and embedded to obtain the final DFCNs representation with prior knowledge. Then, we design a spatial-temporal feature extraction network by stacking multiple residual blocks based on 3D convolutions, and non-local attention is introduced to capture the global information at the temporal level. Finally, we adopt the features from fully connected layer for classification. Experimental results on the DEAP dataset demonstrate the effectiveness of the proposed method.",,http://www.eecs.qmul.ac.uk/mmv/datasets/deap/,Neuroimaging - Functional Brain Networks,Other,EEG/ECG,Video,,,,,,
Privacy-preserving Early Detection of Epileptic Seizures in Videos ,"In this work, we contribute towards the development of video-based epileptic seizure classification by introducing a novel framework (SETR-PKD), which could achieve privacy-preserved early detection of seizures in videos. Specifically, our framework has two significant components - (1) It is built upon optical flow features extracted from the video of a seizure, which encodes the seizure motion semiotics while preserving the privacy of the patient; (2) It utilizes a transformer based progressive knowledge distillation, where the knowledge is gradually distilled from networks trained on a longer portion of video samples to the ones which will operate on shorter portions. Thus, our proposed framework addresses the limitations of the current approaches which compromise the privacy of the patients by directly operating on the RGB video of a seizure as well as impede real-time detection of a seizure by utilizing the full video sample to make a prediction. Our SETR-PKD framework could detect tonic-clonic seizures (TCSs) in a privacy-preserving manner with an accuracy of 83.9% while they are only half-way into their progression.",https://github.com/DevD1092/seizure-detection,https://github.com/DevD1092/seizure-detection,Computer Aided Diagnosis,Other,Transfer learning,Video,,,,,,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation ,"Colorectal polyps detected during colonoscopy are strongly associated with colorectal cancer, making polyp segmentation a critical clinical decision-making tool for diagnosis and treatment planning. However, accurate polyp segmentation remains a challenging task, particularly in cases involving diminutive polyps and other intestinal substances that produce a high false-positive rate. Previous polyp segmentation networks based on supervised binary masks may have lacked global semantic perception of polyps, resulting in a loss of capture and discrimination capability for polyps in complex scenarios. To address this issue, we propose a novel Gaussian-Probabilistic guided semantic fusion method that progressively fuses the probability information of polyp positions with the decoder supervised by binary masks. Our Probabilistic Modeling Ensemble Vision Transformer Network(PETNet) effectively suppresses noise in features and significantly improves expressive capabilities at both pixel and instance levels, using just simple types of convolutional decoders. Extensive experiments on five widely adopted datasets show that PETNet outperforms existing methods in identifying polyp camouflage, appearance changes, and small polyp scenes, and achieves a speed about 27FPS in edge computing devices. Codes are available at: https://github.com/Seasonsling/PETNet.",https://github.com/Seasonsling/PETNet,https://polyp.grand-challenge.org/CVCClinicDB/,Oncology,Image Segmentation,Attention models,Model Generalizability / Federated Learning,Transfer learning,other,,,,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening ,"Vulvovaginal candidiasis (VVC) is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime. It will lead to several symptoms including pruritus, vaginal soreness, and so on. Automatic whole slide image (WSI) classification is highly demanded, for the huge burden of disease control and prevention. However, the WSI-based computer-aided VCC screening method is still vacant due to the scarce labeled data and unique properties of candida. Candida in WSI is challenging to be captured by conventional classification models due to its distinctive elongated shape, the small proportion of their spatial distribution, and the style gap from WSIs. To make the model focus on the candida easier, we propose an attention-guided method, which can obtain a robust diagnosis classification model. Specifically, we first use a pre-trained detection model as prior instruction to initialize the classification model. Then we design a Skip Self-Attention module to refine the attention onto the fined-grained features of candida. Finally, we use a contrastive learning method to alleviate the overfitting caused by the style gap of WSIs and suppress the attention to false positive regions. Our experimental results demonstrate that our framework achieves state-of-the-art performance.",https://github.com/caijd2000/MICCAI2023-VVC-Screening,,Computer Aided Diagnosis,Computational (Integrative) Pathology,,,,,,,,
Progressively Coupling Network for Brain MRI Registration in Few-shot Situation ,"Segmentation-assisted registration models can leverage few available labels in exchange for large performance gains by their complementarity. Recent related works independently build the prediction branches of deformation field and segmentation label without any information interaction except for the joint supervision. They ignore underlying relationship between the two tasks, thereby failing to fully exploit their complementary nature. To this end, we propose a ProGressively Coupling Network (PGCNet) that relies on segmentation to regularize the correct projecting of registration. Our overall framework is a multi-task learning paradigm in which features are extracted by one shared encoder and then separate prediction branches are built for segmentation and registration. In the prediction phase, we utilize the bidirectional deformation fields as bridges to warp the features of moving and fixed images to each other’s segmentation branches, thereby progressively and interactively supplementing additional context information at multiple levels for their segmentation. By establishing the entangled correspondence, segmentation supervision can indirectly regularize registration stream to accurately project semantic layout for segmentation branches. In addition, we design the position correlation calculation for registration to easier capture the spatial correlation of the images from the shared features. Experimental results on public 3D brain MRI datasets show that our work performs favorably against the state-of-the-art methods.",,,Image Registration,Neuroimaging - Functional Brain Networks,Image Segmentation,MRI,,,,,,
Prompt-based Grouping Transformer for Nucleus Detection and Classification ,"Automatic nuclei detection and classification can produce effective information for disease diagnosis. Most existing methods classify nuclei independently or do not make full use of the semantic similarity between nuclei and their grouping features. In this paper, we propose a novel end-to-end nuclei detection and classification framework based on a grouping transformer-based classifier. The nuclei classifier learns and updates the representations of nuclei groups and categories via hierarchically grouping the nucleus embeddings. Then the cell types are predicted with the pairwise correlations between categorical embeddings and nucleus features. For the efficiency of the fully transformer-based framework, we take the nucleus group embeddings as the input prompts of backbone, which helps harvest grouping guided features by tuning only the prompts instead of the whole backbone. Experimental results show that the proposed method significantly outperforms the existing models on three datasets.",,,Computer Aided Diagnosis,Histopathology,Microscopy,,,,,,,
Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific Prompt Tuning ,"Whole slide image (WSI) classification is a critical task in computational pathology, requiring the processing of gigapixel-sized images, which is challenging for current deep-learning methods. Current state of the art methods are based on multi-instance learning schemes (MIL), which usually rely on pretrained features to represent the instances. Due to the lack of task-specific annotated data, these features are either obtained from well-established backbones on natural images, or, more recently from self-supervised models pretrained on histopathology. However, both approaches yield task-agnostic features, resulting in performance loss compared to the appropriate task-related supervision, if available. In this paper, we show that when task-specific annotations are limited, we can inject such supervision into downstream task training, to reduce the gap between fully task-tuned and task agnostic features. We propose Prompt-MIL, an MIL framework that integrates prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism, where only a small fraction of parameters calibrates the pretrained features to encode task-specific information, rather than the conventional full fine-tuning approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters. Compared to conventional full fine-tuning approaches, we fine-tune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45% while training 21%-27% faster.",https://github.com/cvlab-stonybrook/PromptMIL,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,Microscopy,,,,,,,
PROnet: Point Refinement using Shape-guided Offset Map for Nuclei Instance Segmentation ,"Recently, weakly supervised nuclei segmentation methods using only points are gaining attention, as they can ease the tedious labeling process. However, most methods often fail to separate adjacent nuclei and are particularly sensitive to point annotations that deviate from the center of nuclei, resulting in lower accuracy. In this study, we propose a novel weakly supervised method to effectively distinguish adjacent nuclei, and maintain robustness regardless of point label deviation. We detect and segment nuclei by combining a binary segmentation module, an offset regression module and a center detection module to determine foreground pixels, delineate boundaries and identify instances. In training, we first generate pseudo binary masks using geodesic distance-based Voronoi diagrams and k-means clustering. Next, segmentation predictions are used to repeatedly generate pseudo offset maps that indicate the most likely nuclei center. Finally, an Expectation Maximization (EM) based process iteratively refines intial point labels based on the offset map predictions to fine-tune our framework. Experimental results show that our model consistently outperforms state-of-the-art methods on public datasets regardless of the point annotations accuracy.",,https://monusac-2020.grand-challenge.org/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,,,,,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography ,"Aortic stenosis (AS) is a common heart valve disease that requires accurate and timely diagnosis for appropriate treatment. Most current automatic AS severity detection methods rely on black-box models with a low level of trustworthiness, which hinders clinical adoption. To address this issue, we propose ProtoASNet, a prototypical network that directly detects AS from B-mode echocardiography videos, while making interpretable predictions based on the similarity between theinput and learned spatio-temporal prototypes. This approach provides supporting evidence that is clinically relevant, as the prototypes typically highlight markers such as calcification and restricted movement of aortic valve leaflets. Moreover, ProtoASNet utilizes abstention loss to estimate aleatoric uncertainty by defining a set of prototypes that capture ambiguity and insufficient information in the observed data. This provides a reliable system that can detect and explain when it may fail. We evaluate ProtoASNet on a private dataset and the publicly available TMED-2 dataset, where it outperforms existing state-of-the-art methods with an accuracy of 80.7% and 79.7%, respectively. Furthermore, ProtoASNet provides interpretability and an uncertainty measure for each prediction, which can improve transparency and facilitate the interactive usage of deep networks to aid clinical decision-making. Our source code is available at: https://github.com/hooman007/ProtoASNet.",https://github.com/hooman007/ProtoASNet,,Computer Aided Diagnosis,Cardiac,Attention models,Interpretability / Explainability,Uncertainty,Ultrasound,Video,,,
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning ,"Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed.It combines the lesion counterfactual map with the tissue probability map to train a lightweight PWML segmentation network, demonstrating state-of-the-art performance on a real-clinical dataset of infant T1w MR images. The code is available at https://github.com/ladderlab-xjtu/DeepPWML.",https://github.com/ladderlab-xjtu/DeepPWML,,Image Segmentation,Fetal Imaging,Neuroimaging - Brain Development,Computer Aided Diagnosis,MRI,,,,,
QCResUNet: Joint Subject-level and Voxel-level Prediction of Segmentation Quality ,"Deep learning has achieved state-of-the-art performance in automated brain tumor segmentation from magnetic resonance imaging (MRI) scans. However, the unexpected occurrence of poor-quality outliers, especially in out-of-distribution samples, hinders their translation into patient-centered clinical practice. Therefore, it is important to develop automated tools for large-scale segmentation quality control (QC). However, most existing QC methods targeted cardiac MRI segmentation which involves a single modality and a single tissue type. Importantly, these methods only provide a subject-level segmentation-quality prediction, which cannot inform clinicians where the segmentation needs to be refined. To address this gap, we proposed a novel network architecture called QCResUNet that simultaneously produces segmentation-quality measures as well as voxel-level segmentation error maps for brain tumor segmentation QC. To train the proposed model, we created a wide variety of segmentation-quality results by using i) models that have been trained for a varying number of epochs with different modalities; and ii) a newly devised segmentation-generation method called SegGen. The proposed method was validated on a large public brain tumor dataset with segmentations generated by different methods, achieving high performance on the prediction of segmentation-quality metric as well as voxel-wise localization of segmentation errors. 
The implementation will be publicly available at \href{https://github.com/peijie-chiu/QC-ResUNet}{https://github.com/peijie-chiu/QC-ResUNet}.",https://github.com/peijie-chiu/QC-ResUNet,,Image Segmentation,Neuroimaging - Others,Computer Aided Diagnosis,Interpretability / Explainability,Other,MRI,,,,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes ,"Atrial Fibrillation (AF) is characterized by rapid, irregular heartbeats, and can lead to fatal complications such as heart failure. 
The disease is divided into two sub-types based on severity, which can be automatically classified through CT volumes for disease screening of severe cases.However, existing classification approaches rely on generic radiomic features that may not be optimal for the task, whilst deep learning methods tend to over-fit to the high-dimensional volume inputs.In this work, we propose a novel radiomics-informed deep-learning method, RIDL, that combines the advantages of deep learning and radiomic approaches to improve AF sub-type classification. 
Unlike existing hybrid techniques that mostly rely on naïve feature concatenation, we observe that radiomic feature selection methods can serve as an information prior, and propose supplementing low-level deep neural network (DNN) features with locally computed radiomic features. This reduces DNN over-fitting and allows local variations between radiomic features to be better captured. 
Furthermore, we ensure complementary information is learned by deep and radiomic features by designing a novel feature de-correlation loss. Combined, our method addresses the limitations of deep learning and radiomic approaches and outperforms state-of-the-art alternatives, achieving 86.9% AUC on the AF sub-type classification task.",https://github.com/xmed-lab/RIDL,,Cardiac,Computer Aided Diagnosis,Other,CT,Treatment Response and Outcome/Disease Prediction,,,,,
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting ,"Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benchmark VQARad while performing best among methods without domain-specific vision-language pretraining and provides a strong baseline on Rad-ReStruct. Our work represents a significant step towards the automated population of structured radiology reports and provides a valuable first benchmark for future research in this area. We will make all annotations and our code for annotation generation, model evaluation, and training publicly available upon acceptance.",https://github.com/ChantalMP/Rad-ReStruct,https://osf.io/89kps/,Text (clinical/radiology reports),Lung,Computer Aided Diagnosis,,,,,,,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization ,"Choroidal neovascularization (CNV) is a leading cause of visual impairment in retinal diseases. Optical coherence tomography angiography (OCTA) enables non-invasive CNV visualization with micrometerscale resolution, aiding precise extraction and analysis. Nevertheless, the irregular shape patterns, variable scales, and blurred lesion boundaries of CNVs present challenges for their precise segmentation in OCTA images. In this study, we propose a \textbf{R}eliable \textbf{B}oundary-\textbf{G}uided choroidal neovascularization segmentation \textbf{Net}work (RBGNet) to address these issues. Specifically, our RBGNet comprises a dual-stream encoder and a multi-task decoder. 
The encoder consists of a convolutional neural network (CNN) stream and a transformer stream. The transformer captures global context and establishes long-range dependencies, compensating for the limitations of the CNN. The decoder is designed with multiple tasks to address specific challenges. Reliable boundary guidance is achieved by evaluating the uncertainty of each pixel label,
By assigning it as a weight to regions with highly unstable boundaries, the network’s ability to learn precise boundary locations can be improved, ultimately leading to more accurate segmentation results. The prediction results are also used to adaptively adjust the weighting factors between losses to guide the network’s learning process. Our experimental results demonstrate that RBGNet outperforms existing methods,achieving a Dice score of $90.42\%$ for CNV region segmentation and $90.25\%$ for CNV vessel segmentation.",https://github.com/iMED-Lab/RBGnet-Pytorch.git,,Image Segmentation,Ophthalmology,Vascular,,,,,,,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection ,"With an excellent balance between speed and accuracy, cutting-edge YOLO frameworks have become one of the most efficient algorithms for object detection. However, the performance of using YOLO networks is scarcely investigated in brain tumor detection. We propose a novel YOLO architecture with Reparameterized Convolution based on channel Shuffle (RCS-YOLO). We present RCS and a One-Shot Aggregation of RCS (RCS-OSA), which link feature cascade and computation efficiency to extract richer information and reduce time consumption. Experimental results on the brain tumor dataset Br35H show that the proposed model surpasses YOLOv6, YOLOv7, and YOLOv8 in speed and accuracy. Notably, compared with YOLOv7, the precision of RCS-YOLO improves by 2.6\%, and the inference speed by 60\% at 114.8 images detected per second (FPS). Our proposed RCS-YOLO achieves state-of-the-art performance on the brain tumor detection task. The code is available at https://github.com/mkang315/RCS-YOLO.",https://github.com/mkang315/RCS-YOLO,https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection,Image Segmentation,MRI,,,,,,,,
Realistic endoscopic illumination modeling for NeRF-based data generation ,"Expanding training and evaluation data is a major step towards building and deploying reliable localization and 3D reconstruction techniques during colonoscopy screenings. However, training and evaluating pose and depth models in colonoscopy is hard as available datasets are limited in size. This paper proposes a method for generating new pose and depth datasets by fitting NeRFs in already available colonoscopy datasets. Given a set of images, their associated depth maps and pose information, we train a novel light source location-conditioned NeRF to encapsulate the 3D and color information of a colon sequence. Then, we leverage the trained networks to render images from previously unobserved camera poses and simulate different camera systems, effectively expanding the source dataset. Our experiments show that our model is able to generate RGB images and depth maps of a colonoscopy sequence from previously unobserved poses with high accuracy.",https://github.com/surgical-vision/REIM-NeRF,https://arxiv.org/abs/2206.08903,Surgical Data Science,Abdomen,Guided Interventions and Surgery,Other,Video,,,,,
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer ,"The relationship between blood flow and neuronal activity is widely recognized, with blood flow frequently serving as a surrogate for neuronal activity in fMRI studies. At the microscopic level, neuronal activity has been shown to influence blood flow in nearby blood vessels. This study introduces the first predictive model that addresses this issue directly at the explicit neuronal population level. Using in vivo recordings in awake mice, we employ a novel spatiotemporal bimodal transformer architecture to infer current blood flow based on both historical blood flow and ongoing spontaneous neuronal activity. Our findings indicate that incorporating neuronal activity significantly enhances the model’s ability to predict blood flow values. Through analysis of the model’s behavior, we propose hypotheses regarding the largely unexplored nature of the hemodynamic response to neuronal activity.",,,Attention models,Microscopy,,,,,,,,
Recruiting the best teacher modality: A customized knowledge distillation method for IF based nephropathy diagnosis ,"The joint use of multiple imaging modalities for medical image has been widely studied in recent years. The fusion of information from different modalities has demonstrated the performance improvement for some medical tasks. For nephropathy diagnosis, immunofluorescence (IF) is one of the most widely-used medical image due to its ease of acquisition with low cost, which is also an advanced multi-modality technique. However, the existing methods mainly integrate information from diverse sources by averaging or combining them, failing to exploit multi-modality knowledge in details. In this paper, we observe that the 7 modalities of IF images have different impact on different nephropathy categories. Accordingly, we propose a knowledge distillation framework to transfer knowledge from the trained single-modality teacher networks to a multi-modality student network. On top of this, given a input IF sequence, a recruitment module is developed to dynamically assign weights to teacher models and optimize the performance of student model. By applying on several different architectures, the extensive experimental results verify the effectiveness of our method for nephropathy diagnosis.",,,Computer Aided Diagnosis,Computational (Integrative) Pathology,Transfer learning,Histopathology,,,,,,
Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation ,"Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation. Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked. Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels. Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues. First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames. We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels. In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples. This design enables the network to concentrate on clean samples in a coarse-to-fine manner. Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-the-art robust segmentation approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.",https://github.com/BeileiCui/MS-TFAL,https://endovissub2018-roboticscenesegmentation.grand-challenge.org/,Data Efficient Learning,Image Segmentation,Guided Interventions and Surgery,Video,Surgical Data Science,Surgical Scene Understanding,,,,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras ,"Efficient algorithms for solving inverse optical tomography problems with noisy and sparse measurements are a major challenge for near-infrared fluorescence guided surgery. To address that challenge, we propose an Incremental Fluorescent Target Reconstruction scheme based on the recent advances in convex optimization and sparse regularization. We demonstrate the efficacy of the proposed scheme on continuous wave reflectance mode boundary measurements of emission fluence from a 3D fluorophore target immersed in a tissue like media and acquired by an inexpensive consumer-grade camera.",https://github.com/IBM/DOT,,Image Reconstruction,Guided Interventions and Surgery,Surgical Data Science,,,,,,,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography ,"Automated quality assessment (AQA) in transoesophageal echocardiography (TEE) contributes to accurate diagnosis and echocardiographers’ training, providing direct feedback for the development of dexterous skills. 
However, prior works only perform AQA on simulated TEE data due to the scarcity of real data, which lacks applicability in the real world. Considering the cost and limitations of collecting TEE data from real cases, exploiting the readily available simulated data for AQA in real-world TEE is desired. In this paper, we construct the first simulation-to-real TEE dataset, and propose a novel Simulation-to-Real network (SR-AQA) with unsupervised domain adaptation for this problem. It is based on uncertainty-aware feature stylization (UFS), incorporating style consistency learning (SCL) and task-specific learning (TL), to achieve high generalizability. Concretely, UFS estimates the uncertainty of feature statistics in the real domain and diversifies simulated images with style variants extracted from the real images, alleviating the domain gap. We enforce SCL and TL across different real-stylized variants to learn domain-invariant and task-specific representations. Experimental results demonstrate that our SR-AQA outperforms state-of-the-art methods with 3.02% and 4.37% performance gain in two AQA regression tasks, by using only 10% unlabelled real data. Our code and dataset are available at https://doi.org/10.5522/04/23699736.",https://github.com/wzjialang/SR-AQA,https://doi.org/10.5522/04/23699736,Surgical Skill and Work Flow Analysis,Cardiac,Computer Aided Diagnosis,Data Efficient Learning,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,Uncertainty,Ultrasound,Surgical Data Science
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis ,"Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5\% gain in accuracy over regular CNNs.",https://github.com/ThijsKuipers1995/gconv,https://medmnist.com,Other,Data Efficient Learning,Model Generalizability / Federated Learning,,,,,,,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast ,"Image-guided surgery requires fast and accurate registration to align preoperative imaging and surgical spaces. The breast undergoes large nonrigid deformations during surgery, compromising the use of imaging data for intraoperative tumor localization. Rigid registration fails to account for nonrigid soft tissue deformations, and biomechanical modeling approaches like finite element simulations can be cumbersome in implementation and computation. We introduce regularized Kelvinlet functions, which are closed-form smoothed solutions to the partial differential equations for linear elasticity, to model breast deformations. We derive and present analytical equations to represent nonrigid point-based translation (“grab”) and rotation (“twist”) deformations embedded within an infinite elastic domain. Computing a displacement field using this method does not require mesh discretization or large matrix assembly and inversion conventionally associated with finite element or mesh-free methods. We solve for the optimal superposition of regularized Kelvinlet functions that achieves registration of the medical image to simulated intraoperative geometric point data of the breast. We present registration performance results using a dataset of supine MR breast imaging from healthy volunteers mimicking surgical deformations with 237 individual targets from 11 breasts. We include analysis on the method’s sensitivity to regularized Kelvinlet function hyperparameters. To demonstrate application, we perform registration on a breast cancer patient case with a segmented tumor and compare performance to other image-to-physical and image-to-image registration methods. We show comparable accuracy to a previously proposed image-to-physical registration method with improved computation time, making regularized Kelvinlet functions an attractive approach for image-to-physical registration problems.",,,Guided Interventions and Surgery,Breast,Computational Anatomy and Physiology,Image Registration,MRI,Surgical Planning and Simulation,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture ,"Brain tissue microarchitecture is characterized by heterogeneous degrees of diffusivity and rates of transverse relaxation. Unlike standard diffusion MRI with a single echo time (TE), which provides information primarily on diffusivity, relaxation-diffusion MRI involves multiple TEs and multiple diffusion-weighting strengths for probing tissue-specific coupling between relaxation and diffusivity. Here, we introduce a relaxation-diffusion model that characterizes tissue apparent relaxation coefficients for a spectrum of diffusion length scales and at the same time factors out the effects of intra-voxel orientation heterogeneity. We examined the model with an in vivo dataset, acquired using a clinical scanner, involving different health conditions. Experimental results indicate that our model caters to heterogeneous tissue microstructure and can distinguish fiber bundles with similar diffusivities but different relaxation rates. Code with sample data is available at https://github.com/dryewu/RDSI.",https://github.com/dryewu/RDSI,,Neuroimaging - DWI and Tractography,MRI,,,,,,,,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions ,"Multimodality eye disease screening is crucial in ophthalmology as it integrates information from diverse sources to complement their respective performances. However, the existing methods are weak in assessing the reliability of each unimodality, and directly fusing an unreliable modality may cause screening errors. To address this issue, we introduce a novel multimodality evidential fusion pipeline for eye disease screening, EyeMoSt, which provides a measure of confidence for unimodality and elegantly integrates the multimodality information from a multi-distribution fusion perspective. Specifically, our model estimates both local uncertainty for unimodality and global uncertainty for the fusion modality to produce reliable classification results. More importantly, the proposed mixture of Student’s t distributions adaptively integrates different modalities to endow the model with heavy-tailed properties, increasing robustness and reliability. Our experimental findings on both public and in-house datasets show that our model is more reliable than current methods. Additionally, EyeMost has the potential ability to serve as a data quality discriminator, enabling reliable decision-making for multimodality eye disease screening.",https://github.com/Cocofeat/EyeMoSt,,Ophthalmology,Uncertainty,,,,,,,,
"Representation, Alignment, Fusion: A Generic Transformer-based Framework for Multi-modal Glaucoma Recognition ","Early glaucoma can be diagnosed with various modalities based on morphological features. However, most existing automated solutions rely on single-modality, such as Color Fundus Photography(CFP) which lacks 3D structural information, or Optical Coherence Tomography (OCT) which suffers from insufficient specificity for glaucoma. To effectively detect glaucoma with CFP and OCT, we propose a generic multi-modal Transformer-based framework for glaucoma, MM-RAF. Our framework is implemented with pure self-attention mechanisms and consists of three simple and effective modules: Bilateral Contrastive Alignment (BCA) aligns both modalities into the same semantic space to bridge the semantic gap; Multiple Instance Learning Representation (MILR) aggregates multiple OCT B-scans into a semantic structure and downsizes the scale of the OCT branch; Hierarchical Attention Fusion (HAF) enhances the cross-modality interaction capability with spatial information. By incorporating three modules, our framework can effectively handle cross-modality interaction between different modalities with huge disparity. The experimental results show that the framework outperforms the existing multi-modal methods of this task and is robust even with a clinical small dataset. Moreover, by visualizing, OCT can reveal the subtle abnormalities in CFP, indicating that the relationship between various modalities is captured.",https://github.com/YouZhouRUC/MM-RAF,https://ichallenges.grand-challenge.org/,Ophthalmology,Computer Aided Diagnosis,Attention models,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,
RESToring Clarity: Unpaired Retina Image Enhancement using Scattering Transform ,"Retina images are non-invasive and highly effective in the diagnosis of various diseases such as cardiovascular and ophthalmological diseases. Accurate diagnosis depends on the quality of the retina images, however, obtaining high-quality images can be challenging due to various factors, such as noise, artifacts, and eye movement. Methods for enhancing retina images are therefore in high demand for clinical purposes, yet the problem remains challenging as there is a natural trade-off between preserving anatomical details (e.g., vessels) and increasing overall image quality other than the content in it. Moreover, training an enhancement model often requires paired images that map low-quality images to high-quality images, which may not be available in practice. In this regime, we propose a novel Retina image Enhancement framework using Scattering Transform (REST). REST uses unpaired retina image sets and does not require prior knowledge of the degraded factors. The generator in REST enhances retina images by utilizing the Anatomy Preserving Branch (APB) and the Tone Transferring Branch (TTB) with different roles. Our model successfully enhances low-quality retina images demonstrating commendable results on two independent datasets.",,,Image Reconstruction,Ophthalmology,,,,,,,,
Rethinking Semi-Supervised Federated Learning: How to co-train fully-labeled and fully-unlabeled client imaging data ,"The most challenging, yet practical, setting of semi-supervised federated learning (SSFL) is where a few clients have fully labeled data whereas the other clients have fully unlabeled data. This is particularly common in healthcare settings where collaborating partners (typically hospitals) may have images but not annotations. The bottleneck in this setting is the joint training of labeled and unlabeled clients as the objective function for each client varies based on the availability of labels. This paper investigates an alternative way for effective training with labeled and unlabeled clients in a federated setting. We propose a novel learning scheme specifically designed for SSFL which we call Isolated Federated Learning (IsoFed) that circumvents the problem by avoiding simple averaging of supervised and semi-supervised models together. In particular, our training approach consists of two parts - (a) isolated aggregation of labeled and unlabeled client models, and (b) local self-supervised pretraining of isolated global models in all clients. We evaluate our model performance on medical image datasets of four different modalities publicly available within the biomedical image classification benchmark MedMNIST. We further vary the proportion of labeled clients and the degree of heterogeneity to demonstrate the effectiveness of the proposed method under varied experimental settings.",https://github.com/PramitSaha/IsoFed-MICCAI-2023,https://github.com/MedMNIST/MedMNIST,Model Generalizability / Federated Learning,,,,,,,,,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning ,"Retinal age has recently emerged as a reliable ageing biomarker for assessing risks of ageing-related diseases. Several studies propose to train deep learning models to estimate retinal age from fungus images. However, the limitation of these studies lies in 1) both of them only train models on snapshot images from single cohorts; 2) they ignore label ambiguity and individual variance in the modeling part. In this study, we propose a progressive label distribution learning (LDL) method with temporal fundus images to improve the retinal age estimation on snapshot fundus images from multiple cohorts. First, we design a two-stage LDL regression head to estimate adaptive age distribution for individual images. Then, we eliminate cohort variance by introducing a domain-aware ordinal constraint to align image features from distinct data sources. Finally, we add a temporal branch to model sequential fundus images and use the captured temporal evolution as auxiliary knowledge to enhance the model’s predictive performance on snapshot fundus images. We use a large retinal fundus image dataset which consists of around 130k images from multiple cohorts to verify our method. Extensive experiments provide evidence that our model can achieve lower age prediction errors than existing methods.",,,Ophthalmology,Treatment Response and Outcome/Disease Prediction,,,,,,,,
Retinal Thickness Prediction from Multi-modal Fundus Photography ,"Retinal thickness map (RTM), generated from OCT volumes, provides a quantitative representation of the retina, which is then averaged into the ETDRS grid. The RTM and ETDRS grid are often used to diagnose and monitor retinal-related diseases that cause vision loss worldwide. However, OCT examinations can be available to limited patients because it is costly and time-consuming. Fundus photography (FP) is a 2D imaging technique for the retina that captures the reflection of a flash of light. However, current researches often focus on 2D patterns in FP, while its capacity of carrying thickness information is rarely explored. In this paper, we explore the capability of infrared fundus photography (IR-FP) and color fundus photography (C-FP) to provide accurate retinal thickness information. We propose a Multi-Modal Fundus photography enabled Retinal Thickness prediction network (M²FRT). We predict RTM from IR-FP to overcome the limitation of acquiring RTM with OCT, which boosts mass screening with a cost-effective and efficient solution. We first introduce C-FP to provide IR-FP with complementary thickness information for more precise RTM prediction. The misalignment of images from the two modalities is tackled by the Transformer-CNN hybrid design in M²FRT. Furthermore, we obtain the ETDRS grid prediction solely from C-FP using a lightweight decoder, which is optimized with the guidance of the RTM prediction task during the training phase. Our methodology utilizes the easily acquired C-FP, making it a valuable resource for providing retinal thickness quantification in clinical practice and telemedicine, thereby holding immense clinical significance.",,,Ophthalmology,Attention models,other,,,,,,,
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models ,"State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the first step (1), R2R reveals model weaknesses by finding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model’s performance and remaining sensitivity towards the artifact. Using two medical benchmark datasets for Melanoma detection and bone age estimation, we apply our R2R framework to VGG, ResNet and EfficientNet architectures and thereby reveal and correct real dataset-intrinsic artifacts, as well as synthetic variants in a controlled setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations to mitigate different biases. Code is available on https://github.com/maxdreyer/Reveal2Revise.",https://github.com/maxdreyer/Reveal2Revise,https://challenge.isic-archive.com/landing/2019/,Interpretability / Explainability,Computer Aided Diagnosis,,,,,,,,
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction ,"Positron emission tomography (PET) is a molecular imaging technique relying on a step, namely attenuation correction,  to correct radionuclide distribution based on pre-determined attenuation coefficients. Conventional AC techniques require additionally-acquired computed tomography (CT) or magnetic resonance (MR) images to calculate attenuation coefficients, which increases imaging expenses, time costs, or radiation hazards to patients, especially for whole-body scanners. In this paper, considering technological advances in acquiring more anatomical information in raw PET images, we propose to conduct attenuation correction to PET by itself. To achieve this, we design a deep learning based framework, namely anatomical skeleton-enhanced generation (ASEG), to generate pseudo CT images from non-attenuation corrected PET images for attenuation correction. Specifically, ASEG contains two sequential modules, i.e., a skeleton prediction module and a volume rendering module. The former module first delineates anatomical skeleton and the latter module then renders tissue volume. Both modules are trained collaboratively with specific anatomical-consistency constraint to guarantee tissue generation fidelity. Experiments on four public PET/CT datasets demonstrate that our ASEG outperforms existing methods by achieving better consistency of anatomical structures in generated CT images, which are further employed to conduct PET attenuation correction with better similarity to real ones. This work verifies the feasibility of generating pseudo CT from raw PET for attenuation correction without acquisition of additional CT.",https://github.com/YongshengPan/ASEG-for-PET2CT,,Image Reconstruction,CT,PET/SPECT,,,,,,,
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection ,"Early and accurate disease detection is crucial for patient management and successful treatment outcomes. However, the automatic identification of anomalies in medical images can be challenging. Conventional methods rely on large labeled datasets which are difficult to obtain. To overcome these limitations, we introduce a novel unsupervised approach, called PHANES (Pseudo Healthy generative networks for ANomaly Segmentation). Our method has the capability of reversing anomalies, i.e., preserving healthy tissue and replacing anomalous regions with pseudo-healthy (PH) reconstructions. Unlike recent diffusion models, our method does not rely on a learned noise distribution nor does it introduce random alterations to the entire image. Instead, we use latent generative networks to create masks around possible anomalies, which are refined using inpainting generative networks. We demonstrate the effectiveness of PHANES in detecting stroke lesions in T1w brain MRI datasets and show significant improvements over state-of-the-art (SOTA) methods. We believe that our proposed framework will open new avenues for interpretable, fast, and accurate anomaly segmentation with the potential to support various clinical-oriented downstream tasks.",https://github.com/ci-ber/PHANES,https://brain-development.org/ixi-dataset/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Neuroimaging - Others,Computer Aided Diagnosis,MRI,,,,,,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery ,"The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligning (WA) technique is also integrated to adjust the weight bias between old and new tasks. We further establish a CL framework on three public surgical datasets in the context of surgical settings that consist of overlapping classes between old and new surgical VQLA tasks. With extensive experiments, we demonstrate that our proposed method excellently reconciles learning and forgetting on the continual surgical VQLA over conventional CL methods. Our code is publicly accessible at github.com/longbai1006/CS-VQLA.",https://github.com/longbai1006/CS-VQLA,https://endovissub2018-roboticscenesegmentation.grand-challenge.org/home/,Surgical Scene Understanding,Continual Learning,Text (clinical/radiology reports),Video,Surgical Data Science,,,,,
Revisiting Feature Propagation and Aggregation in Polyp Segmentation ,"Accurate segmentation of polyps is a crucial step in efficient diagnosis for colorectal cancer during screening procedures. The prevalent UNet-like encoder-decoder frameworks are commonly employed, due to their capability of capturing multi-scale contextual information efficiently. However, two major limitations hinder network achieving effective feature propogation and aggregation.  Firstly, the skip connection only transmits a single scale feature to the decoder, which can result in limited feature representation. Secondly, the features are transmitted without any information filter, which is inefficient for performing feature fusion at the decoder. To address these limitations, we propose a novel feature enhancement network that leverages feature propagation enhancement and feature aggregation enhancement modules for more efficient feature fusion and multi-scale feature propagation. Specifically, the feature propagation enhancement module transmits full stage features from the encoder to the decoder, while the feature aggregation enhancement module performs feature fusion with gate mechanisms, allowing for more effective information filtering. The multi-scale feature aggregation module provides rich multi-scale semantic information to the decoder, further enhancing the network’s performance. Extensive evaluations on five datasets demonstrate the effectiveness of our method, particularly on challenging datasets such as CVC-ColonDB and ETIS, where it can outperform the previous state-of-the-art models by a significant margin (3\%) in terms of mIoU and mDice.",,,Computer Aided Diagnosis,Image Segmentation,,,,,,,,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology ,"The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like Age-related Macular Edema (AMD), Glaucoma, or Pathological Myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying Space-associated neuro-ocular Syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and Glaucoma) or a terrestrial station (SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that 2x to 4x compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for Fundus Image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR)  on three public datasets. Additionally, we tested the model’s effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.",https://github.com/FarihaHossain/SwinFSR,https://amd.grand-challenge.org/Home/,Ophthalmology,Image Reconstruction,,,,,,,,
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? ,"While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique’s ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.",https://github.com/ss-sun/right-for-the-wrong-reason,,Interpretability / Explainability,Computer Aided Diagnosis,,,,,,,,
Robust and Generalisable Segmentation of Subtle Epilepsy-causing Lesions: a Graph Convolutional Approach ,"Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. “Ground truth” manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex- or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based features and manual lesion masks from structural MRI data, the proposed GCN achieved an AUC of 0.74, a significant improvement against a previously used vertex-wise multi-layer perceptron (MLP) classifier (AUC 0.64). With sensitivity thresholded at 67%, the GCN had a specificity of 71% in comparison to 49% when using the MLP. This improvement in specificity is vital for clinical integration of lesion-detection tools into the radiological workflow, through increasing clinical confidence in the use of AI radiological adjuncts and reducing the number of areas requiring expert review.",https://github.com/MELDProject/meld_graph,,Neuroimaging - Others,Computer Aided Diagnosis,Image Segmentation,Other,MRI,,,,,
Robust Cervical Abnormal Cell Detection via Distillation from Local-scale Consistency Refinement ,"Automated detection of cervical abnormal cells from Thin-prep cytologic test (TCT) images is essential for efficient cervical abnormal screening by computer-aided diagnosis system. However, the detection performance is influenced by noise samples in the training dataset, mainly due to the subjective differences among cytologists in annotating the training samples. Besides, existing detection methods often neglect visual feature correlation information between cells, which can also be utilized to aid the detection model. In this paper, we propose a cervical abnormal cell detection method optimized by a novel distillation strategy based on local-scale consistency refinement. Firstly, we use a vanilla RetinaNet to detect top-K suspicious cells and extract region-of-interest (ROI) features. Then, a pre-trained Patch Correction Network (PCN) is leveraged to obtain local-scale features and conduct further refinement for these suspicious cell patches. We design a classification ranking loss to utilize refined scores for reducing the effects of the noisy label. Furthermore, the proposed ROI-correlation consistency loss is computed between extracted ROI features and local-scale features to exploit correlation information and optimize RetinaNet. Our experiments demonstrate that our distillation method can greatly optimize the performance of cervical abnormal cell detection without changing the detector’s network structure in the inference stage.",https://github.com/feimanman/Cervical-Abnormal-Cell-Detection,,Computational (Integrative) Pathology,Histopathology,,,,,,,,
Robust estimation of the microstructure of the early developing brain using deep learning ,"Diffusion Magnetic Resonance Imaging (dMRI) is a powerful non-invasive method for studying white matter tracts of the brain. However, accurate microstructure estimation with fiber orientation distribution (FOD) using existing computational methods requires a large number of diffusion measurements. In clinical settings, this is often not possible for neonates and fetuses because of increased acquisition times and subject movements. Therefore, methods that can estimate the FOD from reduced measurements are of high practical utility. Here, we exploited deep learning and trained a neural network to directly map dMRI data acquired with as low as six diffusion directions to FODs for neonates and fetuses. We trained the method using target FODs generated from densely-sampled multiple-shell data with the multi-shell multi-tissue constrained spherical deconvolution (MSMT-CSD). Detailed evaluations on independent newborns’ test data show that our method achieved estimation accuracy levels on par with the state-of-the-art methods while reducing the number of required measurements by more than an order of magnitude. Qualitative assessments on two out-of-distribution clinical datasets of fetuses and newborns show the consistency of the estimated FODs and hence the cross-site generalizability of the method.",https://github.com/Medical-Image-Analysis-Laboratory/Perinatal_fODF_DL_estimation,https://www.developingconnectome.org/data-release/second-data-release/,Image Reconstruction,Fetal Imaging,Neuroimaging - Brain Development,Neuroimaging - DWI and Tractography,Other,MRI,,,,
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus ,"The symptoms of neuropsychiatric systemic lupus erythematosus (NPSLE) are subtle and elusive at the early stages. 1H-MRS (proton magnetic resonance spectrum) imaging technology can detect more detailed early appearances of NPSLE compared with conventional ones. However, the noises in 1H-MRS data often bring bias in the diagnostic process. Moreover, the features of specific brain regions are positively correlated with a certain category but may be redundant for other categories. To overcome these issues, we propose a robust exclusive adaptive sparse feature selection (REASFS) algorithm for early diagnosis and biomarker discovery of NPSLE. Specifically, we employ generalized correntropic loss to address non-Gaussian noise and outliers. Then, we develop a generalized correntropy-induced exclusive ℓ2,1 regularization to adaptively accommodate various sparsity levels and preserve informative features. We conduct sufficient experiments on a benchmark NPSLE dataset, and the experimental results demonstrate the superiority of our proposed method compared with state-of-the-art ones.",,,Computer Aided Diagnosis,Other,,,,,,,,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images ,"Standardized image rotation is essential to improve reading
performance in interventional X-ray imaging. To minimize user inter-
action and streamline the 2D imaging workflow, we present a new au-
tomated image rotation method. Image rotation can follow two steps:
First, an anatomy specific centerline image is predicted which depicts
the desired anatomical axis to be aligned vertically after rotation. In a
second step, the necessary rotation angle is calculated from the orien-
tation of the predicted line image. We propose an end-to-end trainable
model with the Hough transform (HT) and a differentiable spatial-to-
angular transform (DSAT) embedded as known operators. This model
allows to robustly regress a rotation angle while maintaining an explain-
able inner structure and allows to be trained with both a centerline seg-
mentation and angle regression loss. The proposed method is compared
to a Hu moments-based method on anterior-posterior X-ray images of
spine, knee, and wrist. For the wrist images, the HT based method re-
duces the mean absolute angular error (MAE) from 9.28° using the Hu
moments-based method to 3.54°. Similar results for the spinal and knee
images can be reported. Furthermore, a large improvement of the 90th
percentile of absolute angular error by a factor of 3 indicates a better
robustness and reduction of outliers for the proposed method.",,,Other,Musculoskeletal,,,,,,,,
Robust Segmentation via Topology Violation Detection and Feature Synthesis ,"Despite recent progress of deep learning-based medical image segmentation techniques, fully automatic results often fail to meet clinically acceptable accuracy, especially when topological constraints should be observed, e.g., closed surfaces. Although modern image segmentation methods show promising results when evaluated based on conventional metrics such as the Dice score or Intersection-over-Union, these metrics do not reflect the correctness of a segmentation in terms of a required topological genus. Existing approaches estimate and constrain the topological structure via persistent homology (PH). However, these methods are not computationally efficient as calculating PH is not differentiable. To overcome this problem, we propose a novel approach for topological constraints based on the multi-scale Euler Characteristic (EC). To mitigate computational complexity, we propose a fast  formulation for the EC that can inform the learning process of arbitrary segmentation networks via topological violation maps. Topological performance is further facilitated through a corrective convolutional network block. Our experiments on two datasets show that our method can significantly improve topological correctness.",https://github.com/smilell/Topology-aware-Segmentation-using-Euler-Characteristic,http://www.developingconnectome.org/,Image Segmentation,Neuroimaging - Brain Development,,,,,,,,
Robust T-Loss for Medical Image Segmentation ,"This paper presents a new robust loss function, T-Loss, for medical image segmentation. The proposed loss is based on the negative log-likelihood of the Student-t distribution and can effectively handle outliers in the data by controlling its sensitivity with a single parameter. This parameter is updated during the backpropagation process, eliminating the need for additional computation or prior information about the level and spread of noisy labels. Our experiments show that T-Loss outperforms traditional loss functions in terms of dice scores on two public medical datasets for skin lesion and lung segmentation. We also demonstrate the ability of T-Loss to handle different types of simulated label noise, resembling human error. Our results provide strong evidence that T-Loss is a promising alternative for medical image segmentation where high levels of noise or outliers in the dataset are a typical phenomenon in practice.",https://robust-tloss.github.io/,https://challenge.isic-archive.com/data/,Image Segmentation,Dermatology,Lung,,,,,,,
Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks ,"Automatic vertebra localization and identification in CT scans is important for numerous clinical applications.
Much progress has been made on this topic, but it mostly targets positional localization of vertebrae, ignoring their orientation.
Additionally, most methods employ heuristics in their pipeline that can be sensitive in real clinical images which tend to contain abnormalities. 
We introduce a simple pipeline that employs a standard prediction with a U-Net, followed by a single graph neural network to associate and classify vertebrae with full orientation.
To test our method, we introduce a new vertebra dataset that also contains pedicle detections that are associated with vertebra bodies, creating a more challenging landmark prediction, association and classification task.
Our method is able to accurately associate the correct body and pedicle landmarks, ignore false positives and classify vertebrae in a simple, fully trainable pipeline avoiding application-specific heuristics.
We show our method outperforms traditional approaches such as Hungarian Matching and Hidden Markov Models.
We also show competitive performance on the standard VerSe challenge body identification task.",https://github.com/ImFusionGmbH/VID-vertebra-identification-dataset,https://github.com/ImFusionGmbH/VID-vertebra-identification-dataset,Other,Musculoskeletal,CT,Surgical Planning and Simulation,,,,,,
S2ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-supervised Polyp Segmentation ,"Fully-supervised polyp segmentation has accomplished significant triumphs over the years in advancing the early diagnosis of colorectal cancer. However, label-efficient solutions from weak supervision like scribbles are rarely explored yet primarily meaningful and demanding in medical practice due to the expensiveness and scarcity of densely-annotated polyp data. Besides, various deployment issues, including data shifts and corruption, put forward further requests for model generalization and robustness. To address these concerns, we design a framework of Spatial-Spectral Dual-branch Mutual Teaching and Entropy-guided Pseudo Label Ensemble Learning (S2ME). Concretely, for the first time in weakly-supervised medical image segmentation, we promote the dual-branch co-teaching framework by leveraging the intrinsic complementarity of features extracted from the spatial and spectral domains and encouraging cross-space consistency through collaborative optimization. Furthermore, to produce reliable mixed pseudo labels, which enhance the effectiveness of ensemble learning, we introduce a novel adaptive pixel-wise fusion technique based on the entropy guidance from the spatial and spectral branches. Our strategy efficiently mitigates the deleterious effects of uncertainty and noise present in pseudo labels and surpasses previous alternatives in terms of efficacy. Ultimately, we formulate a holistic optimization objective to learn from the hybrid supervision of scribbles and pseudo labels. Extensive experiments and evaluation on four public datasets demonstrate the superiority of our method regarding in-distribution accuracy, out-of-distribution generalization, and robustness, highlighting its promising clinical significance. Our code is available at https://github.com/lofrienger/S2ME.",https://github.com/lofrienger/S2ME,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Image Segmentation,Model Generalizability / Federated Learning,,,,,,
S3M: Scalable Statistical Shape Modeling through Unsupervised Correspondences ,"Statistical shape models (SSMs) are an established way to represent the anatomy of a population with various clinically relevant applications.
However, they typically require domain expertise, and labor-intensive landmark annotations to construct.
We address these shortcomings by proposing an unsupervised method that leverages deep geometric features and functional correspondences to simultaneously learn local and global shape structures across population anatomies.
Our pipeline significantly improves unsupervised correspondence estimation for SSMs compared to baseline methods, even on highly irregular surface topologies.
We demonstrate this for two different anatomical structures: the thyroid and a multi-chamber heart dataset. 
Furthermore, our method is robust enough to learn from noisy neural network predictions, potentially enabling scaling SSMs to larger patient populations without manual segmentation annotation.",https://github.com/alexanderbaumann99/S3M,,Computational Anatomy and Physiology,Image Reconstruction,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Visualization in Biomedical Imaging,,,,
SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid ,"Estimating displacement vector field via a cost volume computed in the feature space has shown great success in image registration, but it suffers excessive computation burdens. Moreover, existing feature descriptors only extract local features incapable of representing the global semantic information, which is especially important for solving large transformations. 
To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine discrete optimization method for CT registration that includes a decoupled convex optimization procedure to obtain deformation fields based on a self-supervised anatomical embedding (SAM) feature extractor that captures both local and global information.
To be specific, SAMConvex extracts per-voxel features and builds 6D correlation volumes based on SAM features, and iteratively updates a flow field by performing lookups on the correlation volumes with a coarse-to-fine scheme.
SAMConvex outperforms the state-of-the-art learning-based methods and optimization-based methods over two inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one intra-patient registration dataset (Lung CT). Moreover, as an optimization-based method, SAMConvex only takes $\sim2$s ($\sim5s$ with instance optimization) for one paired images.",,,Image Registration,Abdomen,Lung,Oncology,CT,,,,,
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation ,"Cross-domain distribution shift is a common problem for medical image analysis, because medical images from different devices usually own varied domain distributions. Test-time adaptation (TTA) is a promising solution by efficiently adapting source-domain distributions to target-domain distributions at test time with unsupervised manners, which has increasingly attracted important attentions. Previous TTA methods applied to medical image segmentation tasks usually carry out a global domain adaptation for all semantic categories, but global domain adaptation would be sub-optimal as the influence of domain shift on different semantic categories may be different. To obtain improved domain adaptation results for different semantic categories, we propose Semantic-Aware Test-Time Adaptation (SATTA), which can individually update the model parameters to adapt to target-domain distributions for each semantic category. Specifically, SATTA deploys an uncertainty estimation module to effectively measure the discrepancies of semantic categories in domain shift. Then, a semantic adaptive learning rate is developed based on the estimated discrepancies to achieve a personalized degree of adaptation for each semantic category. Lastly, semantic proxy contrastive learning is proposed to individually adjust the model parameters with the semantic adaptive learning rate. Our SATTA are extensively validated on retinal fluid segmentation based on SD-OCT images. The experimental results demonstrate that SATTA consistently improves domain adaptation performance on semantic categories over other state-of-the-art TTA methods.",,,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,other,,,,,,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification ,"Federated learning (FL) has been introduced to the healthcare domain as a decentralized learning paradigm that allows multiple parties to train a model collaboratively without privacy leakage. However, most previous studies have assumed that every client holds an identical label set. In reality, medical specialists tend to annotate only diseases within their knowledge domain or interest. This implies that label sets in each client can be different and even disjoint. In this paper, we propose the framework FedLSM to solve the problem Label Set Mismatch. FedLSM adopts different training strategies on data with different uncertainty levels to efficiently utilize unlabeled or partially labeled data as well as class-wise adaptive aggregation in the classification layer to avoid inaccurate aggregation when clients have missing labels. We evaluate FedLSM on two public real-world medical image datasets, including chest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosis with 10,015 dermoscopy images, and show that it significantly outperforms other state-of-the-art FL algorithms. Code will be made available upon acceptance.",https://github.com/dzp2095/FedLSM,https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass Segmentation ,"Pulmonary nodules and masses are crucial imaging features in lung cancer screening that require careful management in clinical diagnosis. Despite the success of deep learning-based medical image segmentation, the robust performance on various sizes of lesions is in high demand, not only for lung nodules. Thus, we propose a multi-scale neural network with improved performance to address this limitation. Specifically, we introduce an adaptive Scale-aware Test-time Click Adaptation method that utilizes effortlessly obtainable lesion clicks as test-time cues to enhance segmentation performance, particularly for large lesions. The proposed method can be seamlessly integrated into existing networks. Extensive experiments on both open-source and in-house datasets consistently demonstrate the effectiveness of our method over CNN and Transformer-based segmentation methods.",https://github.com/SplinterLi/SaTTCA,https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966254，,Image Segmentation,Lung,Other,CT,,,,,,
Scaling Up 3D Kernels with Bayesian Frequency Re-Parameterization for Medical Image Segmentation ,"With the inspiration of vision transformers, the concept of depth-wise convolution revisits to provide a large Effective Receptive Field (ERF) using Large Kernel (LK) sizes for medical image segmentation. However, the segmentation performance might be saturated and even degraded as the kernel sizes scaled up (e.g., $21\times 21\times 21$) in a Convolutional Neural Network (CNN). We hypothesize that convolution with LK sizes is limited to maintain an optimal convergence for locality learning. While Structural Re-parameterization (SR) enhances the local convergence with small kernels in parallel, optimal small kernel branches may hinder the computational efficiency for training. In this work, we propose RepUX-Net, a pure CNN architecture with a simple large kernel block design, which competes favorably with current network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6 challenging public datasets. We derive an equivalency between kernel re-parameterization and the branch-wise variation in kernel convergence. Inspired by the spatial frequency in the human visual system, we extend to vary the kernel convergence into element-wise setting and model the spatial frequency as a Bayesian prior to re-parameterize convolutional weights during training. Specifically, a reciprocal function is leveraged to estimate a frequency-weighted value, which rescales the corresponding kernel element for stochastic gradient descent. From the experimental results, RepUX-Net consistently outperforms 3D SOTA benchmarks with internal validation (FLARE: 0.929 to 0.944), external validation (MSD: 0.901 to 0.932, KiTS: 0.815 to 0.847, LiTS: 0.933 to 0.949, TCIA: 0.736 to 0.779) and transfer learning (AMOS: 0.880 to 0.911) scenarios in Dice Score. Both codes and pre-trained models are available at: Both codes and pre-trained models are available at: https://github.com/MASILab/RepUX-Net.",https://github.com/MASILab/RepUX-Net,https://amos22.grand-challenge.org/,Image Segmentation,Other,,,,,,,,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans ,"Abdominal Aortic Calcification (AAC) is a known marker of
asymptomatic Atherosclerotic Cardiovascular Diseases (ASCVDs). AAC
can be observed on Vertebral Fracture Assessment (VFA) scans acquired
using Dual-Energy X-ray Absorptiometry (DXA) machines. Thus, the
automatic quantification of AAC on VFA DXA scans may be used to
screen for CVD risks, allowing early interventions. In this research, we
formulate the quantification of AAC as an ordinal regression problem.
We propose a novel Supervised Contrastive Ordinal Loss (SCOL) by in-
corporating a label-dependent distance metric with existing supervised
contrastive loss to leverage the ordinal information inherent in discrete
AAC regression labels. We develop a Dual-encoder Contrastive Ordinal
Learning (DCOL) framework that learns the contrastive ordinal representation at global and local levels to improve the feature separability
and class diversity in latent space among the AAC-24 genera. We evaluate the performance of the proposed framework using two clinical VFA
DXA scan datasets and compare our work with state-of-the-art methods.
Furthermore, for predicted AAC scores, we provide a clinical analysis to
predict the future risk of a Major Acute Cardiovascular Event (MACE).
Our results demonstrate that this learning enhances inter-class separability and strengthens intra-class consistency, which results in predicting
the high-risk AAC classes with high sensitivity and high accuracy.",https://github.com/AfsahS/Supervised-Contrastive-Ordinal-Loss-for-Ordinal-Regression,,Computer Aided Diagnosis,Transfer learning,other,,,,,,,
Scribble-based 3D Multiple Abdominal Organ Segmentation via Triple-branch Multi-dilated Network with Pixel- and Class-wise Consistency ,"Multi-organ segmentation in abdominal Computed Tomography (CT) images is of great importance for diagnosis of abdominal lesions and subsequent treatment planning. Though deep learning based methods have attained high performance, they rely heavily on large-scale pixel-level annotations that are time-consuming and labor-intensive to obtain. Due to its low dependency on annotation, weakly supervised segmentation has attracted great attention. However, there is still a large performance gap between current weakly-supervised methods and fully supervised learning, leaving room for exploration. In this work, we propose a novel 3D framework with two consistency constraints for scribble-supervised multiple abdominal organ segmentation from CT. Specifically, we employ a Triple-branch multi-Dilated network (TDNet) with one encoder and three decoders using different dilation rates to capture features from different receptive fields that are complementary to each other to generate high-quality soft pseudo labels. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then supervise the outputs of each decoder. To further regularize the network, class relationship information is exploited by encouraging the generated class affinity matrices to be consistent across different decoders under multi-view projection. Experiments on the public WORD dataset show that our method outperforms five existing scribble-supervised methods.",,,Abdomen,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,,,,,,
Second-course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information ,"Esophageal cancer is a significant global health concern, and radiotherapy (RT) is a common treatment option. Accurate delineation of the gross tumor volume (GTV) is essential for optimal treatment outcomes. In clinical practice, patients may undergo a second round of RT to achieve complete tumor control when the first course of treatment fails to eradicate cancer completely. However, manual delineation is labor-intensive, and automatic segmentation of esophageal GTV is difficult due to the ambiguous boundary of the tumor. Detailed tumor information naturally exists in the previous stage, however the correlation between the first and second course RT is rarely explored. In this study, we first reveal the domain gap between the first and second course RT, and aim to improve the accuracy of GTV delineation in the second course RT by incorporating prior information from the first course. We propose a novel prior Anatomy and RT information enhanced Second-course Esophageal GTV segmentation network (ARTSEG). A region-preserving attention module (RAM) is designed to understand the long-range prior knowledge of the esophageal structure, while preserving the regional patterns. Sparsely labeled medical images for various isolated tasks necessitate efficient utilization of knowledge from relevant datasets and tasks. To achieve this, we train our network in an information-querying manner. ARTSEG incorporates various prior knowledge, including: 1) Tumor volume variation between first and second RT courses, 2) Cancer cell proliferation, and 3) Reliance of GTV on esophageal anatomy. Extensive quantitative and qualitative experiments validate our designs.",,https://competitions.codalab.org/competitions/21145,Oncology,Image Segmentation,Attention models,Data Efficient Learning,CT,,,,,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos ,"Thoracoscopy-assisted mitral valve replacement (MVR), an important treatment for mitral regurgitation patients, requires higher surgical skills to prevent avoidable complications and improve patient outcomes. Hence, the surgical skill assessment (SKA) for MVR is essential for surgical training and certification for novice surgeons. Current automatic SKA approaches suffer from several inherent limitations,~\eg, no public thoracoscopy-assisted surgery datasets, the ignoration of inter-video relations, and restricted to SKA of a single short surgical action. In this paper, we collect a new clinical dataset for MVR, which is the first thoracoscopy-assisted long-form surgery dataset to the best of our knowledge. Unlike a short clip that only contains a single action, videos in our datasets record the whole procedure of MVR consisting of multiple complex skill-related surgical events. To tackle the challenges posed by MVR, we propose a novel baseline named \textbf{S}urgical \textbf{E}vent \textbf{D}riven \textbf{S}kill assessment (SEDSkill), a long-form and surgical event driven method. Compared to current methods that capture the intra-video semantics for the global video, our proposed SEDSkill contains a local-global relative module to learn the inter-video relations for both global long-form and local surgical events correlated semantics.  Specifically, an event-aware module is designed to localize skill-related events from long-form videos automatically, thus extracting the local semantics. Furthermore, we introduce a relative regression block to learn the imperceptible discrepancies for accurately assessing surgical skills. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art approaches in the MVR scenario.",https://github.com/xmed-lab/SEDSkill,,Surgical Skill and Work Flow Analysis,Surgical Scene Understanding,,,,,,,,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-level Supervision ,"Accurate segmentation and analysis of membranes from immunohistochemical (IHC) images are crucial for cancer diagnosis and prognosis. Although several fully-supervised deep learning methods for membrane segmentation from IHC images have been proposed recently, the high demand for pixel-level annotations makes this process time-consuming and labor-intensive. To overcome this issue, we propose a novel deep framework for membrane segmentation that utilizes nuclei point-level supervision. Our framework consists of two networks: a Seg-Net that generates segmentation results for membranes and nuclei, and a Tran-Net that transforms the segmentation into semantic points. In this way, the accuracy of the semantic points is closely related to the segmentation quality. Thus, the inconsistency between the semantic points and the point annotations can be used as effective supervision for cell segmentation. We evaluated the proposed method on two IHC membrane-stained datasets and achieved an 81.36% IoU and 85.51% F_1 score of the fully supervised method.  All source codes are available at here.",https://github.com/Lion-shine/Segment-Membranes-and-Nuclei-from-Histopathological-Images-via-Nuclei-Point-level-Supervision,,Histopathology,Image Segmentation,,,,,,,,
Segmentation Distortion: Quantifying Segmentation Uncertainty under Domain Shift via the Effects of Anomalous Activations ,"Domain shift occurs when training U-Nets for medical image segmentation with images from one device, but applying them to images from a different device. This often reduces accuracy, and it poses a challenge for uncertainty quantification, when incorrect segmentations are produced with high confidence. Recent work proposed to detect such failure cases via anomalies in feature space: Activation patterns that deviate from those observed during training are taken as an indication that the input is not handled well by the network, and its output should not be trusted. However, such latent space distances primarily detect whether images are from different scanners, not whether they are correctly segmented. Therefore, we propose a novel segmentation distortion measure for uncertainty quantification. It is based on using an autoencoder to make activations more similar to those that were observed during training, and propagating the result through the remainder of the U-Net. We demonstrate that the extent to which this affects the segmentation correlates much more strongly with segmentation errors than distances in activation space, and that it quantifies uncertainty under domain shift better than entropy in the U-Net’s output.",https://github.com/MedVisBonn/Segmentation-Distortion/,https://portal.conp.ca/dataset?id=projects/calgary-campinas,Uncertainty,Image Segmentation,,,,,,,,
Segmentation of Kidney Tumors on Non-Contrast CT Images using Protuberance Detection Network ,"Many renal cancers are incidentally found on non-contrast CT (NCCT) images. On contrast-enhanced CT (CECT) images, most kidney tumors, especially renal cancers, have different intensity values compared to normal tissues. However, on NCCT images, some tumors called isodensity tumors, have similar intensity values to the surrounding normal tissues, and can only be detected through a change in organ shape. Several deep learning methods which segment kidney tumors from CECT images have been proposed and showed promising results. However, these methods fail to capture such changes in organ shape on NCCT images. In this paper, we present a novel framework, which can explicitly capture protruded regions in kidneys to enable a better segmentation of kidney tumors. We created a synthetic mask dataset that simulates a protuberance, and trained a segmentation network to separate the protruded regions from the normal kidney regions. To achieve the segmentation of whole tumors, our framework consists of three networks. The first network is a conventional semantic segmentation network which extracts a kidney region mask and an initial tumor region mask. The second network, which we name protuberance detection network, identifies the protruded regions from the kidney region mask. Given the initial tumor region mask and the protruded region mask, the last network fuses them and predicts the final kidney tumor mask accurately. The proposed method was evaluated on a publicly available KiTS19 dataset, which contains 108 NCCT images, and showed that our method achieved a higher dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared to 3D-UNet. To the best of our knowledge, this is the first deep learning method that is specifically designed for kidney tumor segmentation on NCCT images.",,https://github.com/neheller/kits19/blob/master/LICENSE,Abdomen,Image Segmentation,CT,,,,,,,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation ,"The digitization of surgical operating rooms (OR) has gained significant traction in the scientific and medical communities. 
However, existing deep-learning methods for operating room recognition tasks still require substantial quantities of annotated data.
In this paper, we introduce a method for weakly-supervised semantic segmentation for surgical operating rooms.
Our method operates directly on 4D point cloud sequences from multiple ceiling-mounted RGB-D sensors and requires less than 0.01\% of annotated data.
This is achieved by incorporating a self-supervised temporal prior, enforcing semantic consistency in 4D point cloud video recordings.
We show how refining these priors with learned semantic features can increase segmentation mIoU to $10\%$ above existing works, achieving higher segmentation scores than baselines that use four times the number of labels.
Furthermore, the 3D semantic predictions from our method can be projected back into 2D images; we establish that these 2D predictions can be used to improve the performance of existing surgical phase recognition methods.
Our method shows promise in automating 3D OR segmentation with a 20 times lower annotation cost than existing methods, demonstrating the potential to improve surgical scene understanding systems.",https://github.com/bastianlb/segmentOR,https://bastianlb.github.io/segmentOR/,Surgical Scene Understanding,Computer Aided Diagnosis,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Video,Surgical Data Science,Surgical Skill and Work Flow Analysis,,,
SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks ,"Recently, U-shaped networks have dominated the field of medical image segmentation due to their simple and easily tuned structure. However, existing U-shaped segmentation networks: 1) mostly focus on designing complex self-attention modules to compensate for the lack of long-term dependence based on convolution operation, which increases the overall number of parameters and computational complexity of the network; 2) simply fuse the features of encoder and decoder, ignoring the connection between their spatial locations. In this paper, we rethink the above problem and build a lightweight medical image segmentation network, called SegNetr. Specifically,  we introduce a novel SegNetr block that can perform local-global interactions dynamically at any stage and with only linear complexity. At the same time, we design a general information retention skip connection (IRSC) to preserve the spatial location information of encoder features and achieve accurate fusion with the decoder features. We validate the effectiveness of SegNetr on four mainstream medical image segmentation datasets, with 59% and 76% fewer parameters and GFLOPs than vanilla U-Net, while achieving segmentation performance comparable to state-of-the-art methods. Notably, the components proposed in this paper can be applied to other U-shaped networks to improve their segmentation performance.",,,Image Segmentation,Computer Aided Diagnosis,Attention models,,,,,,,
Self- and Semi-Supervised Learning for Gastroscopic Lesion Detection ,"Gastroscopic Lesion Detection (GLD) plays a key role in computer-assisted diagnostic procedures. However, this task is not well studied in the literature due to the lack of labeled data and the applicable methods. Generic detectors perform below expectations on GLD tasks for 2 reasons: 1) The scale of labeled data of GLD datasets is far smaller than that of natural-image object detection datasets. 2) Gastroscopic lesions exhibit distinct differences from objects in natural images, which are usually of high similarity in global but high diversity in local. Such characteristic of gastroscopic lesions also degrades the performance of generic self-supervised or semi-supervised methods to solve the labeled data shortage problem using massive unlabeled data. In this paper, we propose Self- and Semi-Supervised Learning (SSL) for GLD tailored for using massive unlabeled gastroscopic images to enhance GLD tasks performance, which consists of a Hybrid Self-Supervised Learning (HSL) method for backbone pre-training and a Prototype-based Pseudo-label Generation (PPG) method for semi-supervised detector training. The HSL combines patch reconstruction with dense contrastive learning to boost their advantages in feature learning from massive unlabeled data. The PGG generates pseudo-labels for unlabeled data based on similarity to the prototype feature vector to discover potential lesion and avoid introducing much noise. Moreover, we contribute the first Large-scale GLD Datasets (LGLDD), which contains 10,083 gastroscopic images with 12,292 well-annotated bounding boxes for four categories of lesions. Experiments on LGLDD demonstrate that SSL can bring significant improvement compared with baseline methods in GLD.",,,Computer Aided Diagnosis,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Self-adaptive Adversarial Training for Robust Medical Segmentation ,"Adversarial training has been demonstrated to be one of the most effective approaches to training deep neural networks that are robust to malicious perturbations. Research on effectively applying it to produce robust 3D medical image segmentation models is ongoing. While few empirical studies have been done in this area, developing effective adversarial training methods for complex segmentation models and high-volume 3D examples is challenging and requires theoretical support. In this paper, we consider the robustness of 3D segmentation tasks from a PAC-Bayes generalisation perceptive and show that reducing the trained models’ Lipschitz constant benefits the models’ robustness performance. Demonstrating by empirical investigation, we show that adjusting the adversarial iteration can help to reduce the model’s Lipschitz constant, enabling a self-adaptive adversarial training strategy. Empirical studies on the medical segmentation decathlon dataset have been done to demonstrate the efficiency of the proposed adversarial training method. Our implementation is available at https://github.com/TrustAI/SEAT.",https://github.com/TrustAI/SEAT,http://medicaldecathlon.com/,Image Segmentation,Model Generalizability / Federated Learning,,,,,,,,
Self-aware and Cross-sample Prototypical Learning for Semi-supervised Medical Image Segmentation ,"Consistency learning plays a crucial role in semi-supervised medical image segmentation as it enables the effective utilization of limited annotated data while leveraging the abundance of unannotated data. The effectiveness and efficiency of consistency learning are challenged by prediction diversity and training stability, which are often overlooked by existing studies. Meanwhile, the limited quantity of labeled data for training often proves inadequate for formulating intra-class compactness and inter-class discrepancy of pseudo labels. 
To address these issues, we propose a self-aware and cross-sample prototypical learning method (SCP-Net) to enhance the diversity of prediction in consistency learning by utilizing a broader range of semantic information derived from multiple inputs. Furthermore, we introduce a self-aware consistency learning method which exploits unlabeled data to improve the compactness of pseudo labels within each class. Moreover, a dual loss re-weighting method is integrated into the cross-sample prototypical consistency learning method to improve the reliability and stability of our model. Extensive experiments on ACDC dataset and PROMISE12 dataset validate that SCP-Net outperforms other state-of-the-art semi-supervised segmentation methods and achieves significant performance gains compared to the limited supervised training.",https://github.com/Medsemiseg/SCP-Net,,Data Efficient Learning,CT,MRI,,,,,,,
Self-distillation for surgical action recognition ,"Surgical scene understanding is a key prerequisite for context-aware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transfomers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the challenge
organizers. According to their analysis, our method outperforms all other solutions submitted to the latest challenge in the field. Our approach thus shows the potential of self-distillation for becoming an important tool in medical image analysis applications.",,https://github.com/CAMMA-public/cholect45,Surgical Scene Understanding,Surgical Data Science,Surgical Skill and Work Flow Analysis,,,,,,,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-world Pancreatic Neuroendocrine Neoplasms Data ,"CAD is an emerging field, but most models are not equipped to handle missing and noisy data in real-world medical scenarios, particularly in the case of rare tumors like pancreatic neuroendocrine neoplasms (pNENs). Multi-label models meet the needs of real-world study, but current methods do not consider the issue of missing and noisy labels. This study introduces a multi-label model called Self-feedback Transformer (SFT) that utilizes a transformer to model the relationships between labels and images, and uses a ingenious self-feedback strategy to improve label utilization. We evaluated SFT on 11 clinical tasks using a real-world dataset of pNENs and achieved higher performance than other state-of-the-art multi-label models with mAUCs of 0.68 and 0.76 on internal and external datasets, respectively. Our model has four inference modes that utilize self-feedback and expert assistance to further increase mAUCs to 0.72 and 0.82 on internal and external datasets, respectively, while maintaining good performance even with input label noise ratios up to 40% in expert-assisted mode.",,,Oncology,Vascular,Computer Aided Diagnosis,,,,,,,
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images ,"Multiple Sclerosis (MS) is a severe neurological disease characterized by inflammatory lesions in the central nervous system. Hence, predicting inflammatory disease activity is crucial for disease assessment and treatment. However, MS lesions can occur throughout the brain and vary in shape, size and total count among patients. The high variance in lesion load and locations makes it challenging for machine learning methods to learn a globally effective representation of whole-brain MRI scans to assess and predict disease. Technically it is non-trivial to incorporate essential biomarkers such as lesion load or spatial proximity. Our work represents the first attempt to utilize graph neural networks (GNN) to aggregate these biomarkers for a novel global representation. We propose a two-stage MS inflammatory disease activity prediction approach. First, a 3D segmentation network detects lesions, and a self-supervised algorithm extracts their image features. Second, the detected lesions are used to build a patient graph. The lesions act as nodes in the graph and are initialized with image features extracted in the first stage. Finally, the lesions are connected based on their spatial proximity and the inflammatory disease activity prediction is formulated as a graph classification task. Furthermore, we propose a self-pruning strategy to auto-select the most critical lesions for prediction. Our proposed method outperforms the existing baseline by a large margin (AUCs of 0.67 vs. 0.61 and 0.66 vs 0.60 for one-year and two-year inflammatory disease activity, respectively). Finally, our proposed method enjoys inherent explainability by assigning an importance score to each lesion for the overall prediction. Code is available at https://github.com/chinmay5/ms_ida.git",https://github.com/chinmay5/ms_progression,,Neuroimaging - Others,Interpretability / Explainability,MRI,Treatment Response and Outcome/Disease Prediction,Visualization in Biomedical Imaging,,,,,
Self-supervised dense representation learning for live-cell microscopy with time arrow prediction ,"State-of-the-art object detection and segmentation methods for microscopy images rely on supervised machine learning, which requires laborious manual annotation of training data. Here we present a self-supervised method based on time arrow prediction pre-training that learns dense image representations from raw, unlabeled live-cell microscopy videos. Our method builds upon the task of predicting the correct order of time-flipped image regions via a single-image feature extractor followed by a time arrow prediction head that operates on the fused features. We show that the resulting dense representations capture inherently time-asymmetric biological processes such as cell divisions on a pixel-level. We furthermore demonstrate the utility of these representations on several live-cell microscopy datasets for detection and segmentation of dividing cells, as well as for cell state classification. Our method outperforms supervised methods, particularly when only limited ground truth annotations are available as is commonly the case in practice. We provide code at https://github.com/weigertlab/tarrow.",https://github.com/weigertlab/tarrow,http://data.celltrackingchallenge.net/training-datasets/Fluo-N2DL-HeLa.zip,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,Data Efficient Learning,Microscopy,Video,,,,,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning ,"Unsupervised domain adaptation (UDA) has attracted much attention in imaging-based diagnosis, due partly to the difficulty in labeling a large number of datasets in target domains, which otherwise adversely affects the diagnostic performance of well-trained deep learning models in a source domain. UDA has enabled deep learning models to make use of large-scale datasets that are acquired in various domains for model deployment. However, UDA has deficiencies in carrying out adaptive feature extraction, when dealing with data without their labels in a target unseen domain. To alleviate this, we propose advanced test-time fine-tuning UDA to better utilize latent features of datasets in an unseen target domain at diagnosis. Specifically, our framework is based on an auto-encoder-based network architecture that fine-tunes the model itself, where our framework learns knowledge pertaining to an unseen target domain at the fine-tuning phase. Additionally, a re-initialization module is introduced to inject randomness into network parameters so that our framework is optimized to a local minimum that is well-suited for an unseen target domain. We also provide a mathematical justification to demonstrate the benefits of our framework for better feature extraction. We carried out experiments on UDA segmentation tasks using breast cancer datasets acquired from multiple domains. Experimental results showed that our framework achieved state-of-the-art performance, compared with other competing UDA models, in segmenting breast cancer on ultrasound images from an unseen domain, which supports its clinical potential in better diagnosing breast cancer in various target domains.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Image Segmentation,Ultrasound,,,,,,
Self-Supervised Learning for Endoscopic Video Analysis ,"Self-supervised learning (SSL) has led to important breakthroughs in computer vision by allowing learning from large amounts of unlabeled data. As such, it might have a pivotal role to play in biomedicine where annotating data requires a highly specialized expertise. Yet, there are many healthcare domains for which SSL has not been extensively explored. One such domain is endoscopy, minimally invasive procedures which are commonly used to detect and treat infections, chronic inflammatory diseases or cancer. In this work, we study the use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit the power of SSL, we create sizable unlabeled endoscopic video datasets for training MSNs. These strong image representations serve as a foundation for secondary training with limited annotated datasets, resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition during laparoscopy and colonoscopic polyp characterization. Additionally, we achieve a 50% reduction in annotated data size without sacrificing performance. Thus, our work provides evidence that SSL can dramatically reduce the need of annotated data in endoscopy.",https://github.com/RoyHirsch/endossl,,Computer Aided Diagnosis,Abdomen,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,Video,Surgical Data Science,Surgical Scene Understanding,,
Self-Supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET ,"Dynamic Positron Emission Tomography imaging (dPET) provides temporally resolved images of a tracer. Voxel-wise physiologically-based pharmacokinetic modeling of the Time Activity Curves (TAC) extracted from dPET can provide relevant diagnostic information for clinical workflow. Conventional fitting strategies for TACs are slow and ignore the spatial relation between neighboring voxels. We train a spatio-temporal UNet to estimate the kinetic parameters given TAC from dPET. This work introduces a self-supervised loss formulation to enforce the similarity between the measured TAC and those generated with the learned kinetic parameters. Our method provides quantitatively comparable results at organ level to the significantly slower conventional approaches while generating pixel-wise kinetic parametric images which are consistent with expected physiology. To the best of our knowledge, this is the first self-supervised network that allows voxel-wise computation of kinetic parameters consistent with a non-linear kinetic model.",https://github.com/FrancescaDB/self_supervised_PBPK_modelling,,Other,Oncology,Computational Anatomy and Physiology,Semi-/Weakly-/Un-/Self-supervised Representation Learning,PET/SPECT,,,,,
Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation ,"Deep learning has become a valuable tool for the automation of certain medical image segmentation tasks, significantly relieving the workload of medical specialists. Some of these tasks require segmentation to be performed on a subset of the input dimensions, the most common case being 3D-to-2D. However, the performance of existing methods is strongly conditioned by the amount of labeled data available, as there is currently no data efficient method, e.g. transfer learning, that has been validated on these tasks. In this work, we propose a novel convolutional neural network (CNN) and self-supervised learning (SSL) method for label-efficient 3D-to-2D segmentation. The CNN is composed of a 3D encoder and a 2D decoder connected by novel 3D-to-2D blocks. The SSL method consists of reconstructing image pairs of modalities with different dimensionality. The approach has been validated in two tasks with clinical relevance: the en-face segmentation of geographic atrophy and reticular pseudodrusen in optical coherence tomography. Results on different datasets demonstrate that the proposed CNN significantly improves the state of the art in scenarios with limited labeled data by up to 8% in Dice score. Moreover, the proposed SSL method allows further improvement of this performance by up to 23%, and we show that the SSL is beneficial regardless of the network architecture. Our code is available at https://github.com/j-morano/multimodal-ssl-fpn.",https://github.com/j-morano/multimodal-ssl-fpn,,Image Segmentation,Ophthalmology,Data Efficient Learning,Transfer learning,,,,,,
Self-Supervised MRI Reconstruction with Unrolled Diffusion Models ,"Magnetic Resonance Imaging (MRI) produces excellent soft tissue contrast, albeit it is an inherently slow imaging modality. Promising deep learning methods have recently been proposed to reconstruct accelerated MRI scans. However, existing methods still suffer from various limitations regarding image fidelity, contextual sensitivity, and reliance on fully-sampled acquisitions for model training. To comprehensively address these limitations, we propose a novel self-supervised deep reconstruction model, named Self-Supervised Diffusion Reconstruction (SSDiffRecon). SSDiffRecon expresses a conditional diffusion process as an unrolled architecture that interleaves cross-attention transformers for reverse diffusion steps with data-consistency blocks for physics-driven processing. Unlike recent diffusion methods for MRI reconstruction, a self-supervision strategy is adopted to train SSDiffRecon using only undersampled k-space data. Comprehensive experiments on public brain MR datasets demonstrates the superiority of SSDiffRecon against state-of-the-art supervised, and self-supervised baselines in terms of reconstruction speed and quality. Implementation will be available at https://github.com/yilmazkorkmaz1/SSDiffRecon.",https://github.com/yilmazkorkmaz1/ssdiffrecon,,Other,Image Reconstruction,Attention models,,,,,,,
Self-Supervised Polyp Re-Identification in Colonoscopy ,"Computer-aided polyp detection (CADe) is becoming a standard, integral part of any modern colonoscopy system. A typical colonoscopy CADe detects a polyp in a single frame and does not track it through the video sequence. Yet, many downstream tasks including polyp characterization (CADx), quality metrics, automatic reporting, require aggregating polyp data from multiple frames. In this work we propose a robust long term polyp tracking method based on re-identification by visual appearance. Our solution uses an attention-based self-supervised ML model, specifically designed to leverage the temporal nature of video input. We quantitatively evaluate method’s performance and demonstrate its value for the CADx task.",,,Computer Aided Diagnosis,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Video,,,,,,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-based Assessment of Intraoperative Suturing Skills ,"Suturing technical skill scores are strong predictors of patient functional recovery following robot-assisted radical prostatectomy, but manual assessment of these skills is a time and resource-intensive process. By automating suturing skill scoring through computer vision (CV) methods, we can significantly reduce the burden on healthcare professionals and enhance the quality and quantity of educational feedback. Although automated skill assessment on simulated virtual reality (VR) environments have been promising, applying CV methods to live (`real’) surgical videos has been challenging due to: 1) the lack of kinematic data from the da Vinci surgical system, a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles, and 2) the lack of training data due to the labor-intensive task of segmenting and scoring individual stitches from live videos. To address these challenges, we developed a self-supervised pre-training paradigm whereby sim-to-real generalizable representations are learned without requiring any live kinematics. Our model is based on a masked autoencoder, termed as LiveMAE. We augment live stitches with VR images during pre-training and require LiveMAE to reconstruct images from both domains while also predicting the corresponding kinematics. This process learns a visual-to-kinematic mapping that seeks to locate the positions and orientations of surgical manipulators and needles, deriving “kinematics” from live videos without requiring supervision. With an additional skill-specific finetuning step, LiveMAE surpasses supervised learning approaches across 6 technical skill assessments, ranging from 0.56-0.84 AUC (0.70-0.91 AUPRC), with improvements of 35.8% in AUC for wrist rotation and 8.7% for needle driving skills. Our contributions provide the foundation to deliver personalized feedback to surgeons training in VR and performing live prostatectomy procedures.",,,Surgical Data Science,Other,other,Video,Surgical Scene Understanding,Surgical Skill and Work Flow Analysis,,,,
Semantic difference guidance for the uncertain boundary segmentation of CT left atrial appendage ,"Atrial fibrillation (AF) is one of the most common types of cardiac
arrhythmia, which is closely relevant to anatomical structures including the left atrium (LA) and the left atrial appendage (LAA). Thus, a thorough understanding of the LA and LAA is essential for the AF treatment. In this paper, we have modeled relative relations between the LA and LAA via deep segmentation networks for the first time, and introduce a new LA & LAA CT dataset. To deal with uncertain boundaries between the LA and LAA, we propose the semantic difference module (SDM) based on diffusion theory to refine features with enhanced boundary information. Besides, disconnections between the LA and LAA are frequently observed in the segmentation results due to uncertain boundaries of the LAA region and CT imaging noise. To address this issue, we devise another connectivity-refined network with the connectivity loss. The loss function exerts a distance regularization on coarse predictions from the first-stage network. Experiments demonstrate that our proposed model can achieve state-of-the-art segmentation performance compared with classic convolutional-neural-networks (CNNs) and recent Transformer-based models on this new dataset. Specifically, SDM can also outperform existing methods on refining uncertain boundaries. Codes are available at https://github.com/AlexYouXin/LA-LAA-segmentation.",https://github.com/AlexYouXin/LA-LAA-segmentation,,Cardiac,Image Segmentation,CT,Surgical Planning and Simulation,,,,,,
Semantic segmentation of surgical hyperspectral images under geometric domain shifts ,"Robust semantic segmentation of intraoperative image data could pave the way for automatic surgical scene understanding and autonomous robotic surgery. Geometric domain shifts, however – although common in real-world open surgeries due to variations in surgical procedures or situs occlusions – remain a topic largely unaddressed in the field. To address this gap in the literature, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation networks in the presence of geometric out-of-distribution (OOD) data, and (2) address generalizability with a dedicated augmentation technique termed ’Organ Transplantation’ that we adapted from the general computer vision community. According to a comprehensive validation on six different OOD data sets comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs semantically annotated with 19 classes, we demonstrate a large performance drop of SOA organ segmentation networks applied to geometric OOD data. Surprisingly, this holds true not only for conventional RGB data (drop of Dice similarity coefficient (DSC) by 46 %) but also for HSI data (drop by 45 %), despite the latter’s rich information content per pixel. Using our augmentation scheme improves on the SOA DSC by up to 67 % (RGB) and 90 % (HSI) and renders performance on par with in-distribution performance on real OOD test data. The simplicity and effectiveness of our augmentation scheme makes it a valuable network-independent tool for addressing geometric domain shifts in semantic scene segmentation of intraoperative data. Our code and pre-trained models are available at https://github.com/IMSY-DKFZ/htc.",https://github.com/IMSY-DKFZ/htc,,Surgical Scene Understanding,Image Segmentation,Guided Interventions and Surgery,Interventional Imaging Systems,Biophotonics,Surgical Data Science,,,,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery ,"Swept-Source Optical Coherence Tomography (SS-OCT) integrated with surgical microscopes has enabled fast, high-resolution, and volumetric visualization of delicate tissue-instrument interactions. However, some visual features, which provide essential perceptual information in microscopic surgery, are not present in 4D OCT. Such a feature is the shadow of the surgical instruments cast onto the retina by the endo-illumination probe, which is among the most important cognitive cues for perceptual distance estimation. In this work, we propose Semantic Virtual Shadows (SVS), a novel concept to artificially generate instrument-specific shadows in OCT volumes, enabling naturally non-existent but important perceptual cues that are present in microscopic surgery. Semantic scene information is leveraged by considering only voxels associated with shadow-casting and shadow-receiving objects, identified using a learning-based approach and efficient volume processing, respectively. Real-time performance is achieved by a precomputed semantic shadow volume texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object. The novelty of the method includes not only instrument-specific shadowing on the surface anatomy but also exclusively on deep-seated subsurface structures, providing advantages for various vitreoretinal procedures. Our user study indicates the benefits of the method for 4D OCT-guided surgery in several cognitive and performance-specific aspects.",,,Surgical Visualization and Mixed/Augmented/Virtual Reality,Ophthalmology,Guided Interventions and Surgery,Visualization in Biomedical Imaging,,,,,,
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation ,"Despite great progress in semi-supervised learning (SSL) that leverages unlabeled data to improve the performance over fully supervised models, existing SSL approaches still fail to exhibit good results when faced with a severe class imbalance problem in medical image segmentation. In this work, we propose a novel Mean-teacher based class imbalanced learning framework for cardiac magnetic resonance imaging (MRI) segmentation, which can effectively conquer the problems of class imbalance and limited labeled data simultaneously. Specifically, in parallel to the traditional linear-based classifier, we additionally train a prototype-based classifier that makes dense predictions by matching test samples with a set of prototypes. The prototypes are iteratively updated by in-class features encoded in the entire sample set, which can better guide the model training by alleviating the class-wise bias exhibited in each individual sample. To reduce the noises in the pseudo labels, we propose a cascaded refining strategy by utilizing two multi-level tree filters that are built upon pairwise pixel similarity in terms of intensity values and semantic features. With the assistance of these affinities, soft pseudo labels are properly refined on-the-fly. Upon evaluation on ACDC and MMWHS, two cardiac MRI datasets with prominent class imbalance problem, the proposed method demonstrates the superiority compared to several state-of-the-art methods, especially in the case where few annotations are available.",https://github.com/IsYuchenYuan/SSCI,https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html,Image Segmentation,Cardiac,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning ,"Although unsupervised domain adaptation (UDA) is a promising direction to alleviate domain shift, they fall short of their supervised counterparts. In this work, we investigate relatively less explored semi-supervised domain adaptation (SSDA) for medical image segmentation, where access to a few labeled target samples can improve the adaptation performance substantially. Specifically, we propose a two-stage training process. First, an encoder is pre-trained in a self-learning paradigm using a novel domain-content disentangled contrastive learning (CL) along with a pixel-level feature consistency constraint. The proposed CL enforces the encoder to learn discriminative content-specific but domain-invariant semantics on a global scale from the source and target images, whereas consistency regularization enforces the mining of local pixel-level information by maintaining spatial sensitivity. This pre-trained encoder, along with a decoder, is further fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a semi-supervised setting. Furthermore, we experimentally validate that our proposed method can easily be extended for UDA settings, adding to the superiority of the proposed strategy. Upon evaluation on two domain adaptive image segmentation tasks, our proposed method outperforms the SoTA methods, both in SSDA and UDA settings. Codes will be released.",https://github.com/hritam-98/GFDA-disentangled,https://datasets.simula.no/kvasir-seg/,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions ,"Segmentation of pathological images is a crucial step for accurate cancer diagnosis. However, acquiring dense annotations of such images for training is labor-intensive and time-consuming. To address this issue, Semi-Supervised Learning (SSL) has the potential for reducing the annotation cost, but it is challenged by the large amount of unlabeled training images. In this paper, we propose a novel SSL method based on Cross Distillation of Multiple Attentions (CDMA) to effectively leverage unlabeled images. Firstly, we propose a Multi-attention Tri-branch Network (MTNet) that consists of an encoder and a three branch decoder, with each branch using a different attention mechanism that calibrates features in different aspects to generate diverse outputs. Secondly, we introduce Cross Decoder Knowledge Distillation (CDKD)between the three decoder branches, allowing them to learn from each other’s soft labels to mitigate the negative impact of incorrect pseudo labels in training. Additionally, uncertainty minimization is applied to the average prediction of the three branches, which further regularizes predictions on unlabeled images and encourages inter-branch consistency. Our proposed CDMA was compared with eight state-of-the-art SSL methods on the public DigestPath dataset, and the experimental results showed that our method outperforms the other approaches under different annotation ratios.",,,Histopathology,Image Segmentation,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking ,"Deformable tracking and real-time estimation of 3D tissue motion is essential to enable automation and image guidance applications in robotically assisted surgery.
Our model, Sparse Efficient Neural Depth and Deformation (SENDD), extends prior 2D tracking work to estimate flow in 3D space.
SENDD introduces novel contributions of learned detection, and sparse per-point depth and 3D flow estimation, all with less than half a million parameters.
SENDD does this by using graph neural networks of sparse keypoint matches to estimate both depth and 3D flow anywhere.
We quantify and benchmark SENDD on a comprehensively labelled tissue dataset, and compare it to an equivalent 2D flow model.
SENDD performs comparably while enabling applications that 2D flow cannot.
SENDD can track points and estimate depth at 10fps on an NVIDIA RTX 4000 for 1280 tracked (query) points and its cost scales linearly with an increasing/decreasing number of points.
SENDD enables multiple downstream applications that require estimation of 3D motion in stereo endoscopy.",,,Guided Interventions and Surgery,Image Reconstruction,Image Registration,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Video,Surgical Visualization and Mixed/Augmented/Virtual Reality,,,,
SFusion: Self-attention based N-to-One Multimodal Fusion Block ,"People perceive the world with different senses, such as sight, hearing, smell, and touch. Processing and fusing information from multiple modalities enables Artificial Intelligence to understand the world around us more easily. However, when there are missing modalities, the number of available modalities is different in diverse situations, which leads to an N-to-One fusion problem. To solve this problem, we propose a self-attention based fusion block called SFusion. Different from preset formulations or convolution based methods, the proposed block automatically learns to fuse available modalities without synthesizing or zero-padding missing ones. Specifically, the feature representations extracted from upstream processing model are projected as tokens and fed into self-attention module to generate latent multimodal correlations. Then, a modal attention mechanism is introduced to build a shared representation, which can be applied by the downstream decision model. The proposed SFusion can be easily integrated into existing multimodal analysis networks. In this work, we apply SFusion to different backbone networks for human activity recognition and brain tumor segmentation tasks. Extensive experimental results show that the SFusion block achieves better performance than the competing fusion strategies. Our code is available at https://github.com/scut-cszcl/SFusion.",https://github.com/scut-cszcl/SFusion,https://www.med.upenn.edu/cbica/brats2020/,Data Efficient Learning,Image Segmentation,MRI,,,,,,,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention ,"The automated segmentation and analysis of small vessels from \emph{in vivo} imaging data is an important task for many clinical applications. While current filtering and learning methods have achieved good performance on the segmentation of large vessels, they are sub-optimal for small vessel detection due to their apparent geometric irregularity and weak contrast given the relatively limited resolution of existing imaging techniques. In addition, for supervised learning approaches, the acquisition of accurate pixel-wise annotations in these small vascular regions heavily relies on skilled experts. In this work, we propose a novel self-supervised network to tackle these challenges and improve the detection of small vessels from 3D imaging data. First, our network maximizes a novel shape-aware flux-based measure to enhance the estimation of small vasculature with non-circular and irregular appearances. Then, we develop novel local contrast guided attention(LCA) and enhancement(LCE) modules to boost the vesselness responses of vascular regions of low contrast. In our experiments, we compare with four filtering-based methods and a state-of-the-art self-supervised deep learning method in multiple 3D datasets to demonstrate that our method achieves significant improvement in all datasets. Further analysis and ablation studies have also been performed to assess the contributions of various modules to the improved performance in 3D small vessel segmentation.",https://github.com/dengchihwei/LCNetVesselSeg,,Image Segmentation,Vascular,Attention models,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI,,,,
Shape-based pose estimation for automatic standard views of the knee ,"Surgical treatment of complicated knee fractures is guided by real-time imaging using a mobile C-arm. Immediate and continuous control is achieved via 2D anatomy-specific standard views that correspond to a specific C-arm pose relative to the patient positioning, which is currently determined manually, following a trial-and-error approach at the cost of time and radiation dose. The characteristics of the standard views of the knee suggests that the shape information of individual bones could guide an automatic positioning procedure, reducing time and the amount of unnecessary radiation during C-arm positioning. To fully automate the C-arm positioning task during knee surgeries, we propose a complete framework that enables (1) automatic laterality and standard view classification and (2) automatic shape-based pose regression toward the desired standard view based on a single initial X-ray. A suitable shape representation is proposed to incorporate semantic information into the pose regression pipeline. The pipeline is designed to handle two distinct standard views with one architecture. Experiments were conducted to assess the performance of the proposed system on 3528 synthetic and 1386 real X-rays for the a.-p. and lateral standard. The view/laterality classificator resulted in an accuracy of 100\%/98\% on the simulated and 99\%/98\% on the real X-rays. The pose regression performance was $d\theta_{a.-p}=5.8\pm3.3\degree,\,d\theta_{lateral}=3.7\pm2.0\degree$ on the simulated data and $d\theta_{a.-p}=7.4\pm5.0\degree,\,d\theta_{lateral}=8.4\pm5.4\degree$ on the real data outperforming intensity-based pose regression.",,,Musculoskeletal,Guided Interventions and Surgery,,,,,,,,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos ,"Breast lesion segmentation in ultrasound (US) videos is essential for diagnosing and treating axillary lymph node metastasis. However, the lack of a well-established and large-scale ultrasound video dataset with high-quality annotations has posed a persistent challenge for the research community. To overcome this issue, we meticulously curated a US video breast lesion segmentation dataset comprising 572 videos and 34,300 annotated frames, covering a wide range of realistic clinical scenarios. Furthermore, we propose a novel frequency and localization feature aggregation network (FLA-Net) that learns temporal features from the frequency domain and predicts additional lesion location positions to assist with breast lesion segmentation. We also devise a localization-based contrastive loss to reduce the lesion location distance between neighboring video frames within the same video and enlarge the location distances between frames from different ultrasound videos. Our experiments on our annotated dataset and two public video polyp segmentation datasets demonstrate that our proposed FLA-Net achieves state-of-the-art performance in breast lesion segmentation in US videos and video polyp segmentation while significantly reducing time and space complexity. Our model and dataset are available at https://github.com/jhl-Det/FLA-Net.",https://github.com/jhl-Det/FLA-Net,https://github.com/jhl-Det/FLA-Net,Video,Breast,Other,Ultrasound,,,,,,
SHISRCNet: Super-resolution And Classification Network For Low-resolution Breast Cancer Histopathology Image ,"The rapid identification and accurate diagnosis of breast cancer, known as the killer of women, have become greatly significant for those patients. Numerous breast cancer histopathological image classification methods have been proposed. But they still suffer from two problems. (1) These methods can only hand high-resolution (HR) images. However, the low-resolution (LR) images are often collected by the digital slide scanner with limited hardware conditions. Compared with HR images, LR images often lose some key features like texture, which deeply affects the accuracy of diagnosis. (2) The existing methods have fixed receptive fields, so they can not extract and fuse multi-scale features well for images with different magnification factors. To fill these gaps, we present a Single Histopathological Image Super-Resolution Classification network (SHISRCNet), which consists of two modules: Super-Resolution (SR) and Classification (CF) modules. SR module reconstructs LR images into SR ones. CF module extracts and fuses the multi-scale features of SR images for classification. In the training stage, we introduce HR images into the CF module to enhance SHISRCNet’s performance. Finally, through the joint training of these two modules, super-resolution and classified of LR images are integrated into our model. The experimental results demonstrate that the effects of our method are close to the SOTA methods with taking HR images as inputs.",https://github.com/xiely-123/SHISRCNet,https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/,Computer Aided Diagnosis,Breast,,,,,,,,
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI ,"Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in the screening and prognosis assessment of high-risk breast cancer. The segmentation of cancerous regions is essential useful for the subsequent analysis of breast MRI. To alleviate the annotation effort to train the segmentation networks, we propose a weakly-supervised strategy using extreme points as annotations for breast cancer segmentation. Without using any bells and whistles, our strategy focuses on fully exploiting the learning capability of the routine training procedure, i.e., the train - fine-tune - retrain process. The network first utilizes the pseudo-masks generated using the extreme points to train itself, by minimizing a contrastive loss, which encourages the network to learn more representative features for cancerous voxels. Then the trained network fine-tunes itself by using a similarity-aware propagation learning (SimPLe) strategy, which leverages feature similarity between unlabeled and positive voxels to propagate labels. Finally the network retrains itself by employing the pseudo-masks generated using previous fine-tuned network. The proposed method is evaluated on our collected DCE-MRI dataset containing 206 patients with biopsy-proven breast cancers. Experimental results demonstrate our method effectively fine-tunes the network by using the SimPLe strategy, and achieves a mean Dice value of 81%. Our code is publicly available at https://github.com/Abner228/SmileCode.",https://github.com/Abner228/SmileCode,,Image Segmentation,Breast,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model ,"Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.",,,Neuroimaging - Others,Other,MRI,,,,,,,
Simulation-based parameter optimization for fetal brain MRI super-resolution reconstruction ,"Tuning the regularization hyperparameter α in inverse problems has been a longstanding problem. This is particularly true in the case of fetal brain magnetic resonance imaging, where an isotropic high-resolution volume is reconstructed from motion-corrupted low-resolution series of two-dimensional thick slices. Indeed, the lack of ground truth images makes challenging the adaptation of α to a given setting of interest in a quantitative manner. In this work, we propose a simulation-based approach to tune α for a given acquisition setting. We focus on the influence of the magnetic field strength and availability of input low-resolution images on the ill-posedness of the problem. Our results show that the optimal α, chosen as the one maximizing the similarity with the simulated reference image, significantly improves the super-resolution reconstruction accuracy compared to the generally adopted default regularization values, independently of the selected reconstruction pipeline. Qualitative validation on clinical data confirms the importance of tuning this parameter to the targeted clinical image setting. The simulated data and their reconstructions are available at https://zenodo.org/record/8123677.",,https://zenodo.org/record/8123677,Fetal Imaging,Image Reconstruction,Other,MRI,,,,,,
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations ,"Clinical routine and retrospective cohorts commonly include multi-parametric Magnetic Resonance Imaging; however, they are mostly acquired in different anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints. Thus acquired views suffer from poor out-of-plane resolution and affect downstream volumetric image analysis that typically requires isotropic 3D scans. Combining different views of multi-contrast scans into high-resolution isotropic 3D scans is challenging due to the lack of a large training cohort, which calls for a subject-specific framework. This work proposes a novel solution to this problem leveraging Implicit Neural Representations (INR). Our proposed INR jointly learns two different contrasts of complementary views in a continuous spatial function and benefits from exchanging anatomical information between them. Trained within minutes on a single commodity GPU, our model provides realistic super-resolution across different pairs of contrasts in our experiments with three datasets. Using Mutual Information (MI) as a metric, we find that our model converges to an optimum MI amongst sequences, achieving anatomically faithful reconstruction.",https://github.com/jqmcginnis/multi_contrast_inr/,https://www.med.upenn.edu/cbica/brats2020/data.html,Neuroimaging - Others,Image Reconstruction,MRI,Visualization in Biomedical Imaging,,,,,,
Skin Lesion Correspondence Localization in Total Body Photography ,"Longitudinal tracking of skin lesions - finding correspondence, changes in morphology, and texture - is beneficial to the early detection of melanoma. However, it has not been well investigated in the context of full-body imaging. We propose a novel framework combining geometric and texture information to localize skin lesion correspondence from a source scan to a target scan in total body photography (TBP). Body landmarks or sparse correspondence are first created on the source and target 3D textured meshes. Every vertex on each of the meshes is then mapped to a feature vector characterizing the geodesic distances to the landmarks on that mesh. Then, for each lesion of interest (LOI) on the source, its corresponding location on the target is first coarsely estimated using the geometric information encoded in the feature vectors and then refined using the texture information. We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10 mm criterion) are comparable to the only reported longitudinal study. As full-body 3D capture becomes more prevalent and has higher quality, we expect the proposed method to constitute a valuable step in the longitudinal tracking of skin lesions.",https://github.com/weilunhuang-jhu/LesionCorrespondenceTBP3D,https://cvi2.uni.lu/datasets/,Dermatology,other,,,,,,,,
SLPD: Slide-level Prototypical Distillation for WSIs ,"Improving the feature representation ability is the foundation of many whole slide pathological image (WSIs) tasks. Recent works have achieved great success in pathological-specific self-supervised learning (SSL). However, most of them only focus on learning patch-level representations, thus there is still a gap between pretext and slide-level  downstream tasks, e.g., subtyping, grading and staging. Aiming towards slide-level representations, we propose Slide-Level Prototypical Distillation (SLPD) to explore intra- and inter-slide semantic structures  for context modeling on WSIs. Specifically, we iteratively perform intra-slide clustering for the regions (4096×4096 patches) within each WSI to yield the  prototypes and  encourage the region representations to be closer to the assigned prototypes.
By representing each slide with its prototypes, we further select similar slides by the set distance of prototypes and assign the regions by cross-slide prototypes for distillation. SLPD achieves state-of-the-art results on multiple slide-level benchmarks, and demonstrates that learning representations with semantic structures of slides can make a more closer proxy to WSI analysis. Code will be available.",https://github.com/Carboxy/SLPD,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,,,,,
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation ,"Medical image analysis using deep learning is often challenged by limited labeled data and high annotation costs. Fine-tuning the entire network in label-limited scenarios can lead to overfitting and suboptimal performance. Recently, prompt tuning has emerged as a more promising technique that introduces a few additional tunable parameters as prompts to a task-agnostic pre-trained model, and updates only these parameters using supervision from limited labeled data while keeping the pre-trained model unchanged. However, previous work has overlooked the importance of selective labeling in downstream tasks, which aims to select the most valuable downstream samples for annotation to achieve the best performance with minimum annotation cost. To address this, we propose a framework that combines selective labeling with prompt tuning (SLPT) to boost performance in limited labels. Specifically, we introduce a feature-aware prompt updater to guide prompt tuning and a TandEm Selective LAbeling (TESLA) strategy. TESLA includes unsupervised diversity selection and supervised selection using prompt-based uncertainty. In addition, we propose a diversified visual prompt tuning strategy to provide multi-prompt-based discrepant predictions for TESLA. We evaluate our method on liver tumor segmentation and achieve state-of-the-art performance, outperforming traditional fine-tuning with only 6% of tunable parameters, also achieving 94% of full-data performance by labeling only 5% of the data.",,,Active Learning,Image Segmentation,Data Efficient Learning,Transfer learning,,,,,,
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection ,"Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods on the same ICH test set.",https://github.com/YunanWu2168/SA-MIL,https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/data,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Attention models,CT,,,,,,
SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models ,"Diffusion models have recently gained popularity for accelerated MRI reconstruction due to their high sample quality. They can effectively serve as rich data priors while incorporating the forward model flexibly at inference time, and they have been shown to be more robust than unrolled methods under distribution shifts. However, diffusion models require careful tuning of inference hyperparameters on a validation set and are still sensitive to distribution shifts during testing. To address these challenges, we introduce SURE-based MRI Reconstruction with Diffusion models (SMRD), a method that performs test-time hyperparameter tuning to enhance robustness during testing. SMRD uses Stein’s
Unbiased Risk Estimator (SURE) to estimate the mean squared error of the reconstruction during testing. SURE is then used to automatically tune the inference hyperparameters and to set an early stopping criterion without the need for validation tuning. To the best of our knowledge, SMRD is the first to incorporate SURE into the sampling stage of diffusion models for automatic hyperparameter selection. SMRD outperforms diffusion model baselines on various measurement noise levels, acceleration factors, and anatomies, achieving a PSNR improvement of up to 6 dB under measurement noise. The code will be made publicly available.",,mridata.org,Other,Model Generalizability / Federated Learning,MRI,,,,,,,
Soft-tissue Driven Craniomaxillofacial Surgical Planning ,"In CMF surgery, the planning of bone movement to achieve a desired facial outcome is a challenging task. Current bone-driven approaches focus on normalizing the bone with the expectation that the facial appearance will be corrected accordingly. However, due to the complex non-linear relationship between bone and the face, such bone-driven methods are insufficient to fully correct facial deformities. Despite efforts to simulate facial changes resulting from bony movement, surgical planning still relies on iterative revisions and educated guesses. To address these issues, we propose a soft-tissue-driven framework that can automatically create and verify surgical plans. Our framework consists of a bony planner network that estimates the bony movements required to achieve the desired facial outcome and a facial simulator that can simulate the possible facial changes resulting from the estimated bony plans. By combining these two models, we can verify and determine the final bony movement required for planning. The proposed framework was evaluated using a clinical dataset, and our experimental results demonstrate that the soft-tissue driven approach greatly improves the accuracy and efficacy of surgical planning when compared to the conventional bone-driven approach.",,,Surgical Planning and Simulation,Surgical Skill and Work Flow Analysis,,,,,,,,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence ,"The Computed Tomography (CT) for diagnosis of lesions in human internal organs is one of the most fundamental topics in medical imaging. Low-dose CT, which offers reduced radiation exposure, is preferred over standard-dose CT, and therefore its reconstruction approaches have been extensively studied. However, current low-dose CT reconstruction techniques mainly rely on model-based methods or deep-learning-based techniques, which often ignore the coherence and smoothness for sequential CT slices. To address this issue, we propose a novel approach using generative adversarial networks (GANs)  with enhanced local coherence. The proposed method can capture the local coherence of adjacent images by optical flow, which yields significant improvements in the precision and stability of the constructed images. We evaluate our proposed method on real datasets and  the experimental results suggest  that it can outperform existing state-of-the-art reconstruction approaches significantly.",https://github.com/lwjie595/GANLC,https://drive.google.com/drive/folders/1gKytBtkTtGxBLRcNInx2OLty4Gie3pCX,Image Reconstruction,Other,,,,,,,,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning ,"Unsupervised domain adaptation (UDA) has increasingly gained interest for its capacity to transfer the knowledge learned from a labeled source domain to an unlabeled target domain. However, typical UDA methods require concurrent access to both the source and target domain data, which largely limits its application in medical scenarios where source data is often unavailable due to privacy concern. To tackle the source data-absent problem, we present a novel two-stage source-free domain adaptation (SFDA) framework for medical image segmentation, where only a well-trained source segmentation model and unlabeled target data are available during domain adaptation. Specifically, in the prototype-anchored feature alignment stage, we first utilize the weights of the pre-trained pixel-wise classifier as source prototypes, which preserve the information of source features. Then, we introduce the bi-directional transport to align the target features with class prototypes by minimizing its expected cost. On top of that, a contrastive learning stage is further devised to utilize those pixels with unreliable predictions for a more compact target feature distribution.  Extensive experiments on a cross-modality medical segmentation task demonstrate the superiority of our method in large domain discrepancy settings compared with the state-of-the-art SFDA approaches and even some UDA methods.",https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA,https://chaos.grand-challenge.org/,Abdomen,Image Segmentation,Data Efficient Learning,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Transfer learning,CT,MRI,,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher ,"This paper studies source-free domain adaptive fundus image segmentation which aims to adapt a pretrained fundus segmentation model to a target domain using unlabeled images. This is a challenging task because it is highly risky to adapt a model only using unlabeled data. Most existing methods tackle this task mainly by designing techniques to carefully generate pseudo labels from the model’s predictions and use the pseudo labels to train the model. While often obtaining positive adaption effects, these methods suffer from two major issues. First, they tend to be fairly unstable - incorrect pseudo labels abruptly emerged may cause a catastrophic impact on the model. Second, they fail to consider the severe class imbalance of fundus images where the foreground (e.g., cup) region is usually very small. This paper aims to address these two issues by proposing the Class-Balanced Mean Teacher (CBMT) model. CBMT addresses the unstable issue by proposing a weak-strong augmented mean teacher learning scheme where only the teacher model generates pseudo labels from weakly augmented images to train a student model that takes strongly augmented images as input. The teacher is updated as the moving average of the instantly trained student, which could be noisy. This prevents the teacher model from being abruptly impacted by incorrect pseudo-labels. For the class imbalance issue, CBMT proposes a novel loss calibration approach to highlight foreground classes according to global statistics. Experiments show that CBMT well addresses these two issues and outperforms existing methods on multiple benchmarks.",https://github.com/lloongx/SFDA-CBMT,https://refuge.grand-challenge.org/,Transfer learning,Computer Aided Diagnosis,Image Segmentation,,,,,,,
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold ,"Advancements in neuroimaging technology have made it possible to measure the connectivity evolution between different brain regions over time. Emerging evi-dence shows that some critical brain regions, known as hub nodes, play a signifi-cant role in updating brain network connectivity over time. However, current spa-tiotemporal hub identification is built on static network-based approaches, where hub regions are identified independently for each temporal brain network without considering their temporal consistency, and fails to align the evolution of hubs with changes in connectivity dynamics. To address this problem, we propose a novel spatiotemporal hub identification method that utilizes dynamic graph embedding to distinguish temporal hubs from peripheral nodes. Specially, to preserve the time consistency information, we put the dynamic graph embedding learning upon a smooth physics model of network-to-network evolution, which mathematically expresses as a total variation of dynamic graph embedding with respect to time.  A novel Grassmannian manifold optimization scheme is further introduced to learn the embeddings accurately and capture the time-varying topology of brain network. Experimental results on real data demonstrate the highest temporal consistency in hub identification, surpassing conventional approaches.",,,Interpretability / Explainability,Neuroimaging - Functional Brain Networks,Computer Aided Diagnosis,Other,,,,,,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change ,"Accurate surgical planning for orthognathic surgical procedures requires biomechanical simulation of facial soft tissue changes. Simulations must be performed quickly and accurately to be useful in a clinical pipeline, and surgeons may try several iterations before arriving at an optimal surgical plan. The finite element method (FEM) is commonly used to perform biomechanical simulations. Previous studies divided FEM simulations into incremental steps to improve convergence and model accuracy. While incremental simulations are more realistic, they greatly increase FEM simulation time, preventing integration into clinical use. In an attempt to make simulations faster, deep learning (DL) models have been developed to replace FEM for biomechanical simulations. However, previous DL models are not designed to utilize temporal information in incremental simulations. In this study, we propose Spatiotemporal Incremental Mechanics Modeling (SIMM), a deep learning method that performs spatiotemporally-aware incremental simulations for mechanical modeling of soft tissues. Our method uses both spatial and temporal information by combining a spatial feature extractor with a temporal aggregation mechanism. We trained our network using incremental FEM simulations of 18 subjects from our repository. We compared SIMM to spatial-only incremental and single-step simulation approaches. Our results suggest that adding spatiotemporal information may improve the accuracy of incremental simulations compared to methods that use only spatial information.",,,Surgical Planning and Simulation,Musculoskeletal,Interventional Simulation Systems,Interpretability / Explainability,Other,,,,,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation ,"Domain shift is a common problem in clinical applications, where the training images (source domain) and the test images (target domain) are under different distributions. Unsupervised Domain Adaptation (UDA) techniques have been proposed to adapt models trained in the source domain to the target domain. However, those methods require a large number of images from the target domain for model training. In this paper, we propose a novel method for Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a limited number of unlabeled target domain samples are available for training. To accomplish this challenging task, first, a spectral sensitivity map is introduced to characterize the generalization weaknesses of models in the frequency domain. We then developed a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images to effectively suppresses the model sensitivity, which leads to improved model generalizability in the target domain. We demonstrated the proposed method and rigorously evaluated its performance on multiple tasks using several public datasets.",https://github.com/RPIDIAL/SAMix,https://wilds.stanford.edu/datasets/#camelyon17,Transfer learning,Image Segmentation,Model Generalizability / Federated Learning,Microscopy,,,,,,
Speech Audio Synthesis from Tagged MRI and Non-Negative Matrix Factorization via Plastic Transformer ,"The tongue’s intricate 3D structure, comprising localized functional units, plays a crucial role in the production of speech. When measured using tagged MRI, these functional units exhibit cohesive displacements and derived quantities that facilitate the complex process of speech production. Non-negative matrix factorization-based approaches have been shown to estimate the functional units through motion features, yielding a set of building blocks and a corresponding weighting map. Investigating the link between weighting maps and speech acoustics can offer significant insights into the intricate process of speech production. To this end, in this work, we utilize two-dimensional spectrograms as a proxy representation, and develop an end-to-end deep learning framework for translating weighting maps to their corresponding audio waveforms. Our proposed plastic light transformer (PLT) framework is based on directional product relative position bias and single-level spatial pyramid pooling, thus enabling flexible processing of weighting maps with variable size to fixed-size spectrograms, without input information loss or dimension expansion. Additionally, our PLT framework efficiently models the global correlation of wide matrix input. To improve the realism of our generated spectrograms with relatively limited training samples, we apply pair-wise utterance consistency with Maximum Mean Discrepancy constraint and adversarial training. Experimental results on a dataset of 29 subjects speaking two utterances demonstrated that our framework is able to synthesize speech audio waveforms from weighting maps, outperforming conventional convolution and transformer models.",,,Musculoskeletal,Other,MRI,Video,,,,,,
Spinal nerve segmentation method and dataset construction in endoscopic surgical scenarios ,"Endoscopic surgery is currently an important treatment method in the field of spinal surgery and avoiding damage to the spinal nerves through video guidance is a key challenge. This paper presents the first real-time segmentation method for spinal nerves in endoscopic surgery, which provides crucial navigational information for surgeons. A finely annotated segmentation dataset of approximately 10,000 consec-utive frames recorded during surgery is constructed for the first time for this field, addressing the problem of semantic segmentation. Based on this dataset, we propose FUnet (Frame-Unet), which achieves state-of-the-art performance by utilizing inter-frame information and self-attention mechanisms. We also conduct extended exper-iments on a similar polyp endoscopy video dataset and show that the model has good generalization ability with advantageous performance. The dataset and code of this work are presented at: https://github.com/zzzzzzpc/FUnet .",https://github.com/zzzzzzpc/FUnet,https://github.com/zzzzzzpc/FUnet,Guided Interventions and Surgery,Musculoskeletal,Image Segmentation,Attention models,Video,Surgical Scene Understanding,,,,
SPR-Net: Structural Points based Registration for Coronary Arteries across Systolic and Diastolic Phases ,"Systolic and diastolic registration of coronary arteries is a critical yet challenging step in coronary artery disease analysis. Most existing methods ignore the important relationship between vascular geometric shape and image contextual information in the two phases, leading to limited performance. In this paper, we propose a novel intrinsic structural point learning-based point cloud registration method, which comprehensively captures both point-level geometric features and image-level semantic features as enriched feature representations to assist coronary registration. Specifically, given the systolic and diastolic CCTA images, our method improves coronary artery registration from three aspects. First, point cloud encoder learns the spatial geometric features of the points in the 3D coronary mask to effectively capture the vascular shape representation. Second, a vision transformer (ViT) is used to extract the image semantic information as a complementary condition of the geometric features to identify the bi-phasic correspondence of different vascular branches. Third, we design a transformer module to fuse the features of points and images to obtain the corresponding structural points in the two phases, and then use structural points to guide the coronary artery registration via thin-plate spline (TPS) method. We evaluate our method on a real-world clinical dataset. Extensive experiments show that our method significantly outperforms the state-of-the-art methods in coronary artery registration.",,,Vascular,Image Registration,,,,,,,,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-Ensemble ,"The commonly presented histology stain variation may moderately obstruct the diagnosis of human experts, but can considerably downgrade the reliability of deep learning models in various diagnostic tasks. Many stain style transfer methods have been proposed to eliminate the variance of stain styles across different medical institutions or even different batches. However, existing solutions are confined to Generative Adversarial Networks (GANs), AutoEncoders (AEs), or their variants, and often fell into the shortcomings of mode collapses or posterior mismatching issues. In this paper, we make the first attempt at a Diffusion Probabilistic Model to cope with the indispensable stain style transfer in histology image context, called \texttt{StainDiff}. Specifically, our diffusion framework enables learning from unpaired images by proposing a novel cycle-consistent constraint, whereas existing diffusion models are restricted to image generation or fully supervised pixel-to-pixel translation. Moreover, given the stochastic nature of \texttt{StainDiff} that multiple transferred results can be generated from one input histology image, we further boost and stabilize the performance by the proposal of a novel self-ensemble scheme. Our model can avoid the challenging issues in mainstream networks, such as the mode collapses in GANs or alignment between posterior distributions in AEs. In conclusion, \texttt{StainDiff} suffices to increase the stain style transfer quality, where the training is straightforward and the model is simplified for real-world clinical deployment.",,,Histopathology,,,,,,,,,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients using Spatiotemporal Analysis and Transformer-Based Radiomics Models ,"Chronic Kidney Disease (CKD) patients are at higher risk of Major Adverse Cardiovascular Events (MACE). Echocardiography evaluates left ventricle (LV) function and heart abnormalities. LV Wall (LVW) pathophysiology and systolic/diastolic dysfunction are linked to MACE outcomes (O and O+) in CKD patients. However, traditional LV volume-based measurements like ejection-fraction offer limited predictive value as they rely only on end-phase frames. We hypothesize that analyzing LVW morphology over time, through spatiotemporal analysis, can predict MACE risk in CKD patients. However, accurately delineating and analyzing LVW at every frame is challenging due to noise, poor resolution, and the need for manual intervention. Our contribution includes (a) developing an automated pipeline for identifying and standardizing heart-beat cycles and segmenting the LVW, (b) introducing a novel computational biomarker—STAR-Echo—which combines spatiotemporal risk from radiomic (MR) and deep learning (MT ) models to predict MACE prognosis in CKD patients, and (c) demonstrating the superior prognostic performance of STAR-Echo compared to MR, MT, as well as clinical-biomarkers (EF, BNP, and NT-proBNP) for characterizing cardiac dysfunction. STAR-Echo captured the gray level intensity distribution, perimeter and sphericity of the LVW that changes differently over time in individuals who encounter MACE outcomes. STAR-Echo achieved an AUC of 0.71[0.53–0.89] for MACE outcome classification and also demonstrated prognostic ability in Kaplan-Meier survival analysis on a holdout cohort (Sv = 44) of CKD patients (N = 150). It achieved superior MACE prognostication (p-value = 0.037 (log-rank test)), compared to MR (p-value = 0.042), MT (p-value = 0.069), clinical biomarkers—EF, BNP, and NT-proBNP (p-value >0.05).",https://github.com/rohand24/STAR_Echo,,Ultrasound,Cardiac,Computer Aided Diagnosis,Interpretability / Explainability,Video,,,,,
Structured State Space Models for Multiple Instance Learning in Digital Pathology ,"Multiple instance learning is an ideal mode of analysis for histopathology data, where vast whole slide images are typically annotated with a single global label. In such cases, a whole slide image is modeled as a collection of tissue patches to be aggregated and classified. Common models for performing this classification include recurrent neural networks and transformers. Although powerful compression algorithms, such as deep pre-trained neural networks, are used to reduce the dimensionality of each patch, the sequences arising from whole slide images remain excessively long, routinely containing tens of thousands of patches. Structured state space models are an emerging alternative for sequence modelling, specifically designed for the efficient modelling of long sequences. These models invoke an optimal projection of an input sequence into memory units that compress the entire sequence. In this paper, we propose the use of state space models as a multiple instance learner to a variety of problems in digital pathology. Across experiments in metastasis detection, cancer subtyping, mutation classification, and multitask learning, we demonstrate the competitiveness of this new class of models with existing state of the art approaches. Our code is available at [this URL].",https://github.com/MICS-Lab/s4_digital_pathology,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Histopathology,,,,,,,,
Structure-decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation ,"Existing methods for unsupervised domain adaptive mitochondrial segmentation perform feature alignment via adversarial learning, and achieve promising performance. However, these methods neglect to the differences in structure of long-range sections. Besides, they fail to utilize the context information to merge the appropriate pixels to construct part-level discriminator. To mitigate these limitations, we propose a Structure-decoupled Adaptive Part Alignment Network (SAPAN) including a structure decoupler and a part miner for robust mitochondrial segmentation. The proposed SAPAN model enjoys several merits. First, the structure decoupler is responsible for modeling long-range section variation in structure,  and decouple it from features in pursuit of domain invariance. Second, part miner aims at absorbing the suitable pixels to aggregate diverse parts in an adaptive manner to construct part-level discriminator. Extensive experimental results on four challenging benchmarks demonstrate that our method performs favorably against state-of-the-art UDA methods.",,,Image Segmentation,,,,,,,,,
StructuRegNet: Structure-guided Multimodal 2D-3D Registration ,"Multimodal 2D-3D co-registration is a challenging problem with numerous clinical applications, including multimodal diagnosis, radiation therapy, or interventional radiology. In this paper, we present StructuRegNet, a deep-learning framework that addresses this problem with three novel contributions. First, we combine a 2D-3D deformable registration network with an adversarial modality translation module, allowing each block to benefit from the signal of the other. Second, we solve the initialization challenge for 2D-3D registration by leveraging tissue structure through cascaded rigid areas guidance and distance field regularization. Third, StructuRegNet handles out-of-plane deformation without requiring any 3D reconstruction thanks to a recursive plane selection. We evaluate the quantitative performance of StructuRegNet for head and neck cancer between 3D CT scans and 2D histopathological slides, enabling pixel-wise mapping of low-quality radiologic imaging to gold-standard tumor extent and bringing biological insights toward homogenized clinical guidelines. Additionally, our method can be used in radiation therapy by mapping 3D planning CT into the 2D MR frame of the treatment day for accurate positioning and dose delivery. Our framework demonstrates superior results to traditional methods for both applications. It is versatile to different locations or magnitudes of deformation and can serve as a backbone for any relevant clinical context.",,,Image Registration,Oncology,Image Reconstruction,Guided Interventions and Surgery,CT,Histopathology,MRI,,,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform ,"Objects with complex structures pose significant challenges to existing instance segmentation methods that rely on boundary or affinity maps, which are vulnerable to small errors around contacting pixels that cause noticeable connectivity change. While the distance transform (DT) makes instance interiors and boundaries more distinguishable, it tends to overlook the intra-object connectivity for instances with varying width and result in over-segmentation. To address these challenges, we propose a skeleton-aware distance transform (SDT) that combines the merits of object skeleton in preserving connectivity and DT in model- ing geometric arrangement to represent instances with arbitrary structures. Comprehensive experiments on histopathology image segmentation demonstrate the state-of-the-art performance achieved by SDT.",,,Image Segmentation,Histopathology,,,,,,,,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation ,"Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and time-consuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures without the need for expert annotations. The code for this paper can be found at https://github.com/HieuPhan33/MaskGAN.",https://github.com/HieuPhan33/MaskGAN,,Image Reconstruction,Neuroimaging - Brain Development,Data Efficient Learning,CT,MRI,,,,,
Style-based Manifold for Weakly-supervised Disease Characteristic Discovery ,"In Alzheimer’s Disease (AD), interpreting relevant tissue changes is key in discovering disease characteristics and mechanisms. However, AD-induced brain atrophy can be difficult to observe without Cognitively Normal (CN) reference images, and collecting co-registered AD and CN images at scale is not practical. We propose Disease Discovery GAN (DiDiGAN), a style-based network that can create representative reference images for disease characteristic discovery. DiDiGAN learns a manifold of disease-specific style codes. In the generator, these style codes are used to “stylise” an anatomical constraint into synthetic reference images (for various disease states). The constraint in this case underpins the high-level anatomical structure upon which disease features are synthesised. Additionally, DiDiGAN’s manifold is smooth such that seamless disease state transitions are possible via style interpolation. Finally, to ensure the generated reference images are anatomically correlated across disease states, we incorporate anti-aliasing inspired by StyleGAN3 to enforce anatomical correspondence. We test DiDiGAN on the ADNI dataset involving CN and AD magnetic resonance images (MRIs), and the generated reference AD and CN images reveal key AD characteristics (hippocampus shrinkage, ventricular enlargement, cortex atrophies). Moreover, by interpolating DiDiGAN’s manifold, smooth CN-AD transitions were acquired further enhancing disease visualisation. In contrast, other methods in the literature lack such dedicated disease manifolds and fail to synthesise usable reference images for disease characteristic discovery.",https://github.com/SiyuLiu0329/DiDiGAN-final,https://adni.loni.usc.edu,Visualization in Biomedical Imaging,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI ,"The baby brain undergoes rapid changes in volume, shape, and structural organization during the first postnatal year. Accurate cortical surface reconstruction is prerequisite to characterizing dynamic variations in cortical morphometry during early brain development. Existing surface reconstruction methods are typically tailored for adult brain MRI and are inaccurate in reconstructing cortical surfaces from infant MRI, owing to the poor tissue contrast, partial volume effect, and rapid changes in cortical folding patterns due to the emergence of secondary and tertiary cortical folds in the first year of life. Here, we propose an infant-centric cortical surface reconstruction method that accommodates dynamic variations in early developing brain. Our method, SurfFlow, sequentially deforms an initial template mesh to target cortical surfaces with 3 seamlessly connected deformation blocks. It can rapidly reconstruct a high-resolution cortical surface mesh with 460k vertices in about one second. Performance evaluation based on a dataset of infants from 0 to 12 months indicates that SurfFlow reduces geometric errors and improves mesh regularity substantially over state-of-the-art deep learning approaches.",,,Other,Neuroimaging - Brain Development,Neuroimaging - Others,MRI,,,,,,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions ,"Surgical action triplets describe instrument-tissue interactions as 〈instrument, verb, target〉 combinations, thereby supporting a detailed analysis of surgical scene activities and workflow. This work focuses on surgical action triplet detection, which is challenging but more precise than the traditional triplet recognition task as it consists of joint (1) localization of surgical instruments and (2) recognition of the surgical action triplet associated with every localized instrument. Triplet detection is highly complex due to the lack of spatial triplet annotation. We analyze how the amount of instrument spatial annotations affects triplet detection and observe that accurate instrument localization does not guarantee a better triplet detection due to the risk of erroneous associations with the verbs and targets. To solve the two tasks, we propose MCIT-IG, a two-stage network, that stands for Multi-Class Instrument-aware Transformer - Interaction Graph. The MCIT stage of our network models per class embedding of the targets as additional features to reduce the risk of misassociating triplets. Furthermore, the IG stage constructs a bipartite dynamic graph to model the interaction between the instruments and targets, cast as the verbs. We utilize a mixed-supervised learning strategy that combines weak target presence labels for MCIT and pseudo triplet labels for IG to train our network. We observed that complementing minimal instrument spatial annotations with target embeddings results in better triplet detection. We evaluate our model on the CholecT50 dataset and show improved performance on both instrument localization and triplet detection, topping the leaderboard of the CholecTriplet challenge in MICCAI 2022.",https://github.com/CAMMA-public/mcit-ig,,Surgical Skill and Work Flow Analysis,Surgical Data Science,Surgical Scene Understanding,,,,,,,
Surgical Activity Triplet Recognition via Triplet Disentanglement ,"Including context-aware decision support in the operating room has the potential to improve surgical safety and efficiency by utilizing real-time feedback obtained from surgical workflow analysis. In this task, recognizing each surgical activity in the endoscopic video as a triplet {instrument, verb, target} is crucial, as it helps to ensure actions occur only after an instrument is present. However, recognizing the states of these three components in one shot poses extra learning ambiguities, as the triplet supervision is highly imbalanced (positive when all components are correct). To remedy this issue, we introduce a triplet disentanglement framework for surgical action triplet recognition, which decomposes the learning objectives to reduce learning difficulties. Particularly, our network decomposes the recognition of triplet into five complementary and simplified sub-networks. While the first sub-network converts the detection into a numerical supplementary task predicting the existence/number of three components only, the second focuses on the association between them, and the other three predict the components individually. In this way, triplet recognition is decoupled in a progressive, easy-to-difficult manner. In addition, we propose a hierarchical training schedule as a way to decompose the difficulty of the task further. Our model first creates several bridges and then progressively identifies the final key task step by step, rather than explicitly identifying surgical activity. Our proposed method has been demonstrated to surpass current state-of-the-art approaches on the CholecT45 endoscopic video dataset.",,,Surgical Scene Understanding,Surgical Skill and Work Flow Analysis,,,,,,,,
Surgical Video Captioning with Mutual-Modal Concept Alignment ,"Automatic surgical video captioning is critical to understanding surgical procedures, and can provide the intra-operative guidance and the post-operative report generation. As the overlap of surgical workflow and vision-language learning, this cross-modal task expects precise text descriptions of complex surgical videos. However, current captioning algorithms neither fully leverage the inherent patterns of surgery, nor coordinate the knowledge of visual and text modalities well. To address these problems, we introduce the surgical concepts into captioning, and propose the Surgical Concept Alignment Network (SCA-Net) to bridge the visual and text modalities via surgical concepts. Specifically, to enable the captioning network to accurately perceive surgical concepts, we first devise the Surgical Concept Learning (SCL) to predict the presence of surgical concepts with the representations of visual and text modalities, respectively. Moreover, to mitigate the semantic gap between visual and text modalities of captioning, we propose the Mutual-Modality Concept Alignment (MC-Align) to mutually coordinate the encoded features with surgical concept representations of the other modality. In this way, the proposed SCA-Net achieves the surgical concept alignment between visual and text modalities, thereby producing more accurate captions with aligned multi-modal knowledge. Extensive experiments on neurosurgery videos and nephrectomy images confirm the effectiveness of our SCA-Net, which outperforms the state-of-the-arts by a large margin. The source code is available at https://github.com/franciszchen/SCA-Net.",https://github.com/franciszchen/SCA-Net,,Surgical Skill and Work Flow Analysis,Text (clinical/radiology reports),Video,,,,,,,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery ,"Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long paragraphs, we carefully sequence the word tokens before vision tokens, mimicking the human thought process of understanding the question to infer an answer from an image. Quantitatively, we prove that the LV-GPT model outperforms other state-of-the-art VQA models on two publically available surgical-VQA datasets (based on endoscopic vision challenge robotic scene segmentation 2018 and CholecTriplet2021)  and on our newly annotated dataset (based on the holistic surgical scene dataset). We further annotate all three datasets to include question-type annotations to allow sub-type analysis. Furthermore, we extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model.",https://github.com/lalithjets/SurgicalGPT,https://github.com/lalithjets/SurgicalGPT,Surgical Data Science,Surgical Scene Understanding,,,,,,,,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation ,"Recent advancements in large-scale Vision Transformers have made significant strides in improving pre-trained models for medical image segmentation. However, these methods face a notable challenge in acquiring a substantial amount of pre-training data, particularly within the medical field. To address this limitation, we present Masked Multi-view with Swin Transformers (SwinMM), a novel multi-view pipeline for enabling accurate and data-efficient self-supervised medical image analysis. Our strategy harnesses the potential of multi-view information by incorporating two principal components. In the pre-training phase, we deploy a masked multi-view encoder devised to concurrently train masked multi-view observations through a range of diverse proxy tasks. These tasks span image reconstruction, rotation, contrastive learning, and a novel task that employs a mutual learning paradigm. This new task capitalizes on the consistency between predictions from various perspectives, enables the extraction of hidden multi-view information from 3D medical data. In the fine-tuning stage, a cross-view decoder is developed to aggregate the multi-view information through a cross-attention block. Compared with the previous state-of-the-art self-supervised learning method Swin UNETR, SwinMM demonstrates a notable advantage on several medical image segmentation tasks. It allows for a smooth integration of multi-view information, significantly boosting both the accuracy and data-efficiency of the model. Code and models are available at https://github.com/UCSC-VLAA/SwinMM/.",https://github.com/UCSC-VLAA/SwinMM/,,Image Segmentation,Abdomen,Cardiac,Lung,Image Reconstruction,Attention models,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,MRI
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation ,"Transformers for medical image segmentation have attracted broad interest. Unlike convolutional networks~(CNNs), transformers use self-attentions that do not have a strong inductive bias. This gives transformers the ability to learn long-range dependencies and stronger modeling capacities. Although they, e.g. SwinUNETR, achieve state-of-the-art~(SOTA) results on some benchmarks, the lack of inductive bias makes transformers harder to train, requires much more training data, and are sensitive to training recipes. In many clinical scenarios and challenges, transformers can still have inferior performances than SOTA CNNs like nnUNet. A transformer backbone and corresponding training recipe, which can achieve top performances under different medical image segmentation scenarios, still needs to be developed.  In this paper, we enhance the SwinUNETR with convolutions, which results in a surprisingly stronger backbone, the SwinUNETR-V2, for 3D medical image segmentation. It achieves top performance on a variety of benchmarks of different sizes and modalities, including the Whole abdominal ORgan Dataset (WORD), MICCAI FLARE2021 dataset, MSD pancreas dataset, MSD prostate dataset, and MSD lung cancer dataset, all using the same training recipe with minimum changes across tasks.",https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/nets/swin_unetr.py,,Image Segmentation,Abdomen,CT,,,,,,,
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings ,"Modern medical image segmentation methods primarily use discrete representations in the form of rasterized masks to learn features and generate predictions. Although effective, this paradigm is spatially inflexible, scales poorly to higher-resolution images, and lacks direct understanding of object shapes. To address these limitations, some recent works utilized implicit neural representations (INRs) to learn continuous representations for segmentation. However, these methods often directly adopted components designed for 3D shape reconstruction. More importantly, these formulations were also constrained to either point-based or global contexts, lacking contextual understanding or local fine-grained details, respectively—both critical for accurate segmentation. To remedy this, we propose a novel approach, SwIPE (Segmentation with Implicit Patch Embeddings), that leverages the advantages of INRs and predicts shapes at the patch level—rather than at the point level or image level—to enable both accurate local boundary delineation and global shape coherence. Extensive evaluations on two tasks (2D polyp segmentation and 3D abdominal organ segmentation) show that SwIPE significantly improves over recent implicit approaches and outperforms state-of-the-art discrete methods with over 10x fewer parameters. Our method also demonstrates superior data efficiency and improved robustness to data shifts across image resolutions and datasets. Code is available on Github.",https://github.com/charzharr/miccai23-swipe-implicit-segmentation/blob/master/README.md,https://www.synapse.org/#!Synapse:syn3193805/wiki/89480,Image Segmentation,Abdomen,Oncology,Computer Aided Diagnosis,Data Efficient Learning,Transfer learning,CT,other,,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-based Hierarchical Fusion Network with Attention Mechanism ,"Magnetic resonance imaging (MRI) is the most sensitive technique for breast cancer detection among current clinical imaging modalities. Contrast-enhanced MRI (CE-MRI)  provides superior differentiation between tumors and invaded healthy tissue, and has become an indispensable technique in the detection and evaluation of cancer. However, the use of gadolinium-based contrast agents (GBCA) to obtain CE-MRI may be associated with nephrogenic systemic fibrosis and may lead to bioaccumulation in the brain, posing a potential risk to human health. Moreover, and likely more important, the use of gadolinium-based contrast agents requires the cannulation of a vein, and the injection of the contrast media which is cumbersome and places a burden on the patient. To reduce the use of contrast agents, diffusion-weighted imaging (DWI) is emerging as a key imaging technique, although currently usually complementing breast CE-MRI. In this study, we develop a multi-sequence fusion network to synthesize CE-MRI based on T1-weighted MRI and DWIs. DWIs with different b-values are fused to efficiently utilize the difference features of DWIs. Rather than proposing a pure data-driven approach, we invent a multi-sequence attention module to obtain refined feature maps, and leverage hierarchical representation information fused at different scales while utilizing the contributions from different sequences from a model-driven approach by introducing the weighted difference module. The results show that the multi-b-value DWI-based fusion model can potentially be used to synthesize CE-MRI, thus theoretically reducing or avoiding the use of GBCA, thereby minimizing the burden to patients. Our code is available at
https://github.com/Netherlands-Cancer-Institute/CE-MRI.",https://github.com/Netherlands-Cancer-Institute/CE-MRI,,Breast,Other,MRI,,,,,,,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models ,"Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier’s performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.",https://github.com/MECLabTUDA/CataSynth,https://cataracts.grand-challenge.org/,Surgical Data Science,Other,Surgical Skill and Work Flow Analysis,,,,,,,
Synthetic Augmentation with Large-scale Unconditional Pre-training ,"Deep learning based medical image recognition systems often require a substantial amount of training data with expert annotations, which can be expensive and time-consuming to obtain. Recently, synthetic augmentation techniques have been proposed to mitigate the issue by generating realistic images conditioned on class labels. However, the effectiveness of these methods heavily depends on the representation capability of the trained generative model, which cannot be guaranteed without sufficient labeled training data. To further reduce the dependency on annotated data, we propose a synthetic augmentation method called HistoDiffusion, which can be pre-trained on large-scale unlabeled datasets and later applied to a small-scale labeled dataset for augmented training. In particular, we train a latent diffusion model (LDM) on diverse unlabeled datasets to learn common features and generate realistic images without conditional inputs. Then, we fine-tune the model with classifier guidance in latent space on an unseen labeled dataset so that the model can synthesize images of specific categories. Additionally, we adopt a selective mechanism to only add synthetic samples with high confidence of matching to target labels. We evaluate our proposed method by pre-training on three histopathology datasets and testing on a histopathology dataset of colorectal cancer (CRC) excluded from the pre-training datasets. With HistoDiffusion augmentation, the classification accuracy of a backbone classifier is remarkably improved by 6.4% using a small set of the original labels. Our code is available at https://github.com/karenyyy/HistoDiffAug.",https://github.com/karenyyy/HistoDiffAug,https://github.com/AdalbertoCq/Pathology-GAN,Data Efficient Learning,Histopathology,,,,,,,,
TabAttention: Learning Attention Conditionally on Tabular Data ,"Medical data analysis often combines both imaging and tabular data processing using machine learning algorithms. While previous studies have investigated the impact of attention mechanisms on deep learning models, few have explored integrating attention modules and tabular data. In this paper, we introduce TabAttention, a novel module that enhances the performance of Convolutional Neural Networks (CNNs) with an attention mechanism that is trained conditionally on tabular data. Specifically, we extend the Convolutional Block Attention Module to 3D by adding a Temporal Attention Module that uses multi-head self-attention to learn attention maps. Furthermore, we enhance all attention modules by integrating tabular data embeddings. Our approach is demonstrated on the fetal birth weight (FBW) estimation task, using 92 fetal abdominal ultrasound video scans and fetal biometry measurements. Our results indicate that TabAttention outperforms clinicians and existing methods that rely on tabular and/or imaging data for FBW prediction. This novel approach has the potential to improve computer-aided diagnosis in various clinical workflows where imaging and tabular data are combined. We provide a source code for integrating TabAttention in CNNs at https://github.com/SanoScience/Tab-Attention.",https://github.com/SanoScience/Tab-Attention,,Computer Aided Diagnosis,Fetal Imaging,Ultrasound,Video,,,,,,
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation ,"Alzheimer’s disease (AD) is characterized by the propagation of tau aggregates throughout the brain in a prion-like manner. Tremendous efforts have made to analyze the spatiotemporal propagation patterns of widespread tau aggregates. However, current works focus on the change of focal patterns in lieu of a system-level understanding of the tau propagation mechanism that can explain and forecast the cascade of tau accumulation. To fill this gap, we conceptualize that the intercellular spreading of tau pathology forms a dynamic system where brain region is ubiquitously wired with other nodes while interacting with the build-up of pathological burdens. In this context, we formulate the biological process of tau spreading in a principled potential energy transport model (constrained by brain network topology), which allows us to develop an explainable neural network for uncovering the spatiotemporal dynamics of tau propagation from the longitudinal tau-PET images. We first translate the transport equation into a backbone of graph neural network (GNN), where the spreading flows are essentially driven by the potential energy of tau accumulation at each node. Further, we introduce the total variation (TV) into the graph transport model to prevent the flow vanishing caused by the l_2-norm regularization, where the nature of system’s Euler-Lagrange equations is to maximize the spreading flow while minimizing the overall potential energy. On top of this min-max optimization scenario, we design a generative adversarial network (GAN) to depict the TV-based spreading flow of tau aggregates, coined TauFlowNet. We evaluate TauFlowNet on ADNI dataset in terms of the prediction accuracy of future tau accumulation and explore the propagation mechanism of tau aggregates as the disease progresses. Compared to current methods, our physics-informed method yields more accurate and interpretable results, demonstrating great potential in discovering novel neurobiological mechanisms.",,,Other,Neuroimaging - DWI and Tractography,Interpretability / Explainability,MRI,PET/SPECT,,,,,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction ,"When deep neural network has been proposed to assist the dentist in designing the location of dental implant, most of them are targeting simple cases where only one missing tooth is available. As a result, literature works do not work well when there are multiple missing teeth and easily generate false predictions when the teeth are sparsely distributed. In this paper, we are trying to integrate a weak supervision text, the target region, to the implant position regression network, to address above issues. We propose a text condition embedded implant position regression network (TCEIP), to embed the text condition into the encoder-decoder framework for improvement of the regression performance. A cross-modal interaction that consists of cross-modal attention (CMA) and knowledge alignment module (KAM) is proposed to facilitate the interaction between features of images and texts. The CMA module performs a cross-attention between the image feature and the text condition, and the KAM mitigates the knowledge gap between the image feature and the image encoder of the CLIP. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than existing methods.",,,Computer Aided Diagnosis,CT,,,,,,,,
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope ,"The depth and pose estimations from monocular images are essential for computer-aided navigation. Since the ground truth of depth and pose are difficult to obtain, the unsupervised training method has a broad prospect in endoscopic scenes. However, endoscopic datasets lack sufficient diversity of visual variations, and appearance inconsistency is also frequently observed in image triplets. In this paper, we propose a triplet-consistency-learning framework (TCL) consisting of two modules: Geometric Consistency module(GC) and Appearance Inconsistency module(AiC). To enrich the diversity of endoscopic datasets, the GC module generates synthesis triplets and enforces geometric consistency via specific losses. To reduce the appearance inconsistency in the video triplets, the AiC module introduces a triplet-masking strategy to act on photometric loss. TCL can be easily embedded into various unsupervised methods without adding extra model parameters. Experiments on public datasets demonstrate that TCL effectively improves the accuracy of unsupervised methods even with limited number of training samples.",https://github.com/EndoluminalSurgicalVision-IMR/TCL,,Guided Interventions and Surgery,Video,Surgical Planning and Simulation,,,,,,,
Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets ,"Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast “wash in/out” phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a “dynamic quality control” (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach, we referred the top 10\% most uncertain segmentations as detected by our dQC tool to the human expert for refinement. This approach resulted in a significant increase in the Dice score (p<0.001) and a notable decrease in the number of images with failed segmentation (16.2% to 11.3%) whereas the alternative approach of randomly selecting the same number of segmentations for human referral did not achieve any significant improvement. Our results suggest that the proposed dQC framework has the potential to accurately identify poor-quality segmentations and may enable efficient DNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical interpretation and reporting of dynamic CMRI datasets.",https://github.com/dyalcink/dQC,,Interpretability / Explainability,Cardiac,Image Segmentation,Uncertainty,MRI,,,,,
Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI ,"Heart failure is a severe and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an uncertainty-based binning strategy to identify poor-quality training samples. We leverage complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and cardiac measurements. The experimental analysis on a large cohort of 1346 subjects who underwent the RHC procedure for PAWP estimation indicates that the proposed pipeline has a diagnostic value and can produce promising performance with significant improvement over the baseline in clinical practice. The decision curve analysis further confirms the clinical utility of our method. The source code can be found at: https://github.com/prasunc/PAWP.",https://github.com/prasunc/PAWP,,Cardiac,Other,MRI,Treatment Response and Outcome/Disease Prediction,,,,,,
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image ,"We propose a novel text-guided cross-position attention module which aims at applying a multi-modality of text and image to position attention in medical image segmentation. To match the dimension of the text feature to that of the image feature map, we multiply learnable parameters by text features and combine the multi-modal semantics via cross-attention. It allows a model to learn the dependency between various characteristics of text and image. Our proposed model demonstrates superior performance compared to other medical models using image-only data or image-text data. Furthermore, we utilize our module as a region of interest (RoI) generator to classify the inflammation of the sacroiliac joints. The RoIs obtained from the model contribute to improve the performance of classification models.",,https://www.kaggle.com/datasets/aysendegerli/qatacov19-dataset,Image Segmentation,Computer Aided Diagnosis,Attention models,Text (clinical/radiology reports),,,,,,
Text-guided Foundation Model Adaptation for Pathological Image Classification ,"The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multi-modal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification. Code is available at https://github.com/Yunkun-Zhang/CITE.",https://github.com/Yunkun-Zhang/CITE,https://zenodo.org/record/6550925,Computer Aided Diagnosis,Abdomen,Computational (Integrative) Pathology,Data Efficient Learning,Model Generalizability / Federated Learning,Transfer learning,Histopathology,Text (clinical/radiology reports),,
The Role of Subgroup Separability in Group-Fair Medical Image Classification ,"We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.",https://github.com/biomedia-mira/subgroup-separability,https://stanfordmlgroup.github.io/competitions/chexpert/,Other,Computer Aided Diagnosis,Interpretability / Explainability,Model Generalizability / Federated Learning,,,,,,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound ,"We explore the potential of deep convolutional neural network (CNN) models for differential diagnosis of gout from musculoskeletal ultrasound (MSKUS), as no prior study on this topic is known. Our exhaustive study of state-of-the-art (SOTA) CNN image classification models for this problem reveals that they often fail to learn the gouty MSKUS features, including the double contour sign, tophus, and snowstorm, which are essential for sonographers’ decisions. To address this issue, we establish a framework to adjust CNNs to “think like sonographers” for gout diagnosis, which consists of three novel components: (1) Where to adjust: Modeling sonographers’ gaze map to emphasize the region that needs adjust; (2) What to adjust: Classifying instances to systematically detect predictions made based on unreasonable/biased reasoning and adjust; (3) How to adjust: Developing a training mechanism to balance gout prediction accuracy and attention reasonability for improved CNNs. The experimental results on clinical MSKUS datasets demonstrate the superiority of our method over several SOTA CNNs.",,,Ultrasound,Musculoskeletal,Data Efficient Learning,,,,,,,
Thyroid Nodule Diagnosis in Dynamic Contrast-enhanced Ultrasound via Microvessel Infiltration Awareness ,"Dynamic contrast-enhanced ultrasound (CEUS) video with microbubble contrast agents reflects the microvessel distribution and dynamic microvessel perfusion, and may provide more discriminative information than conventional gray ultrasound (US). Thus, CEUS video has vital clinical value in differentiating between malignant and benign thyroid nodules. In particular, the CEUS video can show numerous neovascularisations around the nodule, which constantly infiltrate the surrounding tissues. Although the infiltrative of microvessel is ambiguous on CEUS video, it causes the tumor size and margin to be larger on CEUS video than on conventional gray US and may promote the diagnosis of thyroid nodules. In this paper, we propose a novel framework to diagnose thyroid nodules based on dynamic CEUS video by considering microvessel infiltration and via segmented confidence mapping assists diagnosis. Specifically, the Temporal Projection Attention (TPA) is proposed to complement and interact with the semantic information of microvessel perfusion from the time dimension of dynamic CEUS. In addition, we employ a group of confidence maps with a series of flexible Sigmoid Alpha Functions (SAF) to aware and describe the infiltrative area of microvessel for enhancing diagnosis. The experimental results on clinical CEUS video data indicate that our approach can attain an diagnostic accuracy of 88.79% for thyroid nodule and perform better than conventional methods. In addition, we also achieve an optimal dice of 85.54% compared to other classical segmentation methods. Therefore, consideration of dynamic microvessel perfusion and infiltrative expansion is helpful for CEUS-based diagnosis and segmentation of thyroid nodules. The datasets and codes will be available.",,,Ultrasound,Oncology,Interpretability / Explainability,other,Video,Treatment Response and Outcome/Disease Prediction,,,,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset ,"Accurate segmentation of pulmonary airways and vessels is crucial for the diagnosis and treatment of pulmonary diseases. However, current deep learning approaches suffer from disconnectivity issues that hinder their clinical usefulness. To address this challenge, we propose a post-processing approach that leverages a data-driven method to repair the topology of disconnected pulmonary tubular structures. Our approach formulates the problem as a keypoint detection task, where a neural network is trained to predict keypoints that can bridge disconnected components. We use a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. Moreover, the new Pulmonary Tree Repairing (PTR) dataset is publicly available, which comprises 800 complete 3D models of pulmonary airways, arteries, and veins, as well as the synthetic disconnected data. Our code and data are available at \url{https://github.com/M3DV/pulmonary-tree-repairing}.",https://github.com/M3DV/pulmonary-tree-repairing,https://onedrive.live.com/?authkey=%21AEq1v5hZHJORzRA&id=66346B2D10575CA6%21252787&cid=66346B2D10575CA6,Lung,Vascular,Computer Aided Diagnosis,,,,,,,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier ,"Automatic labeling of coronary arteries is an essential task in the practical diagnosis process of cardiovascular diseases. For experienced radiologists, the anatomically predetermined connections are important for labeling the artery segments accurately, while this prior knowledge is barely explored in previous studies. In this paper, we present a new framework called TopoLab which incorporates the anatomical connections into the network design explicitly. Specifically, the strategies of intra-segment feature aggregation and inter-segment feature interaction are introduced for hierarchical segment feature extraction. Moreover, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.",,https://github.com/zutsusemi/MICCAI2023-TopoLab-Labels/,Vascular,Cardiac,CT,,,,,,,
Topology-Preserving Computed Tomography Super-resolution Based on Dual-stream Diffusion Model ,"X-ray computed tomography (CT) is indispensable for modern medical diagnosis, but the degradation of spatial resolution and image quality can adversely affect analysis and diagnosis. Although super-resolution (SR) techniques can help restore lost spatial information and improve imaging resolution for low-resolution CT (LRCT), they are always criticized for topology distortions and secondary artifacts. To address this challenge, we propose a dual-stream diffusion model for super-resolution with topology preservation and structure fidelity. The diffusion model employs a dual-stream structure-preserving network and an imaging enhancement operator in the denoising process for image information and structural feature recovery. The imaging enhancement operator can achieve simultaneous enhancement of vascular and blob structures in CT scans, providing the structure priors in the super-resolution process. The final super-resolved CT is optimized in both the conventional imaging domain and the proposed vascular structure domain. Furthermore, for the first time, we constructed an ultra-high resolution CT scan dataset with a spatial resolution of 0.34×0.34 mm^2 and an image size of 1024×1024 as a super-resolution training set. Quantitative and qualitative evaluations show that our proposed model can achieve comparable information recovery and much better structure fidelity compared to the other state-of-the-art methods. The performance of high-level tasks, including vascular segmentation and lesion detection on super-resolved CT scans, is comparable to or even better than that of raw HRCT. The source code is publicly available at (***).",https://github.com/Arturia-Pendragon-Iris/UHRCT_SR,,Image Reconstruction,Lung,CT,,,,,,,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis ,"Fairness has become increasingly pivotal in medical image recognition. However, without mitigating bias, deploying unfair medical AI systems could harm the interests of underprivileged populations. In this paper, we observe that while features extracted from the deeper layers of neural networks generally offer higher accuracy, fairness conditions deteriorate as we extract features from deeper layers. This phenomenon motivates us to extend the concept of multi-exit frameworks. Unlike existing works mainly focusing on accuracy, our multi-exit framework is fairness-oriented; the internal classifiers are trained to be more accurate and fairer, with high extensibility to apply to most existing fairness-aware frameworks. During inference, any instance with high confidence from an internal classifier is allowed to exit early. Experimental results show that the proposed framework can improve the fairness condition over the state-of-the-art in two dermatological disease datasets.",https://github.com/chiuhaohao/Fair-Multi-Exit-Framework/tree/master,https://challenge.isic-archive.com/data/#2019,Other,Dermatology,,,,,,,,
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer ,"Deep learning has drawn increasing attention in microstructure estimation with undersampled diffusion MRI (dMRI) data. A representative method is the hybrid graph transformer (HGT), which achieves promising performance by integrating q-space graph learning and x-space transformer learning into a uniﬁed framework. However, this method overlooks the 3D spatial information as it relies on training with 2D slices. To address this limitation, we propose 3D hybrid graph transformer (3D-HGT), an advanced microstructure estimation model capable of making full use of 3D spatial information and angular information. To tackle the large computation burden associated with 3D x-space learning, we propose an efficient q-space learning model based on simplified graph neural networks. Furthermore, we propose a 3D x-space learning module based on the transformer. Extensive experiments on data from the human connectome project show that our 3D-HGT outperforms state-of-the-art methods, including HGT, in both quantitative and qualitative evaluations.",,,Neuroimaging - DWI and Tractography,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,,
Towards AI-driven radiology education: A self-supervised segmentation-based framework for high-precision medical image editing ,"Medical education is essential for providing the best patient care in medicine, but creating educational materials using real-world data poses many challenges. For example, the diagnosis and treatment of a disease can be affected by small but significant differences in medical images; however, collecting images to highlight such differences is often costly. Therefore, medical image editing, which allows users to create their intended disease characteristics, can be useful for education. However, existing image-editing methods typically require manually annotated labels, which are labor-intensive and often challenging to represent fine-grained anatomical elements precisely. Herein, we present a novel algorithm for editing anatomical elements using segmentation labels acquired through self-supervised learning. Our self-supervised segmentation achieves pixel-wise clustering under the constraint of invariance to photometric and geometric transformations, which are assumed not to change the clinical interpretation of anatomical elements. The user then edits the segmentation map to produce a medical image with the intended detailed findings. Evaluation by five expert physicians demonstrated that the edited images appeared natural as medical images and that the disease characteristics were accurately reproduced.",https://github.com/Kaz-K/medical-image-editing,,Other,Lung,Oncology,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels ,"Deep learning-based medical image segmentation usually requires abundant high-quality labeled data from experts, yet, it is often infeasible in clinical practice. Without sufficient expert-examined labels, the supervised approaches often struggle with inferior performance. Unfortunately, directly introducing additional data with low-quality cheap annotations (e.g., crowdsourcing from non-experts) may confuse the training. To address this, we propose a Prototypical Label Isolation Learning (PLIL) framework to robustly learn left atrium segmentation from scarce high-quality labeled data and massive low-quality labeled data, which enables effective expert-amateur collaboration. Particularly, PLIL is built upon the popular teacher-student framework. Considering the structural characteristics that the semantic regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space, the self-ensembling teacher model isolates clean and noisy labeled voxels by exploiting their relative feature distances to the class prototypes via multi-scale voting. Then, the student follows the teacher’s instruction for adaptive learning, wherein the clean voxels are introduced as supervised signals and the noisy ones are regularized via perturbed stability learning, considering their large intra-class variation. Comprehensive experiments on the left atrium segmentation benchmark demonstrate the superior performance of our approach.",https://github.com/lemoshu/PLIL,https://github.com/yulequan/UA-MT/tree/master/data,Cardiac,Image Segmentation,Data Efficient Learning,MRI,,,,,,
Towards frugal unsupervised detection of subtle abnormalities in medical imaging ,"Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of nor- mal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-off between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design effort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and efficient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data volumes as they require high memory usage. To address this issue, we propose to incrementally compute inferential quantities. This online ap- proach is illustrated on the challenging detection of subtle abnormalities in MR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The identified structural abnormalities are consistent with the disease progression, as accounted by the Hoehn and Yahr scale.",https://github.com/geoffroyO/onlineEM,,Other,Neuroimaging - Others,Image Segmentation,MRI,,,,,,
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains ,"Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide. Early and accurate grading of its severity is crucial for disease management. Although deep learning (DL) has shown great potential for automated DR grading, its real-world deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization (DG) problem. Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios.  Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet).  GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR).  FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations. Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs. Finally, we design a publicly available benchmark for fair evaluations. Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet.",https://github.com/chehx/DGDR,,Computer Aided Diagnosis,Ophthalmology,Model Generalizability / Federated Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,,
Towards multi-modal anatomical landmark detection for ultrasound-guided brain tumor resection with contrastive learning ,"Homologous anatomical landmarks between medical scans are instrumental in quantitative assessment of image registration quality in various clinical applications, such as MRI-ultrasound registration for tissue shift correction in ultrasound-guided brain tumor resection. While manually identified landmark pairs between MRI and ultrasound (US) have greatly facilitated the validation of different registration algorithms for the task, the procedure requires significant expertise, labor, and time, and can be prone to inter- and intra-rater inconsistency. So far, many traditional and machine learning  approaches have been presented for anatomical landmark detection, but they primarily focus on mono-modal applications. Unfortunately, despite the clinical needs, inter-modal/contrast landmark detection has very rarely been attempted. Therefore, we propose a novel contrastive learning framework to detect corresponding landmarks between MRI and intra-operative US scans in neurosurgery. Specifically, two convolutional neural networks were trained jointly to encode image features in MRI and US scans to help match the US image patch that contain the corresponding landmarks in the MRI. We developed and validated the technique using the public RESECT database. With a mean landmark detection accuracy of 5.88±4.79 mm against 18.78±4.77 mm with SIFT features, the proposed method offers promising results for MRI-US landmark detection in neurosurgical applications for the first time.",,,Guided Interventions and Surgery,Neuroimaging - Others,Image Registration,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,Ultrasound,,,,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering ,"Existing deep learning models have achieved promising performance in recognizing skin diseases from dermoscopic images. However, these models can only recognize samples from predefined categories, when they are deployed in the clinic, data from new unknown categories are constantly emerging. Therefore, it is crucial to automatically discover and identify new semantic categories from new data. In this paper, we propose a new novel class discovery framework for automatically discovering new semantic classes from dermoscopy image datasets based on the knowledge of known classes. Specifically, we first use contrastive learning to learn a robust and unbiased feature representation based on all data from known and unknown categories. We then propose an uncertainty-aware multi-view cross pseudo-supervision strategy, which is trained jointly on all categories of data using pseudo labels generated by a self-labeling strategy. Finally, we further refine the pseudo label by aggregating neighborhood information through local sample similarity to improve the clustering performance of the model for unknown categories. We conducted extensive experiments on the dermatology dataset ISIC 2019, and the experimental results show that our approach can effectively leverage knowledge from known categories to discover new semantic categories. We also further validated the effectiveness of the different modules through extensive ablation experiments. Our code will be released soon.",,,Computer Aided Diagnosis,Other,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,
TPRO: Text-prompting-based Weakly Supervised Histopathology Tissue Segmentation ,"Most existing weakly-supervised segmentation methods rely on class activation maps (CAM) to generate pseudo-labels for training segmentation models. However, CAM has been criticized for highlighting only the most discriminative parts of the object, leading to poor quality of pseudo-labels. Although some recent methods have attempted to extend CAM to cover more areas, the fundamental problem still needs to be solved. We believe this problem is due to the huge gap between image-level labels and pixel-level predictions and that additional information must be introduced to address this issue. Thus, we propose a text-prompting-based weakly supervised segmentation method (TPRO), which uses text to introduce additional information. TPRO employs a vision and label encoder to generate a similarity map for each image, which serves as our localization map. Pathological knowledge is gathered from the internet and embedded as knowledge features, which are used to guide the image features through a knowledge attention module. Additionally, we employ a deep supervision strategy to utilize the network’s shallow information fully. Our approach outperforms other weakly supervised segmentation methods on benchmark datasets LUAD-HistoSeg and BCSS-WSSS datasets, setting a new state of the art. Code is available at: https://github.com/zhangst431/TPRO.",https://github.com/zhangst431/TPRO,https://drive.google.com/drive/folders/1E3Yei3Or3xJXukHIybZAgochxfn6FJpr,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Image Segmentation,Attention models,Data Efficient Learning,Histopathology,Text (clinical/radiology reports),,,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images ,"This paper proposes a method for trackerless ultrasound volume reconstruction in the context of minimally invasive surgery. It is based on a Siamese architecture, including a recurrent neural network that leverages the ultrasound image features and the optical flow to estimate the relative position of frames. Our method does not use any additional sensor and was evaluated on \textit{ex vivo} porcine data. It achieves translation and orientation errors of $0.449 \pm 0.189 $ mm and $1.3 \pm 1.5 $ degrees respectively for the relative pose estimation. In addition, despite the predominant non-linearity motion in our context, our method achieves a good reconstruction with final and average drift rates of 23.11\% and 28.71\% respectively.  To the best of our knowledge, this is the first work to address volume reconstruction in the context of intravascular ultrasound.",https://github.com/Sidaty1/IVUS_Trakerless_Volume_Reconstruction,,Image Reconstruction,Abdomen,Vascular,Ultrasound,,,,,,
Tracking adaptation to improve SuperPoint for 3D reconstruction in endoscopy ,"Endoscopy is the gold standard procedure for early detection and treatment of numerous diseases. Obtaining 3D reconstructions from real endoscopic videos would facilitate the development of assistive tools for practitioners, but it is a challenging problem for current Structure From Motion (SfM) methods. Feature extraction and matching are key steps in SfM approaches, and these are particularly difficult in the endoscopy domain due to deformations, poor texture, and numerous artifacts in the images. 
This work presents a novel learned model for feature extraction in endoscopy, called SuperPoint-E, which improves upon existing work using recordings from real medical practice. 
SuperPoint-E is based on the SuperPoint architecture but is trained with a novel supervision strategy. The supervisory signal used in our work comes from features extracted with existing detectors (SIFT and SuperPoint) that can be successfully tracked and triangulated in short endoscopy clips (building a 3D model using COLMAP).
In our experiments, SuperPoint-E obtains more and better features than any of the baseline detectors used as supervision. We validate the effectiveness of our model for 3D reconstruction in real endoscopy data.",https://github.com/LeonBP/SuperPointTrackingAdaptation,https://arxiv.org/abs/2204.14240,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Video,,,,,,,,
TractCloud: Registration-free Tractography Parcellation with a Novel Local-global Streamline Point Cloud Representation ,"Diffusion MRI tractography parcellation classifies streamlines into anatomical fiber tracts to enable quantification and visualization for clinical and scientific applications. Current tractography parcellation methods rely heavily on registration, but registration inaccuracies can affect parcellation and the computational cost of registration is high for large-scale datasets. Recently, deep-learning-based methods have been proposed for tractography parcellation using various types of representations for streamlines. However, these methods only focus on the information from a single streamline, ignoring geometric relationships between the streamlines in the brain. We propose TractCloud, a registration-free framework that performs whole-brain tractography parcellation directly in individual subject space. We propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain. We train our framework on a large-scale labeled tractography dataset, which we augment by applying synthetic transforms including rotation, scaling, and translations. We test our framework on five independently acquired datasets across populations and health conditions. TractCloud significantly outperforms several state-of-the-art methods on all testing datasets. TractCloud achieves efficient and consistent whole-brain white matter parcellation across the lifespan (from neonates to elderly subjects, including brain tumor patients) without the need for registration. The robustness and high inference speed of TractCloud make it suitable for large-scale tractography data analysis. Our project page is available at https://tractcloud.github.io.",https://tractcloud.github.io/,https://tractcloud.github.io/,Neuroimaging - DWI and Tractography,Other,MRI,,,,,,,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors ,"Whole-Slide Histopathology Image (WSI) is regarded as the gold standard for survival prediction of Breast Cancer (BC) across different subtypes. However, in cancer prognosis applications, the cost of acquiring patients’ survival information is high and can be extremely difficult in practice. By considering that there exists certain common mechanism for  tumor progression among different subtypes of the Breast Invasive Carcinoma(BRCA), it becomes critical to utilize data from a related subtype of BRCA to help predict the patients’ survival on target domain. To address this issue, we proposed a TILs-Tumor interactions guided unsupervised domain adaptation (T2UDA) algorithm to predict the patients’ survival on the target BC subtype. Different from the existing feature-level or instance-level transfer learning strategy, our study considered the fact that the tumor-infiltrating lymphocytes (TILs) and its correlation with tumors reveal similar role in the prognosis of different BRCA subtypes. More specifically, T2UDA firstly employed the Graph Attention Network (GAT) to learn the node embeddings and the spatial interactions between tumor and TILs patches in WSI. Then, besides aligning the embeddings of different types of nodes across the source and target domains, we proposed a novel Tumor-TILs interaction alignment (TTIA) module to ensure that the distribution of interaction weights are similar in both domains. We evaluated the performance of our method on the BRCA cohort derived from the Cancer Genome Atlas (TCGA), and the experimental results indicated that T2UDA outperformed other domain adaption methods for predicting patients’ clinical outcome.",,,Computational (Integrative) Pathology,Histopathology,,,,,,,,
Transferability-Guided Multi-Source Model Adaptation for Medical Image Segmentation ,"Unsupervised domain adaptation has drawn sustained attentions in medical image segmentation by transferring knowledge from labeled source data to unlabeled target domain. However, most existing approaches assume the source data are collected from a single client, which cannot be successfully applied to explore complementary transferable knowledge from multiple source domains with large distribution discrepancy. Moreover, they require access to source data during training, which is inefficient and unpractical due to privacy preservation and memory storage. To address these challenges, we study a novel and practical problem, named multi-source model adaptation (MSMA), which aims to transfer multiple source models to the unlabeled target domain without any source data. Since no target label and source data is provided to evaluate the transferability of each source model or domain gap between the source and the target domain, we may encounter negative transfer by those less related source domains, thus hurting target performance. To solve this problem, we propose a transferability-guided model adaptation (TGMA) framework to eliminate negative transfer. Specifically, 1) A label-free transferability metric (LFTM) is designed to evaluate transferability of source models without target annotations for the first time. 2) Based on the designed metric, we compute instance-level transferability matrix (ITM) for target pseudo label correction and domain-level transferability matrix (DTM) to achieve model selection for better target model initialization. Extensive experiments on multi-site prostate segmentation dataset demonstrate the superiority of our framework.",https://github.com/CityU-AIM-Group/TGMA,,Data Efficient Learning,Transfer learning,,,,,,,,
Transformer-based Annotation Bias-aware Medical Image Segmentation ,"Manual medical image segmentation is subjective and suffers from annotator-related bias, which can be mimicked or amplified by deep learning methods. Recently, researchers have suggested that such bias is the combination of the annotator preference and stochastic error, which are modeled by convolution blocks located after decoder and pixel-wise independent Gaussian distribution, respectively. It is unlikely that convolution blocks can effectively model the varying degrees of preference at the full resolution level. Additionally, the independent pixel-wise Gaussian distribution disregards pixel correlations, leading to a discontinuous boundary. This paper proposes a Transformer-based Annotation Bias-aware (TAB) medical image segmentation model, which tackles the annotator-related bias via modeling annotator preference and stochastic errors. TAB employs the Transformer with learnable queries to extract the different preference-focused features. This enables TAB to produce segmentation with various preferences simultaneously using a single segmentation head. Moreover, TAB takes the multivariant normal distribution assumption that models pixel correlations, and learns the annotation distribution to disentangle the stochastic error. We evaluated our TAB on an OD/OC segmentation benchmark annotated by six annotators. Our results suggest that TAB outperforms existing medical image segmentation models which take into account the annotator-related bias.",https://github.com/Merrical/TAB,https://deepblue.lib.umich.edu/data/concern/data_sets/3b591905z,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,,,,,,,
Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions ,"Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.",,,Image Reconstruction,Cardiac,PET/SPECT,,,,,,,
Transformer-based end-to-end classification of variable-length volumetric data ,"The automatic classification of 3D medical data is memory-intensive. Also, variations in the number of slices between samples is common. Naïve solutions such as subsampling can solve these problems, but at the cost of potentially eliminating relevant diagnosis information. Transformers have shown promising performance for sequential data analysis.  However, their application for long sequences is data, computationally, and memory demanding.
In this paper, we propose an end-to-end Transformer-based framework that allows to classify volumetric data of variable length in an efficient fashion. Particularly, by randomizing the input volume-wise resolution(#slices) during training, we enhance the capacity of the learnable positional embedding assigned to each volume slice. Consequently, the accumulated positional information in each positional embedding can be generalized to the neighbouring slices, even for high-resolution volumes at the test time. By doing so, the model will be more robust to variable volume length and amenable to different computational budgets. We evaluated the proposed approach in retinal OCT volume classification and achieved 21.96% average improvement in balanced accuracy on a 9-class diagnostic task, compared to state-of-the-art video transformers. Our findings show that varying the volume-wise resolution of the input during training results in more informative volume representation as compared to training with fixed number of slices per volume.",https://github.com/marziehoghbaie/VLFAT,https://zenodo.org/record/7105232#.ZArNp-zML60,Computer Aided Diagnosis,Ophthalmology,other,,,,,,,
"Transformer-based tooth segmentation, identification and pulp calcification recognition in CBCT ","The recognition of dental pulp calcification has important value for oral clinic, which determines the subsequent treatment decision. However, the recognition of dental pulp calcification is remarkably difficult in clinical practice due to its atypical morphological characteristics. In addition, pulp calcification is also difficult to be visualized in high-resolution CBCT due to its small area and weak contrast. In this work, we proposed a new method of tooth segmentation, identification and pulp calcification recognition based on Transformer to achieve accurate recognition of pulp calcification in high-resolution CBCT images. First, in order to realize that the network can handle extremely high-resolution CBCT, we proposed a coarse-to-fine method to segment the tooth instance in the down-scaled low-resolution CBCT image, and then back to the high-resolution CBCT image to intercept the region of the tooth as the input for the fine segmentation, identification and pulp calcification recognition. Then, in order to enhance the weak distinction between normal teeth and calcified teeth, we proposed tooth instance correlation and triple loss to improve the recognition performance of calcification. Finally, we built a multi-task learning architecture based on Transformer to realize the tooth segmentation, identification and calcification recognition for mutual promotion between tasks. The clinical data verified the effectiveness of the proposed method for the recognition of pulp calcification in high-resolution CBCT for digital dentistry.",,,Computer Aided Diagnosis,CT,,,,,,,,
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification ,"Early diagnosis of focal liver lesions (FLLs) can decrease the fatality rate of liver cancer, which remains a big challenge. We designed a deep learning approach based on CT to assess and differentiate FLLs. To achieve high accuracy, CTs in different phases are integrated to provide more information than single-phase images. While most of the related studies use convolutional neural networks, we exploit the Transformer for multi-phase liver lesion classification. We propose a hybrid model called TransLiver, which has a transformer backbone and complementary convolutional modules. Specifically, we connect modified transformer blocks with convolutional encoder and down-samplers. For multi-phase fusion, we utilize cross phase tokens to reinforce the phases communication. In addition, we introduce a pre-processing unit to resolve realistic annotation issues. Extensive experiments are conducted, in which we achieve an overall accuracy of 90.9% on an in-house dataset of four CT phases and seven liver lesion classes. The results also show distinct advantages in comparison to state-of-art approaches in classification.",https://github.com/sherrydoge/TransLiver,,Attention models,Abdomen,Oncology,Computer Aided Diagnosis,Transfer learning,CT,,,,
TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation ,"Nuclei appear small in size, yet, in real clinical practice, the global spatial information and correlation of the color or brightness contrast between nuclei and background, have been considered a crucial component for accurate nuclei segmentation. However, the field of automatic nuclei segmentation is dominated by Convolutional Neural Networks (CNNs), meanwhile, the potential of the recently prevalent Transformers has not been fully explored, which is powerful in capturing local-global correlations. To this end, we make the first attempt at a pure Transformer framework for nuclei segmentation, called TransNuSeg. Different from prior work, we decouple the challenging nuclei segmentation task into an intrinsic multi-task learning task, where a tri-decoder structure is employed for nuclei instance, nuclei edge, and clustered edge segmentation respectively. To eliminate the divergent predictions from different branches in previous work, a novel self-distillation loss is introduced to explicitly impose consistency regulation between branches. Moreover, to formulate the high correlation between branches and also reduce the number of parameters, an efficient attention sharing scheme is proposed by partially sharing the self-attention heads amongst the tri-decoders. Finally, a token MLP bottleneck replaces the over-parameterized Transformer bottleneck for a further reduction in model complexity. Experiments on two datasets of different modalities, including MoNuSeg have shown that our methods can outperform state-of-the-art counterparts such as CA-2.5 by 2-3% Dice with 30% fewer parameters. In conclusion, TransNuSeg confirms the strength of Transformer in the context of nuclei segmentation, which thus can serve as an efficient solution for real clinical practice. Code is available at https://github.com/zhenqi-he/transnuseg.",https://github.com/zhenqi-he/transnuseg,https://github.com/lu-yizhou/ClusterSeg,Image Segmentation,,,,,,,,,
Treasure in Distribution: A Domain Randomization based Multi-Source Domain Generalization for 2D Medical Image Segmentation ,"Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications. Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability. Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles. In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution. To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods. Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data. Code is available at https://github.com/Chen-Ziyang/TriD.",https://github.com/Chen-Ziyang/TriD,https://zenodo.org/record/8009107,Image Segmentation,Model Generalizability / Federated Learning,MRI,other,,,,,,
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data ,"Intracerebral hemorrhage (ICH) is the second most common and deadliest form of stroke. Despite medical advances, predicting treatment outcomes for ICH remains a challenge. This paper proposes a novel prognostic model that utilizes both imaging and tabular data to predict treatment outcome for ICH. Our model is designed to be trained on observational data collected from non-randomized controlled trials, providing reliable predictions of treatment success. Specifically, we propose to employ a variational autoencoder model to generate a low-dimensional prognostic score, which can effectively address the selection bias resulting from the non-randomized controlled trials. Importantly, we develop a variational distributions combination module that combines the information from imaging data, non-imaging clinical data, and treatment assignment to accurately generate the prognostic score. We conducted extensive experiments on a real-world clinical dataset of intracerebral hemorrhage. Our proposed method demonstrates a substantial improvement in treatment outcome prediction compared to existing state-of-the-art approaches. Code is released in the supplementary.",,,Treatment Response and Outcome/Disease Prediction,Imaging Biomarkers,,,,,,,,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer ,"Quantitative immunofluorescence (QIF) enables identifying immune cell subtypes across histopathology images. There is substantial evidence to show that spatial architecture of immune cell populations (e.g. CD4+, CD8+, CD20+) is associated with therapy response in cancers, yet there is a paucity of approaches to quantify spatial statistics of interplay across immune subtypes. 
Previously, analyzing spatial cell interplay have been limited to either building subgraphs on individual cell types before feature extraction or capturing the interaction between two cell types. However, looking at the spatial interplay between more than two cell types reveals complex interactions and co-dependencies that might have implications in predicting response to therapies like immunotherapy. In this work we present, Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL), a novel approach involving building of heterogeneous subgraphs to precisely capture the spatial interplay between multiple cell families. Primarily,  TriAnGIL focuses on triadic closures, and uses metrics to quantify triads instead of two-by-two relations and therefore considers both inter- and intra-family relationships between cells.
The TriaAnGIL’s efficacy for microenvironment characterization from QmIF images is demonstrated in problems of predicting (1) response to immunotherapy (N=122) and (2) overall survival (N=135) in patients with lung cancer in comparison with four hand-crafted approaches namely DenTIL, GG, CCG, SpaTIL, and deep learning with GNN.
For both tasks, TriaAnGIL outperformed hand-crafted approaches, and GNN with AUC=.70, C-index=.64. In terms of interpretability, TriAnGIL easily beats GNN, by pulling biological insights from immune cells interplay and shedding light on the triadic interaction of CD4+-Tumor-stromal cells.",https://github.com/sarayar/TriAnGIL,,Other,Lung,Computational (Integrative) Pathology,Histopathology,,,,,,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms ,"To obtain high-quality positron emission tomography (PET) images while minimizing radiation exposure, various methods have been proposed for reconstructing standard-dose PET (SPET) images from low-dose PET (LPET) sinograms directly. However, current methods often neglect boundaries dur-ing sinogram-to-image reconstruction, resulting in high-frequency distortion in the frequency domain and diminished or fuzzy edges in the reconstructed images. Furthermore, the convolutional architectures, which are commonly used, lack the ability to model long-range non-local interactions, potentially leading to inaccurate representations of global structures. To alleviate these problems, in this paper, we propose a transformer-based model that unites triple domains of sinogram, image, and frequency for direct PET reconstruc-tion, namely TriDo-Former. Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Different from the vanilla transformer that splits an image into 2D patches, based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from prorogating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.",https://github.com/gluucose/TriDoFormer,,Image Reconstruction,Neuroimaging - Others,PET/SPECT,,,,,,,
Trust your neighbours: Penalty-based constraints for model calibration ,"Ensuring reliable confidence scores from deep networks is of pivotal importance in critical decision-making systems, notably in the medical domain. While recent literature on calibrating deep segmentation networks has led to significant progress, their uncertainty is usually modeled by leveraging the information of individual pixels, which disregards the local structure of the object of interest. In particular, only the recent Spatially Varying Label Smoothing (SVLS) approach addresses this issue by softening the pixel label assignments with a discrete spatial Gaussian kernel. In this work, we first present a constrained optimization perspective of SVLS and demonstrate that it enforces an implicit constraint on soft class proportions of surrounding pixels. Furthermore, our analysis shows that SVLS lacks a mechanism to balance the contribution of the constraint with the primary objective, potentially hindering the optimization process. Based on these observations, we propose a principled and simple solution based on equality constraints on the logit values, which enables to control explicitly both the enforced constraint and the weight of the penalty, offering more flexibility. Comprehensive experiments on a variety of well-known segmentation benchmarks demonstrate the superior performance of the proposed approach. The code is available at \url{https://github.com/Bala93/MarginLoss}",https://github.com/Bala93/MarginLoss,https://www.med.upenn.edu/cbica/brats2020/data.html,Image Segmentation,Uncertainty,,,,,,,,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer ,"Optical Intraoral Scanners (IOS) are widely used in digi- tal dentistry to provide detailed 3D information of dental crowns and the gingiva. Accurate 3D tooth segmentation in IOSs is critical for vari- ous dental applications, while previous methods are error-prone at com- plicated boundaries and exhibit unsatisfactory results across patients. In this paper, we propose TSegFormer which captures both local and global dependencies among different teeth and the gingiva in the IOS point clouds with a multi-task 3D transformer architecture. Moreover, we design a geometry-guided loss based on a novel point curvature to refine boundaries in an end-to-end manner, avoiding time-consuming post-processing to reach clinically applicable segmentation. In addition, we create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of our knowledge. The experimental results demonstrate that our TSegFormer consistently surpasses existing state-of-the-art baselines. The superiority of TSegFormer is corroborated by extensive analysis, vi- sualizations and real-world clinical applicability tests. Our code is avail- able at https://github.com/huiminxiong/TSegFormer.",https://github.com/huiminxiong/TSegFormer,,Computer Aided Diagnosis,Image Segmentation,other,,,,,,,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks ,"Recent advances in wearable healthcare devices such as smartwatches allow us to monitor and manage our health condition more actively, for example, by measuring our electrocardiogram (ECG) and predicting cardiovascular diseases (CVDs) such as atrial fibrillation in real-time. Nevertheless, most smart devices can only measure single-lead signals, such as Lead I, while multichannel ECGs, such as twelve-lead signals, are necessary to identify more intricate CVDs such as left and right bundle branch blocks. In this paper, to address this problem, we propose a novel generative adversarial network (GAN) that can faithfully reconstruct 12-lead ECG signals from single-lead signals, which consists of two generators and one 1D U-Net discriminator. Experimental results show that it outperforms other representative generative models. Moreover, we also validate our method’s ability to effectively reconstruct CVD-related characteristics by evaluating reconstructed ECGs with a highly accurate 12-lead ECG-based prediction model and three cardiologists.",https://github.com/knu-plml/ecg-recon,,EEG/ECG,Cardiac,Computer Aided Diagnosis,Image Reconstruction,Other,,,,,
Ultrasonic tracking of a rapid-exchange microcatheter with simultaneous pressure sensing for cardiovascular interventions ,"Ultrasound imaging is widely used for guiding minimally invasive cardiovascular procedures such as structural heart repair and renal denervation. Visualisation of medical devices such as catheters is critically important and it remains challenging in many clinical contexts. When 2D ultrasound imaging is used, the catheter can readily stray from the imaging plane; with 3D imaging, there can be a loss of visibility at steep angles of insonification.  When the catheter tip is not accurately identified, there can be damage to critical structures and procedural inefficiencies. In this paper, we present a tracking system to directly visualise the catheter tip with a custom fibre optic ultrasound sensor integrated into a microcatheter, in concert with an external ultrasound imaging probe. Pairs of co-registered images were acquired in rapid succession: a tracking image obtained from the ultrasonic sensor signals that were time-synchronised to the ultrasound imaging probe transmissions, and a conventional B-mode ultrasound image. The custom fibre-optic sensor comprised a free-standing membrane originally developed for blood pressure sensing, which was optically interrogated with a wavelength-tuneable laser for ultrasound reception. The measured axial and lateral tracking accuracies in water were both within the range of 0.2 to 1 mm. To obtain a preliminary indication of the clinical potential of this ultrasonic catheter tracking system, insertions of the catheter was delivered over a guidewire into the femoral and renal arteries in an in vivo porcine model and intravascular blood pressure waveforms were obtained concurrently. The results demonstrate that ultrasonic catheter tracking using optically-interrogated fibre optic blood pressure sensors is viable, and that it could be useful to guide minimally invasive cardiovascular procedures by providing accurate, real-time visualisation of the catheter tip.",,,Guided Interventions and Surgery,Cardiac,Vascular,Ultrasound,,,,,,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation ,"Accurate segmentation of the fetal brain from Magnetic Resonance Image (MRI) is important for prenatal assessment of fetal development. Although deep learning has shown the potential to achieve this task, it requires a large fine annotated dataset that is difficult to collect. To address this issue, weakly-supervised segmentation methods with image-level labels have gained attention, which are commonly based on class activation maps from a classification network trained with image-level labels. However, most of these methods suffer from incomplete activation regions, due to the low-resolution localization without detailed boundary cues. To this end, we propose a novel weakly-supervised method with image-level labels based on semantic features and context information exploration. We first propose an Uncertainty-weighted Multi-resolution Class Activation Map (UM-CAM) to generate high-quality pixel-level supervision. Then, we design a Geodesic distance-based Seed Expansion (GSE) method to provide context information for rectifying the ambiguous boundaries of UM-CAM. Extensive experiments on a fetal brain dataset show that our UM-CAM can provide more accurate activation regions with fewer false positive regions than existing CAM variants, and our proposed method outperforms state-of-the-art weakly-supervised segmentation methods learning from image-level labels.",,,Fetal Imaging,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,,
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images ,"Continual test-time adaptation (CTTA) aims to continuously adapt a source-trained model to a target domain with minimal performance loss while assuming no access to the source data. Typically, source models are trained with empirical risk minimization (ERM) and assumed to perform reasonably on the target domain to allow for further adaptation. However, ERM-trained models often fail to perform adequately on a severely drifted target domain, resulting in unsatisfactory adaptation results. To tackle this issue, we propose a generalizable CTTA framework. First, we incorporate domain-invariant shape modeling into the model and train it using domain-generalization (DG) techniques, promoting target-domain adaptability regardless of the severity of the domain shift. Then, an uncertainty and shape-aware mean teacher network performs adaptation with uncertainty-weighted pseudo-labels and shape information. Lastly, small portions of the model’s weights are stochastically reset to the initial domain-generalized state at each adaptation step, preventing the model from ‘diving too deep’ into any specific test samples. The proposed method demonstrates strong continual adaptability and outperforms its peers on three cross-domain segmentation tasks. Code is available online.",https://github.com/ThisGame42/CTTA,https://chaos.grand-challenge.org/,Image Segmentation,Transfer learning,CT,MRI,,,,,,
Uncertainty Inspired Autism Spectrum Disorder Screening ,"People with autism spectrum disorder (ASD) show distinguishing preferences for specific visual stimuli compared to typically developed (TD) individuals, opening the door for objective and quantitative screening by eye-tracking data analysis. However, existing eye-tracking-based ASD screening approaches often assume that there are no individual differences and that all stimuli contribute equally to the prediction of an ASD. Consequently, a fixed number of images are usually selected by a pre-defined strategy for further training and testing, ignoring the distinct characteristics of various subjects viewing the same image. To address the aforementioned difficulties, we propose a novel Uncertainty-inspired ASD Screening Network (UASN) that dynamically modifies the contribution of each stimulus viewed by different subjects. Specifically, we estimate the uncertainty of each stimulus by considering the variation between the subject’s fixation map and the ones of the two clinical groups (i.e., ASD and TD) and further utilize it for weighting the training loss. Besides, to reduce the diagnosis time, instead of the shuffle-appeared mode of image viewing, we propose an uncertainty-based personalized diagnosis method to dynamically rank the viewing images according to the preferences of different subjects, which can achieve high prediction accuracy with only a small set of images. Experiments demonstrate the superior performance of our proposed UASN.",,https://saliency4asd.ls2n.fr/datasets/,Computer Aided Diagnosis,Guided Interventions and Surgery,,,,,,,,
Uncertainty-informed Mutual Learning for Joint Medical Image Classification and Segmentation ,"Classification and segmentation are crucial in medical image analysis as they enable accurate diagnosis and disease monitoring. However, current methods often prioritize the mutual learning features and shared model parameters, while neglecting the reliability of features and performances. In this paper, we propose a novel Uncertainty-informed Mutual Learning (UML) framework for reliable and interpretable medical image analysis. Our UML introduces reliability to joint classification and segmentation tasks, leveraging mutual learning with uncertainty to improve performance. To achieve this, we first use evidential deep learning to provide image-level and pixel-wise confidences. Then, an uncertainty navigator is constructed for better using mutual features and generating segmentation results. Besides, an uncertainty instructor is proposed to screen reliable masks for classification. Overall, UML could produce confidence estimation in features and performance for each link (classification and segmentation). The experiments on the public datasets demonstrate that our UML outperforms existing methods in terms of both accuracy and robustness. Our UML has the potential to explore the development of more reliable and explainable medical image analysis models.",https://github.com/KarryRen/UML,,Image Segmentation,Uncertainty,,,,,,,,
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography ,"Growing evidence from post-mortem and in vivo studies have demonstrated the substantial variability of tau pathology spreading patterns in Alzheimer’s disease(AD). Automated tools for characterizing the heterogeneity of tau pathology will enable a more accurate understanding of the disease and help the development of targeted treatment. In this paper, we propose a Reeb graph representation of tau pathology topography on cortical surfaces using tau PET imaging data. By comparing the spatial and temporal coherence of the Reeb graph representation across subjects, we can build a directed graph to represent the distribution of tau topography over a population, which naturally facilitates the discovery of spatiotemporal subtypes of tau pathology with graph-based clustering. In our experiments, we conducted extensive comparisons with state-of-the-art event-based model on synthetic and large-scale tau PET imaging data from ADNI3 and A4 studies. We demonstrated that our proposed method can more robustly achieve the subtyping of tau pathology with clear clinical significance and demonstrated superior generalization performance than event-based model.",,,Computer Aided Diagnosis,Neuroimaging - Others,Computational Anatomy and Physiology,,,,,,,
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases ,"A confluence of neuroscience and clinical evidence suggests that the disruption of structural connectivity (SC) and functional connectivity (FC) in the brain is an early sign of neurodegenerative diseases years before any clinical signs of the disease progression. Since the changes in SC-FC coupling may provide a potential putative biomarker that detects subtle brain network dysfunction more sensitively than does a single modality, tremendous efforts have been made to understand the relationship between SC and FC from the perspective of connectivity, sub-networks, and network topology. However, the methodology design of current analytic methods lacks the in-depth neuroscience underpinning of to what extent the altered SC-FC coupling mechanisms underline the cognitive decline. To address this challenge, we put the spotlight on a neural oscillation model that characterizes the system behavior of a set of (functional) neural oscillators coupled via (structural) nerve fibers throughout the brain. On top of this, we present a physics-guided graph neural network to understand the synchronization mechanism of system dynamics that is capable of predicting self-organized functional fluctuations. By doing so, we generate a novel SC-FC coupling biomarker that allows us to recognize the early sign of neurodegeneration through the lens of an altered SC-FC relationship. We have evaluated the statistical power and clinical value of new SC-FC biomarker in the early diagnosis of Alzheimer’s disease using the ADNI dataset. Compared to conventional SC-FC coupling methods, our physics-guided deep model not only yields higher prediction accuracy but also reveals the mechanistic role of SC-FC coupling alterations in disease progression.",,,Other,Neuroimaging - DWI and Tractography,Neuroimaging - Functional Brain Networks,MRI,,,,,,
Understanding Silent Failures in Medical Image Classification ,"To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of various examples, we demonstrate how this tool can help researchers gain insight into the requirements for safe application of classification systems in the medical domain. The open-source benchmark and tool are at: https://github.com/IML-DKFZ/sf-visuals.",https://github.com/IML-DKFZ/sf-visuals,,Uncertainty,Visualization in Biomedical Imaging,,,,,,,,
Unified Brain MR-Ultrasound Synthesis using Multi-Modal Hierarchical Representations ,"We introduce MHVAE, a deep hierarchical variational auto-encoder (VAE) that synthesizes missing images from various modalities. Extending multi-modal VAEs with a hierarchical latent structure, we introduce a probabilistic formulation for fusing multi-modal images in a common latent representation while having the flexibility to handle incomplete image sets as input. Moreover, adversarial learning is employed to generate sharper images. Extensive experiments are performed on the challenging problem of joint intra-operative ultrasound (iUS) and Magnetic Resonance (MR) synthesis. Our model outperformed multi-modal VAEs, conditional GANs, and the current state-of-the-art unified method (ResViT) for synthesizing missing images, demonstrating the advantage of using a hierarchical latent representation and a principled probabilistic fusion operation. Our code is publicly available.",https://github.com/ReubenDo/MHVAE,,Image Reconstruction,Neuroimaging - Others,Guided Interventions and Surgery,Interventional Simulation Systems,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,Ultrasound,,,
Unified surface and volumetric inference on functional imaging data ,"Surface-based analysis methods for functional imaging data have been shown to offer substantial benefits for the study of the human cortex, namely in the localisation of functional areas and the establishment of inter-subject correspondence. A new approach for surface-based parameter estimation via non-linear model fitting on functional timeseries data is presented. It treats the different anatomies within the brain in the manner that is most appropriate: surface-based for the cortex, volumetric for white matter, and using regions-of-interest for sub-cortical grey matter structures. The mapping between these different domains is  incorporated using a novel algorithm that accounts for partial volume effects. A variational Bayesian framework is used to perform parameter inference in all anatomies simultaneously rather than separately. This approach, called hybrid inference, has been implemented using stochastic optimisation techniques. A comparison against a conventional volumetric workflow with post-projection on simulated perfusion data reveals improvements parameter recovery, preservation of spatial detail and consistency between spatial resolutions. At 4mm isotropic resolution, the following improvements were obtained: 2.7% in SSD error of perfusion, 16% in SSD error of Z-score perfusion, and 27% in Bhattacharyya distance of perfusion distribution.",https://github.com/tomfrankkirk/svb_evaluation,,Neuroimaging - Others,Other,MRI,,,,,,,
UniSeg: A Prompt-driven Universal Segmentation Model as well as A Strong Representation Learner ,"The universal model emerges as a promising trend for medical image segmentation, paving up the way to build medical imaging large model (MILM). One popular strategy to build universal models is to encode each task as a one-hot vector and generate dynamic convolutional layers at the end of the decoder to extract the interested target. Although successful, it ignores the correlations among tasks and meanwhile is too late to make the modelaware' of the ongoing task. To address both issues, we propose a prompt-driven Universal Segmentation model (UniSeg) for multi-task medical image segmentation using diverse modalities and domains. We first devise a learnable universal prompt to describe the correlations among all tasks and then convert this prompt and image features into a task-specific prompt, which is fed to the decoder as a part of its input. Thus, we make the modelaware’ of the ongoing task early and boost the task-specific training of the whole decoder. Our results indicate that the proposed UniSeg outperforms other universal models and single-task models on 11 upstream tasks. Moreover, UniSeg also beats other pre-trained models on two downstream datasets, providing the community with a high-quality pre-trained model for 3D medical image segmentation. Code and model are available at https://github.com/yeerwen/UniSeg.",https://github.com/yeerwen/UniSeg,https://competitions.codalab.org/competitions/17094,Image Segmentation,Other,CT,MRI,PET/SPECT,,,,,
Unpaired Cross-modal Interaction Learning for COVID-19 Segmentation on Limited CT images ,"Accurate automated segmentation of infected regions in CT images is crucial for predicting COVID-19’s pathological stage and treat- ment response. Although deep learning has shown promise in medical image segmentation, the scarcity of pixel-level annotations due to their expense and time-consuming nature limits its application in COVID- 19 segmentation. In this paper, we propose utilizing large-scale unpaired chest X-rays with classification labels as a means of compensating for the limited availability of densely annotated CT scans, aiming to learn robust representations for accurate COVID-19 segmentation. To achieve this, we design an Unpaired Cross-modal Interaction (UCI) learning frame- work. It comprises a multi-modal encoder, a knowledge condensation (KC) and knowledge-guided interaction (KI) module, and task-specific networks for final predictions. The encoder is built to capture optimal feature representations for both CT and X-ray images. To facilitate in- formation interaction between unpaired cross-modal data, we propose the KC that introduces a momentum-updated prototype learning strat- egy to condense modality-specific knowledge. The condensed knowledge is fed into the KI module for interaction learning, enabling the UCI to capture critical features and relationships across modalities and en- hance its representation ability for COVID-19 segmentation. The results on the public COVID-19 segmentation benchmark show that our UCI with the inclusion of chest X-rays can significantly improve segmentation performance, outperforming advanced segmentation approaches includ- ing nnUNet, CoTr, nnFormer, and Swin UNETR. Code is available at: https://github.com/GQBBBB/UCI.",https://github.com/GQBBBB/UCI,https://covid-segmentation.grand-challenge.org/,Image Segmentation,Lung,CT,,,,,,,
Unsupervised 3D out-of-distribution detection with latent diffusion models ,"Methods for out-of-distribution (OOD) detection that scale to 3D data are crucial components of any real-world clinical deep learning system. Classic denoising diffusion probabilistic models (DDPMs) have been recently proposed as a robust way to perform reconstruction-based OOD detection on 2D datasets, but do not trivially scale to 3D data. In this work, we propose to use Latent Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution 3D medical data. We validate the proposed approach on near- and far-OOD datasets and compare it to a recently proposed, 3D-enabled approach using Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach achieve statistically significant better performance, it also shows less sensitivity to the underlying latent representation, more favourable memory scaling, and produces better spatial anomaly maps. Code is available at https://github.com/marksgraham/ddpm-ood.",https://github.com/marksgraham/ddpm-ood,http://medicaldecathlon.com/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,,,,,,,,
Unsupervised 3D registration through optimization-guided cyclical self-training ,"State-of-the-art deep learning-based registration methods employ three different learning strategies: supervised learning, which requires costly manual annotations, unsupervised learning, which heavily relies on hand-crafted similarity metrics designed by domain experts, or learning from synthetic data, which introduces a domain shift. To overcome the limitations of these strategies, we propose a novel self-supervised learning paradigm for unsupervised registration, relying on self-training. Our idea is based on two key insights. Feature-based differentiable optimizers 1) perform reasonable registration even from random features and 2) stabilize the training of the preceding feature extraction network on noisy labels. Consequently, we propose cyclical self-training, where pseudo labels are initialized as the displacement fields inferred from random features and cyclically updated based on more and more expressive features from the learning feature extractor, yielding a self-reinforcement effect. We evaluate the method for abdomen and lung registration, consistently surpassing metric-based supervision and outperforming diverse state-of-the-art competitors. Source code is available at https://github.com/multimodallearning/reg-cyclical-self-train.",https://github.com/multimodallearning/reg-cyclical-self-train,https://learn2reg.grand-challenge.org/Datasets,Image Registration,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
Unsupervised classification of congenital inner ear malformations using DeepDiffusion for latent space representation ,"The identification of congenital inner ear malformations is a challenging task even for experienced clinicians. In this study, we present the first automated method for classifying congenital inner ear malformations. We generate 3D meshes of the cochlear structure in 364 normative and 107 abnormal anatomies using a segmentation model trained exclusively with normative anatomies. Given the sparsity and natural unbalance of such datasets, we use an unsupervised method for learning a feature representation of the 3D meshes using DeepDiffusion. In this approach, we use the PointNet architecture for the network-based unsupervised feature learning and combine it with the diffusion distance on a feature manifold. This unsupervised approach captures the variability of the different cochlear shapes and generates clusters in the latent space which faithfully represent the variability observed in the data. We report a mean average precision of 0.77 over the seven main pathological subgroups diagnosed by an ENT (Ear, Nose, and Throat) surgeon specialized in congenital inner ear malformations.",https://github.com/paulalopez10/Deep-Diffusion-Unsupervised-Classification-3D-Mesh,,Computer Aided Diagnosis,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,CT,,,,,,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features ,"Inspired by findings that generative diffusion models learn semantically meaningful representations, we use them to discover the intrinsic hierarchical structure in biomedical 3D images using unsupervised segmentation. We show that features of diffusion models from different stages of a U-Net-based ladder-like architecture capture different hierarchy levels in 3D biomedical images. We design three losses to train a predictive unsupervised segmentation network that encourages the decomposition of 3D volumes into meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D diffusion models and use the consistency of their features across subvolumes. Second, we use the visual consistency between subvolumes. Third, we use the invariance to photometric augmentations as a regularizer. Our models perform better than prior unsupervised structure discovery approaches on challenging biologically-inspired synthetic datasets and on a real-world brain tumor MRI dataset. Code is available at https://github.com/uncbiag/diffusion-3D-discovery.",https://github.com/uncbiag/diffusion-3D-discovery,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,,,,,,,,
Unsupervised Domain Adaptation for Anatomical Landmark Detection ,"Recently, anatomical landmark detection has achieved great progresses on single-domain data, which usually assumes training and test sets are from the same domain. However, such an assumption is not always true in practice, which can cause significant performance drop due to domain shift. To tackle this problem, we propose a novel framework for anatomical landmark detection under the setting of unsupervised domain adaptation (UDA), which aims to transfer the knowledge from labeled source domain to unlabeled target domain. The framework leverages self-training and domain adversarial learning to address the domain gap during adaptation. Specifically, a self-training strategy is proposed to select reliable landmark-level pseudo-labels of target domain data with dynamic thresholds, which makes the adaptation more effective. Furthermore, a domain adversarial learning module is designed to handle the unaligned data distributions of two domains by learning domain-invariant features via adversarial training. Our experiments on cephalometric and lung landmark detection show the effectiveness of the method, which reduces the domain gap by a large margin and outperforms other UDA methods consistently.",https://github.com/jhb86253817/UDA_Med_Landmark,,Transfer learning,Data Efficient Learning,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,,,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks ,"Synthetic medical image generation has evolved as a key technique for neural network training and validation. A core challenge, however, remains in the domain gap between simulations and real data. While deep learning-based domain transfer using Cycle Generative Adversarial Networks and similar architectures has led to substantial progress in the field, there are use cases in which state-of-the-art approaches still fail to generate training images that produce convincing results on relevant downstream tasks. Here, we address this issue with a domain transfer approach based on conditional invertible neural networks (cINNs). As a particular advantage, our method inherently guarantees cycle consistency through its invertible architecture, and network training can efficiently be conducted with maximum likelihood training. To showcase our method’s generic applicability, we apply it to two spectral imaging modalities at different scales, namely hyperspectral imaging (pixel-level) and photoacoustic tomography (image-level). According to comprehensive experiments, our method enables the generation of realistic spectral data and outperforms the state of the art on two downstream classification tasks (binary and multi-class). cINN-based domain transfer could thus evolve as an important method for realistic synthetic data generation in the field of spectral imaging and beyond.
The code is available at https://github.com/IMSY-DKFZ/UDT-cINN.",https://github.com/IMSY-DKFZ/UDT-cINN,,Transfer learning,Biophotonics,Surgical Data Science,,,,,,,
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos ,"Zebrafish are widely used in biomedical research and developmental stages of their embryos often need to be synchronized for further analysis. We present an unsupervised approach to extract descriptive features from 3D+t point clouds of zebrafish embryos and subsequently use those features to temporally align corresponding developmental stages. An autoencoder architecture is proposed to learn a descriptive representation of the point clouds and we designed a deep regression network for their temporal alignment. We achieve a high alignment accuracy with an average mismatch of only 3.83 minutes over an experimental duration of 5.3 hours. As a fully-unsupervised approach, there is no manual labeling effort required and unlike manual analyses the method easily scales. Besides, the alignment without human annotation of the data also avoids any influence caused by subjective bias.",,https://www.nature.com/articles/srep08601,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Registration,Data Efficient Learning,Microscopy,Visualization in Biomedical Imaging,,,,,
UOD: Universal One-shot Detection of Anatomical Landmarks ,"One-shot medical landmark detection gains much attention and achieves great success for its label-efficient training process. However, existing one-shot learning methods are highly specialized in a single domain and suffer domain preference heavily in the situation of multi-domain unlabeled data. Moreover, one-shot learning is not robust that it faces performance drop when annotating a sub-optimal image. To tackle these issues, we resort to developing a domain-adaptive one-shot landmark detection framework for handling multi-domain medical images, named Universal One-shot Detection (UOD). UOD consists of two stages and two corresponding universal models which are designed as combinations of domain-specific modules and domain-shared modules. In the first stage, a domain-adaptive convolution model is self-supervised learned to generate pseudo landmark labels. In the second stage, we design a domain-adaptive transformer to eliminate domain preference and build the global context for multi-domain data. Even though only one annotated sample from each domain is available for training, the domain-shared modules help UOD aggregate all one-shot samples to detect more robust and accurate landmarks. We investigated both qualitatively and quantitatively the proposed UOD on three widely-used public X-ray datasets in different anatomical domains (i.e., head, hand, chest) and obtained state-of-the-art performances in each domain. The code is at https://github.com/heqin-zhu/UOD_universal_oneshot_detection.",https://github.com/heqin-zhu/UOD_universal_oneshot_detection,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computational Anatomy and Physiology,Data Efficient Learning,,,,,,,
UPCoL: Uncertainty-informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation ,"Semi-supervised learning (SSL) has emerged as a promising approach for medical image segmentation, while its capacity has still been limited by the difficulty in quantifying the reliability of unlabeled data and the lack of effective strategies for exploiting unlabeled regions with ambiguous predictions. To address these issues, we propose an Uncertainty-informed Prototype Consistency Learning (UPCoL) framework, which learns fused prototype representations from labeled and unlabeled data judiciously by incorporating an entropy-based uncertainty mask. The consistency constraint enforced on prototypes leads to a more discriminative and compact prototype representation for each class, thus optimizing the distribution of hidden embeddings. We experiment with two benchmark datasets of two-class semi-supervised segmentation, left atrium and pancreas, as well as a three-class multi-center dataset of type B aortic dissection. For all three datasets, UPCoL outperforms the state-of-the-art SSL methods, demonstrating the efficacy of the uncertainty-informed prototype learning strategy.",https://github.com/VivienLu/UPCoL,https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,CT,MRI,,,,,
Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports ,"Despite the reduction in turn-around times in radiology reporting with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of radiology reports. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite multiple efforts in literature to generate comprehensive medical reports, there lacks approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset. To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and the previous visit report, to pre-fill the “findings” section of the patient’s current visit. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset, and created a new dataset called \textit{Longitudinal-MIMIC}. With this new dataset, a transformer-based model was trained to capture the multi-modal longitudinal information from patient visit records (CXR images + reports) via a cross-attention-based multi-modal fusion module and a hierarchical memory-driven decoder. In contrast to previous works that only uses current visit data as input to train a model, our work exploits the longitudinal information available to pre-fill the “findings” section of radiology reports.  Experiments show that our approach outperforms several recent approaches by ≥3% on F1 score, and ≥2% for BLEU-4, METEOR and ROUGE-L respectively.",https://github.com/CelestialShine/Longitudinal-Chest-X-Ray,,Text (clinical/radiology reports),Lung,Other,,,,,,,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-wide-angle Transformation Multi-scale GAN ,"Fundus photography is an essential examination for clinical and differential diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF) techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have detrimental influences. To avoid negative impacts, cross-modality medical image generation algorithms have been proposed. Nevertheless, current methods in fundus imaging could not produce high-resolution images and are unable to capture tiny vascular lesion areas. This paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO. Using multi-scale generators and a fusion module patch to better extract global and local information, our model can generate high-resolution images. Moreover, an attention transmit module is proposed to help the decoder learn effectively. Besides, a supervised approach is used to train the network using multiple new weighted losses on different scales of data. Experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods. The source code is available at: https://github.com/Tinysqua/UWAT-GAN.",https://github.com/Tinysqua/UWAT-GAN,,Ophthalmology,other,,,,,,,,
UXDiff: Synthesis of X-ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network ,"Abstract. X-ray radiography with measurement of the Cobb angle is the gold standard for scoliosis diagnosis. However, cumulative exposure to ionizing radiation risks the health of patients. As a radiation-free alternative, imaging of scoliosis using 3D ultrasound scanning has recently been developed for the assessment of spinal deformity. Although these coronal ultrasound images of the spine can provide angle measurement comparable to X-rays, not all spinal bone features are visible. Diffu- sion probabilistic models (DPMs) have recently emerged as high-fidelity image generation models in medical imaging. To enhance the visualization of bony structures in coronal ultrasound images, we proposed UX-Diffusion, the first diffusion-based model for translating ultrasound coronal images to X-ray-like images of the human spine. To mitigate the underestimation in angle measurement, we first explored using ultrasound curve angle (UCA) to approximate the distribution of X-ray under Cobb angle condition in the reverse process. We then presented an angle embedding transformer module, establishing the angular variability conditions in the sampling stage. The quantitative results on the ultrasound and X-ray pair dataset achieved the state-of-the-art performance of high-quality X-ray generation and showed superior results in comparison with other reported methods. This study demonstrated that the proposed UX-diffusion method has the potential to convert the coronal ultrasound image of spine into the X-ray image for better visualization.",,,Ultrasound,Computer Aided Diagnosis,Guided Interventions and Surgery,Attention models,other,Treatment Response and Outcome/Disease Prediction,Visualization in Biomedical Imaging,,,
Vertex Correspondence in Cortical Surface Reconstruction ,"Mesh-based cortical surface reconstruction is a fundamental task in neuroimaging  that enables highly accurate measurements of brain morphology. 
Vertex correspondence between a patient’s cortical mesh and a group template is necessary for comparing cortical thickness and other measures at the vertex level. However, post-processing methods for generating vertex correspondence are time-consuming and involve registering and remeshing a patient’s surfaces to an atlas.
Recent deep learning methods for cortex reconstruction have neither been optimized for generating vertex correspondence nor have they analyzed the quality of such correspondence.
In this work, we propose to learn vertex correspondence by optimizing an L1 loss  on registered surfaces instead of the commonly used Chamfer loss. 
This results in improved inter- and intra-subject correspondence suitable for direct group comparison and atlas-based parcellation. 
We demonstrate that state-of-the-art methods provide insufficient correspondence for mapping parcellations, highlighting the importance of optimizing for accurate vertex correspondence.",https://github.com/ai-med/V2CC,,Neuroimaging - Others,Image Segmentation,Other,MRI,,,,,,
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis ,"We present a data-driven generative framework for synthesizing blood vessel 3D geometry. This is a challenging task due to the complexity of vascular systems, which are highly variating in shape, size, and structure. Existing model-based methods provide some degree of control and variation in the structures produced, but fail to capture the diversity of actual anatomical data. We developed VesselVAE, a recursive variational Neural Network that fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface.  After training, the VesselVAE latent space can be sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to utilize this technique for synthesizing blood vessels. We achieve similarities of synthetic and real data for radius (.97), length (.95), and tortuosity (.96). By leveraging the power of deep neural networks, we generate 3D models of blood vessels that are both accurate and diverse, which is crucial for medical and surgical training, hemodynamic simulations, and many other purposes.",https://github.com/LIA-DiTella/VesselVAE,https://github.com/intra3d2019/IntrA,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computational Anatomy and Physiology,,,,,,,,
VF-HM: Vision Loss Estimation using Fundus Photograph for High Myopia ,"High myopia (HM) is a leading cause of irreversible vision loss due to its association with various ocular complications including myopic maculopathy (MM). Visual field (VF) sensitivity systematically quantifies visual function, thereby revealing vision loss, and is integral to the evaluation of HM-related complications. However, measuring VF is subjective and time-consuming as it highly relies on patient compliance. Conversely, fundus photographs provide an objective measurement of retinal morphology, which reflects visual function. Therefore, utilizing machine learning models to estimate VF from fundus photographs becomes a feasible alternative. Yet, estimating VF with regression models using fundus photographs fails to predict local vision loss, producing stationary nonsense predictions. To tackle this challenge, we propose a novel method for VF estimation that incorporates VF properties and is additionally regularized by an auxiliary task. Specifically, we first formulate VF estimation as an ordinal classification problem, where each VF point is interpreted as an ordinal variable rather than a continuous one, given that any VF point is a discrete integer with a relative ordering. Besides, we introduce an auxiliary task for MM severity classification to assist the generalization of VF estimation, as MM is strongly associated with vision loss in HM. Our method outperforms conventional regression by 16.61% in MAE metric on a real-world dataset. Moreover, our method is the first work for VF estimation using fundus photographs in HM, allowing for more convenient and accurate detection of vision loss in HM, which could be useful for not only clinics but also large-scale vision screenings.",https://github.com/yanzipei/VF-HM,,Ophthalmology,Computer Aided Diagnosis,other,Treatment Response and Outcome/Disease Prediction,,,,,,
Virtual Heart models help elucidate the role of border zone in sustained monomorphic Ventricular Tachycardia ,"Post-ischemic Ventricular Tachycardia (VT) is sustained by a depolarization wave re-entry through channel-like structures within the post-ischemic scar. These structures are usually formed by partially viable tissue, called Border Zone (BZ). Understanding the anatomical and electrical properties of the BZ is crucial to guide ablation therapy to the right targets, reducing the likelihood of VT recurrence. Virtual Heart methods can provide ablation guidance non-invasively, but they have high computational complexity and have shown limited capability to accurately reproduce the specific mechanisms responsible for clinically observed VT. These outstanding challenges undermine the utility of Virtual Hearts for high precision ablation guidance in clinical practice. In this work, fast phenomenological models are developed to efficiently and accurately simulate the re-entrant dynamics of VT as observed in 12-lead ECG. Two porcine models of Myocardial Infarction (MI) are used to generate personalized bi-ventricular models from pre-operative LGE-MRI images. Myocardial conductivity and action potential duration are estimated using sinus rhythm ECG measurements. Multiple hypotheses for the BZ tissue properties are tested, and optimal values are identified. These allow the Virtual Heart model to produce VTs with good agreements with measurements in terms of ECG lead polarity and VT cycle length. Efficient GPU implementation of the cardiac electrophysiology model allows computation of sustained monomorphic VT in times compatible with the clinical workflow.",,,Computational Anatomy and Physiology,Cardiac,Surgical Planning and Simulation,,,,,,,
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation ,"Few-shot segmentation (FSS) models have gained popularity in medical imaging analysis due to their ability to generalize well to unseen classes with only a small amount of annotated data. A key requirement for the success of FSS models is a diverse set of annotated classes as the base training tasks.
This is a difficult condition to meet in the medical domain due to the lack of annotations, especially in volumetric images. 
To tackle this problem, self-supervised FSS methods for 3D images have been introduced. 
However, existing methods often ignore intra-volume information in 3D image segmentation, which can limit their performance.
To address this issue, we propose a novel self-supervised volume-aware FSS framework for 3D medical images, termed VISA-FSS. 
In general, VISA-FSS aims to learn continuous shape changes that exist among consecutive slices within a volumetric image to improve the performance of 3D medical segmentation.
To achieve this goal, we introduce a volume-aware task generation method that utilizes consecutive slices within a 3D image to construct more varied and realistic self-supervised FSS tasks during training.
In addition, to provide pseudo-labels for consecutive slices, a novel strategy is proposed that propagates pseudo-labels of a slice to its adjacent slices using flow field vectors to preserve anatomical shape continuity. 
In the inference time, we then introduce a volumetric segmentation strategy to fully exploit the inter-slice information within volumetric images.
Comprehensive experiments on two common medical benchmarks, including abdomen CT and MRI, demonstrate effectiveness of our model over state-of-the-art methods.",https://github.com/sharif-ml-lab/visa-fss,https://www.synapse.org/#!Synapse:syn3193805/wiki/217789,Meta-learning,CT,MRI,,,,,,,
Vision Transformer based Multi-Class Lesion Detection in IVOCT ,"Abstract. Cardiovascular disease is a high-fatality illness. Intravascular
Optical Coherence Tomography (IVOCT) technology can significantly
assist in diagnosing and treating cardiovascular diseases. However, locating
and classifying lesions from hundreds of IVOCT images is time-consuming
and challenging, especially for junior physicians. An automatic
lesion detection and classification model is desirable. To achieve
this goal, in this work, we first collect an IVOCT dataset, including 2,988
images from 69 IVOCT data and 4,734 annotations of lesions spanning
over three categories. Based on the newly-collected dataset, we propose a
multi-class detection model based on Vision Transformer, called G-Swin
Transformer. The essential part of our model is grid attention which
is used to model relations among consecutive IVOCT images. Through
extensive experiments, we show that the proposed G-Swin Transformer
can effectively localize different types of lesions in IVOCT images, significantly
outperforming baseline methods in all evaluation metrics. Our
code is available via this link. https://github.com/Shao1Fan/G-Swin-
Transformer",https://github.com/Shao1Fan/G-Swin-Transformer,https://drive.google.com/drive/folders/1Y3K2SNz26Nl6EMC6PqGYpmqAYFsH2ExZ?usp=drive_link,Computer Aided Diagnosis,Vascular,Attention models,other,,,,,,
Visual Grounding of Whole Radiology Reports for 3D CT Images ,"Building a large-scale training dataset is an essential problem in the development of medical image recognition systems. Visual grounding techniques, which automatically associate objects in images with corresponding descriptions, can facilitate labeling of large number of images. However, visual grounding of radiology reports for CT images remains challenging, because so many kinds of anomalies are detectable via CT imaging, and resulting report descriptions are long and complex. In this paper, we present the first visual grounding framework designed for CT image and report pairs covering various body parts and diverse anomaly types. Our framework combines two components of 1) anatomical segmentation of images, and 2) report structuring. The anatomical segmentation provides multiple organ masks of given CT images, and helps the grounding model recognize detailed anatomies. The report structuring helps to accurately extract information regarding the presence, location, and type of each anomaly described in corresponding reports. Given the two additional image/report features, the grounding model can achieve better localization. In the verification process, we constructed a large-scale dataset with region-description correspondence annotations for 10,410 studies of 7,321 unique patients. We evaluated our framework using grounding accuracy, the percentage of correctly localized anomalies, as a metric and demonstrated that the combination of the anatomical segmentation and the report structuring improves the performance with a large margin over the baseline model (66.0% vs 77.8%). Comparison with the prior techniques also showed higher performance of our method.",,,Computer Aided Diagnosis,Attention models,Other,CT,,,,,,
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction ,"Deep learning (DL) has been used in the automatic diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer’s Disease (AD) with brain imaging data. However, previous methods have not fully exploited the relation between brain image and clinical information that is widely adopted by experts in practice. To exploit the heterogeneous features from imaging and tabular data simultaneously, we propose the Visual-Attribute Prompt Learning-based Transformer (VAP-Former), a transformer-based network that efficiently extracts and fuses the multi-modal features with prompt fine-tuning. Furthermore, we propose a Prompt fine-Tuning (PT) scheme to transfer the knowledge from AD prediction task for progressive MCI (pMCI) diagnosis. In details, we first pre-train the VAP-Former without prompts on the AD diagnosis task and then fine-tune the model on the pMCI detection task with PT, which only needs to optimize a small amount of parameters while keeping the backbone frozen. Next, we propose a novel global prompt token for the visual prompts to provide global guidance to the multi-modal representations. Extensive experiments not only show the superiority of our method compared with the state-of-the-art methods in pMCI prediction but also demonstrate that the global prompt can make the prompt learning process more effective and stable. Interestingly, the proposed prompt learning model even outperforms the fully fine-tuning baseline on transferring the knowledge from AD to pMCI.",,,Computer Aided Diagnosis,Neuroimaging - Others,MRI,,,,,,,
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images ,"This paper introduces vox2vec - a contrastive method for self-supervised learning (SSL) of voxel-level representations. vox2vec representations are modeled by a Feature Pyramid Network (FPN): a voxel representation is a concatenation of the corresponding feature vectors from different pyramid levels. The FPN is pre-trained to produce similar representations for the same voxel in different augmented contexts and distinctive representations for different voxels. This results in unified multi-scale representations that capture both global semantics (e.g., body part) and local semantics (e.g., different small organs or healthy versus tumor tissue). We use vox2vec to pre-train a FPN on more than 6500 publicly available computed tomography images. We evaluate the pre-trained representations by attaching simple heads on top of them and training the resulting models for 22 segmentation tasks. We show that vox2vec outperforms existing medical imaging SSL techniques in three evaluation setups: linear and non-linear probing and end-to-end fine-tuning. Moreover, a non-linear head trained on top of the frozen vox2vec representations achieves competitive performance with the FPN trained from scratch while having 50 times fewer trainable parameters. The code is available at https://github.com/mishgon/vox2vec.",https://github.com/mishgon/vox2vec,https://www.synapse.org/#!Synapse:syn3193805/wiki/89480,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Image Segmentation,Data Efficient Learning,Transfer learning,CT,,,,,
Wall thickness estimation from short axis ultrasound images via temporal compatible deformation learning ,"Structural parameters of the heart, such as left ventricular wall thickness (LVWT), have important clinical significance for cardiac disease. In clinical practice, it requires tedious labor work to be obtained manually from ultrasound images, and results in large variation between experts. Great challenges exist to automatize this procedure: the myocardium boundary is sensitive to the heavy noise and can lead to irregular boundaries; the temporal dynamics in the ultrasound video are not well retained.In this paper, we propose a Temporally Compatible Deformation learning network, named TC-Deformer, to detect the myocardium boundaries and estimate LVWT automatically. Specifically, we first propose a two-stage deformation learning network to estimate the myocardium boundaries by deforming a prior myocardium template. A global affine transformation is first learned to shift and scale the template. Then a dense deformation filed is learned to adjust locally the template to match the myocardium boundaries. Second, to make the deformation learning of different frames become compatible in the temporal dynamics, we adopt the mean parameters of affine transformation for all frames and propose a bi-direction deformation learning to guarantee that the deformation fields across the whole sequences can be applied to both the myocardium boundaries and the ultrasound images. Experimental results on a ultrasound dataset of 201 participants show that the proposed method can achieve good boundary detection of basal, middle, and apical myocardium, and lead to accurate estimation of the LVWT, with a mean absolute error of less than 1.00 mm. When compared with human methods, our TC-Deformer performs better than the junior cardiologists and on par with the senior cardiologists.",,,Image Segmentation,Cardiac,Computer Aided Diagnosis,Other,Ultrasound,,,,,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-guided Procedures ,"Accurate catheter tracking is crucial during minimally invasive endovascular procedures (MIEP), and electromagnetic (EM) tracking is a widely used technology that serves this purpose. However, registration between preoperative images and the EM tracking system is often challenging. Existing registration methods typically require manual interactions, which can be time-consuming, increase the risk of errors and change the procedural workflow. Although several registration methods are available for catheter tracking, such as marker-based and path-based approaches, their limitations can impact the accuracy of the resulting tracking solution, consequently, the outcome of the medical procedure.
This paper introduces a novel automated catheter registration method for EM-guided MIEP. The method utilizes 3D signal temporal analysis, such as Dynamic Time Warping (DTW) algorithms, to improve registration accuracy and reliability compared to existing methods. DTW can accurately warp and match EM-tracked paths to the vessel’s centerline, making it particularly suitable for registration. The introduced registration method is evaluated for accuracy in a vascular phantom using a marker-based registration as the ground truth. The results indicate that the DTW method yields accurate and reliable registration outcomes, with a mean error of 2.22mm. The introduced registration method presents several advantages over state-of-the-art methods, such as high registration accuracy, no initialization required, and increased automation.",,,Vascular,Cardiac,Computer Aided Diagnosis,Guided Interventions and Surgery,CT,MRI,,,,
Wasserstein Distance-Preserving Vector Space of Persistent Homology ,"Analysis of large and dense networks based on topology is very difficult due to the computational challenges of extracting meaningful topological features from networks. In this paper, we present a computationally tractable approach to topological data analysis of large and dense networks. The approach utilizes principled theory from persistent homology and optimal transport to define a novel vector space representation for topological features. The feature vectors are based on persistence diagrams of connected components and cycles and are computed very efficiently. The associated vector space preserves the Wasserstein distance between persistence diagrams and fully leverages the Wasserstein stability properties. This vector space representation enables the application of a rich collection of vector-based models from statistics and machine learning to topological analyses. The effectiveness of the proposed representation is demonstrated using support vector machines to classify measured functional brain networks.",https://github.com/topolearn,,Neuroimaging - Functional Brain Networks,Other,EEG/ECG,,,,,,,
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning ,"The cerebellum (i.e., little brain) plays an important role in motion and balances control abilities, despite its much smaller size and deeper sulci compared to the cerebrum. Previous cerebellum studies mainly relied on and focused on conventional volumetric analysis, which ignores the extremely deep and highly convoluted nature of the cerebellar cortex. To better reveal localized functional and structural changes, we propose cortical surface-based analysis of the cerebellar cortex. Specifically, we first reconstruct the cerebellar cortical surfaces to represent and characterize the highly folded cerebellar cortex in a geometrically accurate and topologically correct manner. Then, we propose a novel method to automatically parcellate the cerebellar cortical surface into anatomically meaningful regions by a weakly supervised graph convolutional neural network. Instead of relying on registration or requiring mapping the cerebellar surface to a sphere, which are either inaccurate or have large geometric distortions due to the deep cerebellar sulci, our learning-based model directly deals with the original cerebellar cortical surface by decomposing this challenging task into two steps. First, we learn the effective representation of the cerebellar cortical surface patches with a contrastive self-learning framework. Then, we map the learned representations to parcellation labels. We have validated our method using data from the Baby Connectome Project and the experimental results demonstrate its superior effectiveness and accuracy, compared to existing methods.",,,Neuroimaging - Brain Development,Image Registration,Image Segmentation,Active Learning,Attention models,Transfer learning,,,,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration ,"The optical coherence tomography (OCT) signs of nascent geographic atrophy (nGA) are highly associated with GA onset. Automatically localizing nGA lesions can assist patient screening and endpoint evaluation in clinical trials. This task can be achieved with supervised object detection models, but they require laborious bounding box annotations. This study thus evaluated whether a weakly supervised method could localize nGA lesions based on the saliency map generated from a deep learning nGA classification model. This multi-instance deep learning model is based on 2D ResNet with late fusion and was trained to classify nGA on OCT volumes. The proposed method was cross-validated using a dataset consisting of 1884 volumes from 280 eyes of 140 subjects, which had volume-wise nGA labels and expert-graded slice-wise lesion bounding box annotations. The area under Precision-Recall curve (AUPRC) or correctly localized lesions was 0.72(±0.08), compared to 0.77(±0.07) from a fully supervised method with YOLO V3. No statistically significant difference is observed between the weakly supervised and fully supervised methods (Wilcoxon signed-rank test, p=1.0).",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,other,,,,,,,,
Weakly Supervised Medical Image Segmentation via Superpixel-guided Scribble Walking and Class-wise Contrastive Regularization ,"Deep learning-based segmentation typically requires a large amount of data with dense manual delineation, which is both time-consuming and expensive to obtain for medical images. Consequently, weakly-supervised learning, which attempts to utilize sparse annotations such as scribbles for effective training, has garnered considerable attention. However, such scribble-supervision inherently lacks sufficient structural information, leading to two critical challenges: (i) while achieving good performance in overall overlap metrics such as Dice score, the existing methods struggle to perform satisfactory local prediction because no desired structural priors are accessible during training; (ii) the class feature distributions are inevitably less-compact due to sparse and extremely incomplete supervision, leading to poor generalizability. To address these, in this paper, we propose the SC-Net, a new scribble-supervised approach that combines \textbf{S}uperpixel-guided scribble walking with \textbf{C}lass-wise contrastive regularization. Specifically, the framework is built upon the recent dual-decoder backbone design, where predictions from two slightly different decoders are randomly mixed to provide auxiliary pseudo-label supervision. Besides the sparse and pseudo supervision, the scribbles walk towards unlabeled pixels guided by superpixel connectivity and image content to offer as much dense supervision as possible. Then, the class-wise contrastive regularization disconnects the feature manifolds of different classes to encourage the compactness of class feature distributions. We evaluate our approach on the public cardiac dataset ACDC and demonstrate the superiority of our method compared to recent scribble-supervised and semi-supervised methods with similar labeling efforts.",https://github.com/Lemonzhoumeng/SC-Net,,Data Efficient Learning,Cardiac,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,MRI,,,,,
Weakly-supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery ,"The COVID-19 pandemic has prompted a surge in drug repurposing studies. However, many promising hits identified by modern neural networks failed in the preclinical research, which has raised concerns about the reliability of current drug discovery methods. Among studies that explore the therapeutic potential of drugs for COVID-19 treatment is RxRx19a. Its dataset was derived from High Throughput Screening (HTS) experiments conducted by the Recursion biotechnology company. Prior research on hit discovery using this dataset involved learning healthy and infected cells’ morphological features and utilizing this knowledge to estimate contaminated drugged cells’ scores. Nevertheless, models have never seen drugged cells during training, so these cells’ phenotypic features are out of their trained distribution. That being said, model estimations for treatment samples are not trusted in these methods and can lead to false positives. This work offers a first-in-field weakly-supervised drug efficiency estimation pipeline that utilizes the mixup methodology with a confidence score for its predictions. We applied our method to the RxRx19a dataset and showed that consensus between top hits predicted on different representation spaces increases using our confidence method. Further, we demonstrate that our pipeline is robust, stable, and sensitive to drug toxicity.",https://github.com/rohban-lab/Drug-Efficiency-Estimation-with-Confidence-Score,http://hpc.sharif.edu:8080/HRCE/,Microscopy,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Uncertainty,Treatment Response and Outcome/Disease Prediction,,,,,,
Weakly-supervised positional contrastive learning: application to cirrhosis classification ,"Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological score). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. Code will be made publicly available.",https://github.com/Guerbet-AI/wsp-contrastive,https://portal.gdc.cancer.gov/projects/TCGA-LIHC,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Abdomen,Computer Aided Diagnosis,Transfer learning,CT,Histopathology,,,,
WeakPolyp: You Only Look Bounding Box for Polyp Segmentation ,"Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization. In contrast, polyp bounding box annotations are much cheaper and more accessible. Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e., WeakPolyp) completely based on bounding box annotations. However, coarse bounding boxes contain too much noise. To avoid interference, we introduce the mask-to-box (M2B) transformation. By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction. But, M2B only provides sparse supervision, leading to non-unique predictions. Therefore, we further propose a scale consistency (SC) loss for dense supervision. By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions. Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones. Besides, the proposed modules are only used during training, bringing no computation cost to inference. Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all. Codes are available at https://github.com/weijun88/WeakPolyp.",https://github.com/weijun88/WeakPolyp,,Image Segmentation,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,,,,
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection ,"Detecting abnormal findings in medical images is a critical task that enables timely diagnoses, effective screening, and urgent case prioritization. Autoencoders (AEs) have emerged as a popular choice for anomaly detection and have achieved state-of-the-art (SOTA) performance in detecting pathology. However, their effectiveness is often hindered by the assumption that the learned manifold only contains information that is important for describing samples within the training distribution. In this work, we challenge this assumption and investigate what AEs actually learn when they are posed to solve anomaly detection tasks. We have found that standard, variational, and recent adversarial AEs are generally not well-suited for pathology detection tasks where the distributions of normal and abnormal strongly overlap. In this work, we propose MorphAEus, novel deformable AEs to produce pseudo-healthy reconstructions refined by estimated dense deformation fields. Our approach improves the learned representations, leading to more accurate reconstructions, reduced false positives, and precise localization of pathology. We extensively validate our method on two public datasets and demonstrate SOTA performance in detecting pneumonia and COVID-19. 
Code: \url{https://github.com/ci-ber/MorphAEus}",https://github.com/ci-ber/MorphAEus,https://bimcv.cipf.es/bimcv-projects/padchest/,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Lung,Computer Aided Diagnosis,Interpretability / Explainability,,,,,,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning ,"Reconstruction and visualization of cardiac structures play
significant roles in computer-aided clinical practice as well as scientific
research. With the advancement of medical imaging techniques, computing facilities, and deep learning models, automatically generating
whole-heart meshes directly from medical imaging data becomes feasible and shows great potential. Existing works usually employ a point
cloud metric, namely the Chamfer distance, as the optimization objective
when reconstructing the whole-heart meshes, which nevertheless does not
take the cardiac topology into consideration. Here, we propose a novel
currents-represented surface loss to optimize the reconstructed meshes’
topology. Due to currents’s favorable property of encoding the topology
of a whole surface, our proposed pipeline delivers whole-heart reconstruction results with correct topology and comparable or even higher accuracy.",,,Visualization in Biomedical Imaging,Computational Anatomy and Physiology,,,,,,,,
X2Vision : 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior ,"We propose an unsupervised deep learning method to reconstruct a 3D tomographic image from biplanar X-rays, to reduce the number of required projections, the patient dose, and the acquisition time. To address this ill-posed problem, we introduce prior knowledge of anatomic structures by training a generative model on 3D CTs of head and neck. We optimize the latent vectors of the generative model to recover a volume that both integrates this prior knowledge and ensures consistency between the reconstructed image and input projections. Our method outperforms recent methods in terms of reconstruction error while being faster and less radiating than current clinical workflow. We evaluate our method in a clinical configuration for radiotherapy.",,,Image Reconstruction,Oncology,Computer Aided Diagnosis,Image Registration,Guided Interventions and Surgery,Other,CT,other,,
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis ,"Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals’ trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologist would look for on an X-Ray scan, and use the descriptor probabilities to estimate the likelihood of a diagnosis. Our model is explainable by design, as the final diagnosis prediction is directly based on the prediction of the underlying descriptors. We evaluate Xplainer on two chest X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in improving the performance and explainability of zero-shot diagnosis. Our results suggest that Xplainer provides a more detailed understanding of the decision-making process and can be a valuable tool for clinical diagnosis.",https://github.com/ChantalMP/Xplainer,https://stanfordaimi.azurewebsites.net/datasets/23c56a0d-15de-405b-87c8-99c30138950c,Text (clinical/radiology reports),Lung,Computer Aided Diagnosis,Interpretability / Explainability,Semi-/Weakly-/Un-/Self-supervised Representation Learning,,,,,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression ,"Intraoperative fluoroscopy is a frequently used modality in minimally invasive orthopedic surgeries. Aligning the intraoperatively acquired X-ray image with the preoperatively acquired 3D model of a computed tomography (CT) scan reduces the mental burden on surgeons induced by the overlapping anatomical structures in the acquired images.  This paper proposes a fully automatic registration method that is robust to extreme viewpoints and does not require manual annotation of landmark points during training. It is based on a fully convolutional neural network (CNN) that regresses the scene coordinates for a given X-ray image. The scene coordinates are defined as the intersection of the back-projected rays from a pixel toward the 3D model. Training data for a patient-specific model were generated through a realistic simulation of a C-arm device using preoperative CT scans. In contrast, intraoperative registration was achieved by solving the perspective-n-point (PnP) problem with a random sample and consensus (RANSAC) algorithm. Experiments were conducted using a pelvic CT dataset that included several real fluoroscopic (X-ray) images with ground truth annotations. The proposed method achieved an average mean target registration error (mTRE) of 3.79+/1.67 mm in the 50th percentile of the simulated test dataset and projected mTRE of 9.65+/-4.07 mm in the 50thpercentile of real fluoroscopic images for pelvis registration. The code is available at https://github.com/Pragyanstha/SCR-Registration.",https://github.com/Pragyanstha/SCR-Registration,,Image Registration,Transfer learning,,,,,,,,
YONA: You Only Need One Adjacent Reference-frame for Accurate and Fast Video Polyp Detection ,"Accurate polyp detection is essential for assisting clinical rectal cancer diagnoses. Colonoscopy videos contain richer information than still images, making them a valuable resource for deep learning methods.However, unlike common fixed-camera video, the camera-moving scene in colonoscopy videos can cause rapid video jitters, leading to unstable training for existing video detection models. In this paper, we propose the YONA (You Only Need one Adjacent Reference-frame) method, an efficient end-to-end training framework for video polyp detection. YONA fully exploits the information of one previous adjacent frame and conducts polyp detection on the current frame without multi-frame collaborations. Specifically, for the foreground, YONA adaptively aligns the current frame’s channel activation patterns with its adjacent reference frame according to their foreground similarity. For the background, YONA conducts background dynamic alignment guided by inter-frame difference to eliminate the invalid features produced by drastic spatial jitters. Moreover, YONA applies cross-frame contrastive learning during training, leveraging the ground truth bounding box to improve the model’s perception of polyp and background. Quantitative and qualitative experiments on three public challenging benchmarks demonstrate that our proposed YONA outperforms previous state-of-the-art competitors by a large margin in both accuracy and speed.",,https://github.com/GewelsJI/VPS/tree/main#3-vps-dataset,Computer Aided Diagnosis,Data Efficient Learning,Video,,,,,,,
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images ,"Synthetic images generated from deep generative models seem to become a silver bullet to data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fréchet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that there is an inevitable trade-off between synthetic image fidelity, variety, and privacy. In addition, we have empirically demonstrated that the utility score does not require images with both high fidelity and high variety. For intra- and cross-task data augmentation, mode-collapsed images and low-fidelity images can still demonstrate high utility. Finally, our experiments have also showed that it is possible to produce images with both high utility and privacy, which can provide a strong rationale for the use of deep generative models in privacy-preserving applications. Our study can shore up comprehensive guidance for the evaluation of synthetic images and elicit further developments for utility-aware deep generative models in medical image synthesis.",https://github.com/ayanglab/MedSynAnalyzer,,Rigorous Evaluations of Methodology in Clinical Workflows,Computer Aided Diagnosis,Other,,,,,,,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-ray ,"Chest X-ray (CXR) anatomical abnormality detection aims at localizing and characterising cardiopulmonary radiological findings in the radiographs, which can expedite clinical workflow and reduce observational oversights. Most existing methods attempted this task in either fully supervised settings which demanded costly mass per-abnormality annotations, or weakly supervised settings which still lagged badly behind fully supervised methods in performance. In this work, we propose a co-evolutionary image and report distillation (CEIRD) framework, which approaches semi-supervised abnormality detection in CXR by grounding the visual detection results with text-classified abnormalities from paired radiology reports, and vice versa. Concretely, based on the classical teacher-student pseudo label distillation (TSD) paradigm, we additionally introduce an auxiliary report classification model, whose prediction is used for report-guided pseudo detection label refinement (RPDLR) in the primary vision detection task. Inversely, we also use the prediction of the vision detection model for abnormality-guided pseudo classification label refinement (APCLR) in the auxiliary report classification task, and propose a co-evolution strategy where the vision and report models mutually promote each other with RPDLR and APCLR performed alternatively. To this end, we effectively incorporate the weak supervision by reports into the semi-supervised TSD pipeline. Besides the cross-modal pseudo label refinement, we further propose an intraimage-modal self-adaptive non-maximum suppression, where the pseudo detection labels generated by the teacher vision model are dynamically rectified by high-confidence predictions by the student. Experimental results on the public MIMIC-CXR benchmark demonstrate CEIRD’s superior performance to several up-to-date weakly and semi-supervised methods. Our code will be available.",,,Semi-/Weakly-/Un-/Self-supervised Representation Learning,Computer Aided Diagnosis,Text (clinical/radiology reports),,,,,,,
Zero-shot Nuclei Detection via Visual-Language Pre-trained Models ,"Large-scale visual-language pre-trained models (VLPM) have proven their excellent performance in downstream object detection for natural scenes. However, zero-shot nuclei detection on H\&E images via VLPMs remains underexplored. The large gap between medical images and the web-originated text-image pairs used for pre-training makes it a challenging task. In this paper, we attempt to explore the potential of the object-level VLPM, Grounded Language-Image Pre-training (GLIP) model, for zero-shot nuclei detection. Concretely, an automatic prompts design pipeline is devised based on the association binding trait of VLPM and the image-to-text VLPM BLIP, avoiding empirical manual prompts engineering. We further establish a self-training framework, using the automatically designed prompts to generate the preliminary results as pseudo labels from GLIP and refine the predicted boxes in an iterative manner. Our method achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. Foremost, our work demonstrates that the VLPM pre-trained on natural image-text pairs exhibits astonishing potential for downstream tasks in the medical field as well. Code will be released at https://github.com/wuyongjianCODE/VLPMNuD.",https://github.com/wuyongjianCODE/VLPMNuD,,Histopathology,Data Efficient Learning,,,,,,,,
